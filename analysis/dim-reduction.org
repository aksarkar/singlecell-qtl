#+TITLE: Dimensionality reduction
#+OPTIONS: toc:nil
#+SETUPFILE: setup.org

* Introduction

  The fundamental inference task is to infer \(p(z_i \mid x_i)\), where \(x_i\)
  is the \(p\)-dimensional observation (one sample), \(z_i\) is a
  \(k\)-dimensional latent variable, and \(k \ll n\).

  Why do we want to do this? 
  - determine how much variation in the data is explained by known technical
    factors
  - remove that variation before trying to explain the data using biological
    covariates

  The typical procedure to apply PCA to single cell RNA-Seq data is:

  1. Normalize counts
  2. Select a subset of genes, e.g. those detected in some minimum proportion
     of cells, with high mean/variability of expression, etc.
  3. Compute singular vectors (principal components) of the count matrix
  4. Take loadings on the top \(k\) singular vectors as the latent \(z_i\)
  5. Correlate the loadings with the known covariates.

  Assuming this is the right way to quantify how much variation in the data is
  explained by covariates, there are several problems with this
  procedure. Prior work showed that (probabilistic) PCA fails to accurately
  infer \(z_i\) under a variety of scenarios ([[https://dx.doi.org/10.1186/s13059-015-0805-z][Pierson et al 2015]]):

  - high missingness
  - stringent gene selection

  Several methods have been proposed to improve inference of latent gene
  expression:

  - ZIFA ([[https://dx.doi.org/10.1186/s13059-015-0805-z][Pierson et al 2015]])
  - ZINB-WAVE ([[https://www.nature.com/articles/s41467-017-02554-5][Risso et al 2018]])
  - scVI ([[https://arxiv.org/abs/1709.02082][Lopez et al 2017]])
  - countae (Eraslen et al 2017)

  The main disadvantage of the first two methods is high computational cost and
  poor scaling with data size. The main disadvantage of the last two is they
  rely on neural networks making interpretation more difficult.

  It is also worth asking whether this is actually the right way to quantify
  how much variation in the data is explained by covariates. A different, but
  still natural way to phrase the question is to ask whether the expression
  values can be accurately predicted using known covariates. This procedure
  naturally suggests a confounder correction method: regress out known
  covariates from the expression values.

  This idea is related to half-sibling regression ([[http://www.pnas.org/cgi/doi/10.1073/pnas.1511656113][Sch√∂lkopf et al 2016]]): we
  can only accurately predict gene expression using genes on other chromosomes
  if there is true /trans/-regulation or systematic confounding. Again, the
  natural solution is to regress out the rest of the genome from the gene
  expression values.

  Importantly, these analyses are not directly usable for confounder correction
  for QTL mapping. Instead, we first need to [[file:zinb.org][learn the underlying distributions
  of the data]] and then perform dimensionality reduction on those parameters.

  Here, we perform the following analyses:

  1. [[*Principal components analysis][We perform PCA on the post-QC data]] and show that most variation is
     still explained by variation in sequencing metrics
  2. [[*Zero-inflated factor analysis][We provide an accelerated implementation of zero-inflated factor analysis]]
  3. [[*Variational autoencoder][We fit a variational autoencoder to the data]]
  4. [[*Predict the data using covariates][We predict mean expression and dropout probability using covariates only]]

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="16G",venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 42818938

  #+BEGIN_SRC ipython
    %matplotlib inline
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+NAME: dim-reduction-imports
  #+BEGIN_SRC ipython
    import colorcet
    import functools
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import scipy.stats as st
    import sklearn.decomposition as skd
    import sklearn.linear_model as sklm
    import ZIFA.block_ZIFA as zifa
  #+END_SRC

  #+RESULTS: dim-reduction-imports
  :RESULTS:
  # Out[1]:
  :END:

* Read the data

  Read the full data matrix and apply the QC filters.

  #+NAME: read-data-qc
  #+BEGIN_SRC ipython
    umi = pd.read_table('/home/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0)
    annotations = pd.read_table('/home/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    keep_samples = pd.read_table('/home/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    keep_genes = pd.read_table('/home/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
    umi = umi.loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
    annotations = annotations.loc[keep_samples.values.ravel()]
  #+END_SRC

  #+RESULTS: read-data-qc
  :RESULTS:
  # Out[22]:
  :END:

  #+BEGIN_SRC ipython
    umi.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  : (10978, 4865)
  :END:

  The expected ERCC spike-in molecule counts were previously tabulated for
  1:50\(\times\) dilution.

  #+BEGIN_SRC ipython
    ercc_mean_mols_50x = pd.read_table('https://raw.githubusercontent.com/jdblischak/singleCellSeq/master/data/expected-ercc-molecules.txt')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[34]:
  :END:

  #+BEGIN_SRC ipython
    def efficiency(sample, ercc_mean_mols_50x):
      res = sample['mol_ercc'] / ercc_mean_mols_50x['ercc_molecules_well'].sum()
      if sample['ERCC'] == '50x dilution':
        return res
      elif sample['ERCC'] == '100x dilution':
        return .5 * res
      else:
        return np.nan
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[35]:
  :END:

* Principal components analysis

  Use PPCA ([[http://www.miketipping.com/papers/met-mppca.pdf][Tipping et al 1999]]) to incorporate gene-specific mean
  expression. Use the ~edgeR~ psuedocount.

  #+NAME: normalize
  #+BEGIN_SRC ipython
    libsize = umi.sum(axis=0)
    psuedocount = .5 * libsize / libsize.mean()
    log_cpm = (np.log(umi + psuedocount) - np.log(libsize + 2 * psuedocount) + 6 * np.log(10)) / np.log(2)
  #+END_SRC

  #+RESULTS: normalize
  :RESULTS:
  # Out[23]:
  :END:

  #+BEGIN_SRC ipython
    ppca = skd.PCA(n_components=10)
    loadings = ppca.fit_transform(log_cpm.values.T)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[8]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca.png
    plt.clf()
    fig, ax = plt.subplots(2, 2)
    fig.set_size_inches(12, 12)
    for i in range(2):
      for j in range(i, 2):
        ax[i][j].scatter(loadings[:,i], loadings[:,j + 1])
        ax[i][j].set_xlabel('PC{}'.format(j + 2))
        ax[i][j].set_ylabel('PC{}'.format(i + 1))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[9]:
  [[file:figure/dim-reduction.org/pca.png]]
  :END:

  Correlate PCs with known continuous covariates.

  #+BEGIN_SRC ipython
    def extract_covars(annotations):
      return pd.Series({
        'batch': int(annotations['batch'][1:]),
        # Recode experiment YYYYMMDD so it is strictly increasing with time
        'experiment': 10000 * (annotations['experiment'] % 10000) + annotations['experiment'] // 10000,
        'index': annotations['index'],
        'concentration': annotations['concentration'],
        'reads_with_umi': annotations['umi'],
        'mols': annotations['molecules'],
        'efficiency': efficiency(annotations, ercc_mean_mols_50x=ercc_mean_mols_50x),
        'mapped_prop_hs': annotations['reads_hs'] / annotations['mapped'],
        'mapped_prop_dm': annotations['reads_dm'] / annotations['mapped'],
        'mapped_prop_ce': annotations['reads_ce'] / annotations['mapped'],
        'detect_hs': annotations['detect_hs'],
        'chipmix': annotations['chipmix'],
        'freemix': annotations['freemix'],
      })
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[32]:
  :END:

  #+BEGIN_SRC ipython
    cont_covars = annotations.apply(extract_covars, axis=1)
    cat_covars = annotations[['chip_id', 'well']]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[36]:
  :END:

  #+BEGIN_SRC ipython
    def correlation(pcs, cont_covars):
      """Return squared correlation between principal components and covariates

      pcs - DataFrame (n x k)
      cont_covars - DataFrame (n x q)

      """
      result = []
      for i in pcs:
        for j in cont_covars:
          keep = np.isfinite(cont_covars[j].values)
          result.append([i, j, np.square(st.pearsonr(pcs[i][keep], cont_covars[j][keep]))[0]])
      return pd.DataFrame(result, columns=['pc', 'covar', 'corr'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[37]:
  :END:

  Correlating with individual is non-obvious because it is a categorical
  variable, and simply recoding it as integer is sensitive to
  ordering. Instead, regress the loading of each cell on each principal
  component \(l_{ij}\) against indicator variables for each individual
  \(X_{ik}\).

  \[ l_{ij} = \sum_j X_{ik} \beta_{jk} + \mu + \epsilon \]

  From the regression fit, we can compute the coefficient of determination
  \(R^2\) for each PC \(j\):

  \[ 1 - \frac{l_j - X \hat{\beta}_j}{l_j - \bar{l_j}} \]

  #+BEGIN_SRC ipython
    def categorical_r2(loadings, annotations, key):
      categories = sorted(annotations[key].unique())
      onehot = np.zeros((annotations.shape[0], len(categories)), dtype=np.float32)
      onehot[np.arange(onehot.shape[0]), annotations[key].apply(lambda x: categories.index(x))] = 1
      m = sklm.LinearRegression(fit_intercept=True, copy_X=True).fit(onehot, loadings)
      return pd.DataFrame({
          'pc': np.arange(10),
          'covar': key,
          'corr': 1 - np.square(loadings - m.predict(onehot)).sum(axis=0) / np.square(loadings - loadings.mean(axis=0)).sum(axis=0)})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[38]:
  :END:

  #+BEGIN_SRC ipython
    corr = pd.concat(
      [correlation(pd.DataFrame(loadings), cont_covars)] +
      [categorical_r2(loadings, annotations, k) for k in cat_covars])
    corr = corr.pivot(index='covar', columns='pc')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[14]:
  :END:

  #+BEGIN_SRC ipython
    def plot_pca_covar_corr(pca, corr):
      plt.clf()
      fig, ax = plt.subplots(2, 1, gridspec_kw={'height_ratios': [.25, .75]}, sharex=True)
      fig.set_size_inches(8, 12)
      ax[0].bar(np.arange(len(pca.components_)), pca.explained_variance_ratio_)
      ax[0].set_xticks(np.arange(len(pca.components_)))
      ax[0].set_xticklabels([str(x) for x in np.arange(1, len(pca.components_) + 1)])
      ax[0].set_xlabel('Principal component')
      ax[0].set_ylabel('PVE')

      im = ax[1].imshow(corr.values, cmap=colorcet.cm['fire'], vmin=0, vmax=1, aspect='auto')
      cb = plt.colorbar(im, ax=ax[1], orientation='horizontal')
      cb.set_label('Squared correlation')
      ax[1].set_xlabel('Principal component')
      ax[1].set_yticks(np.arange(corr.shape[0]))
      ax[1].set_yticklabels(corr.index)
      ax[1].set_ylabel('Covariate')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[44]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca-vs-covars.png
    plot_pca_covar_corr(ppca, corr)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[16]:
  [[file:figure/dim-reduction.org/pca-vs-covars.png]]
  :END:

  The top 10 PCs define a low-rank approximation to the original data, so we
  should ask how good the approximation was, by comparing the distribution of
  the original data to the distribution of the reconstructed data.

  #+BEGIN_SRC ipython
    reconstructed = ppca.inverse_transform(loadings)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[17]:
  :END:

  #+BEGIN_SRC ipython
    def plot_reconstruction(obs, approx):
      plt.clf()
      plt.hist(obs, bins=50, density=True, histtype='step', color='k', label='Observed')
      plt.hist(approx, bins=50, density=True, histtype='step', color='r', label='Reconstructed')
      plt.legend()
      plt.xlabel('$\log_2(\mathrm{CPM} + 1)$')
      plt.ylabel('Empirical density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[18]:
  :END:

  For genes with high proportion of zero counts, the low-rank approximation is
  mainly capturing the mean of the data, which is maybe more indicative of the
  zero proportion in the data rather than the actual mean of the data.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/reconstruction-error-max-zero.png
    num_zero = np.isclose(umi, 0).sum(axis=1)
    max_zero = num_zero.argmax()
    plot_reconstruction(log_cpm.iloc[max_zero], reconstructed[:,max_zero])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[19]:
  [[file:figure/dim-reduction.org/reconstruction-error-max-zero.png]]
  :END:

  This is true even for genes with the lowest proportion of zero counts.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/reconstruction-error-min-zero.png
    min_zero = num_zero.argmin()
    plot_reconstruction(log_cpm.iloc[min_zero], reconstructed[:,min_zero])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[20]:
  [[file:figure/dim-reduction.org/reconstruction-error-min-zero.png]]
  :END:

  This might still be OK, if the reconstructed gene expression values are
  predictive of the original gene expression values.

  #+BEGIN_SRC ipython :async t
    pred_score = [sklm.LinearRegression(fit_intercept=True).fit(x.values.reshape(-1, 1), y).score(x.values.reshape(-1, 1), y)
                  for (_, x), (_, y)
                  in zip(log_cpm.iteritems(),
                         pd.DataFrame(reconstructed.T).iteritems())]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[21]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/reconstruction-pred-score.png
    plt.clf()
    plt.hist(pred_score, bins=50)
    plt.xlabel('Prediction $R^2$')
    plt.ylabel('Number of genes')
    plt.title('Correlation between PCA and original data')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[22]:
  : Text(0.5,1,'Correlation between PCA and original data')
  [[file:figure/dim-reduction.org/reconstruction-pred-score.png]]
  :END:

  The distribution of squared correlations suggest that the low rank
  approximation is better for some genes than others, i.e. that there could be
  gene-specific or gene module-specific effects. These are unlikely to be
  captured by PCA or factor analysis.

* Zero-inflated factor analysis

  If dropout changes the distribution of the non-zero observations, we might
  hope that fitting a latent variable model which explicitly includes dropout
  might eliminate that effect. Intuitively, we should downweight the evidence
  of observed zeroes on the inferred distribution which generated the
  observations. 

  However, fitting ZIFA ([[https://dx.doi.org/10.1186/s13059-015-0805-z][Pierson et al 2015]]) on the data again recovers a
  factor which is detection rate.

  #+BEGIN_SRC ipython :eval never :tangle zifa.py :noweb tangle
    <<dim-reduction-imports>>
    <<read-data-qc>>
    <<normalize>>
    latent, params = zifa.fitModel(Y=np.log(umi.values.T + 1), K=10, p0_thresh=.7)
    np.save('/home/aksarkar/projects/singlecell-qtl/data/zifa-loadings.npy', latent)
  #+END_SRC

  Run on 16 cores to complete in a reasonable amount of time.

  #+BEGIN_SRC sh :eval never-export :exports both
    sbatch --partition=broadwl --time=400 --mem=32G -c16 --out=zifa.out --err zifa.err
    #!/bin/bash
    source activate scqtl
    python zifa.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 42700513

  #+ATTR_HTML: :class table
  #+BEGIN_SRC sh :eval never-export :exports both
    sacct -j 42700513 -o Elapsed,MaxRSS,MaxVMSize
  #+END_SRC

  #+RESULTS:
  |    Elapsed | MaxRSS     | MaxVMSize  |
  | ---------- | ---------- | ---------- |
  |   03:15:28 |            |            |
  |   03:15:28 | 19195312K  | 23219728K  |

  #+BEGIN_SRC ipython
    latent = np.load('/home/aksarkar/projects/singlecell-qtl/data/zifa-loadings.npy')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[59]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/zifa-covars.png
    corr_zifa = pd.concat(
      [correlation(pd.DataFrame(latent), cont_covars)] +
      [categorical_r2(latent, annotations, k) for k in cat_covars])
    corr_zifa = corr_zifa.pivot(index='covar', columns='pc')

    plt.clf()
    plt.gcf().set_size_inches(8, 12)
    im = plt.imshow(corr_zifa.values, cmap=colorcet.cm['fire'], vmin=0, vmax=1, aspect='auto')
    cb = plt.colorbar(im, orientation='horizontal')
    cb.set_label('Squared correlation')
    plt.xlabel('Learned factor')
    _ = plt.xticks(np.arange(latent.shape[1]), np.arange(1, latent.shape[1] + 1))
    plt.ylabel('Covariate')
    _ = plt.yticks(np.arange(corr_zifa.shape[0]), corr_zifa.index)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[58]:
  [[file:figure/dim-reduction.org/zifa-covars.png]]
  :END:

  ZIFA does not constrain factors to be orthogonal, so we would not expect it
  to get the same result as PPCA. However, the latent factor inferred by ZIFA
  correlated with sequencing depth still is highly correlated with PC1 inferred
  by PPCA, suggesting that it is not immune to whatever is biasing PPCA.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca-vs-zifa-corr.png
    plt.clf()
    plt.gcf().set_size_inches(12, 12)
    im = plt.imshow(np.tril(np.corrcoef(latent.T, loadings.T)), cmap=colorcet.cm['coolwarm'], vmin=-1, vmax=1)
    cb = plt.colorbar(im)
    cb.set_label('Correlation')
    labels = ['{} {}'.format(v, i) for v in ('Factor', 'PC') for i in range(10)]
    _ = plt.xticks(np.arange(20), labels, rotation=90)
    _ = plt.yticks(np.arange(20), labels)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[97]:
  [[file:figure/dim-reduction.org/pca-vs-zifa-corr.png]]
  :END:

* Effect of normalization on PCA

  Should we expect that the top principal component of log CPM is still
  sequencing depth?  [[https://dx.doi.org/10.1093/biostatistics/kxx053][Hicks et al 2017]] claim that correlation of the first
  principal component with detection rate can be explained by two facts:

  1. Centering the values of \(X\) does not center the values of \(\log X\)
     (and vice versa)
  2. \(E[\log X]\) depends on the gene detection rate

  To see whether these facts can explain the correlation between PC1 and
  sequencing metrics we see in our data, we perform the following simulation:

  1. Generate un-normalized relative expression values for two groups of
     samples, where one group has a true fold change in mean expression
     \(\beta\):

     \[ a^{(1)}_j \sim \mathrm{LogNormal}(0, 1) \]

     \[ a^{(2)}_0 = \beta a^{(1)}_0 \]

     \[ a^{(2)}_j = a^{(1)}_j,\ j \neq 0 \]

  2. Compute the relative expression for each group \(p^{(k)}_j = a^{(k)}_j /
     \sum_j a^{(k)}_j\)
  3. Sample a number of molecules \(R_i\) from the observed distribution of
     molecules
  4. Sample molecule counts \(x_{ij} \sim \mathrm{Multinomial}(R_i, p)\)
  5. Normalize to log CPM using the ~edgeR~ definition
  6. Fit probabilistic PCA ([[http://www.miketipping.com/papers/met-mppca.pdf][Tipping et al 1999]]) to explicitly account for mean
     differences in the normalized data:

     \[ p(x_i \mid z_i) \sim N(W z_i + \mu, \sigma^2 I) \]

     \[ p(z_i) \sim N(0, I) \]

     where:

     - \(x_i\) is a \(p \times 1\) observation
     - \(z_i\) is the associated \(k \times 1\) latent variable, \(k \ll p\)
     - \(W\) is the \(p \times k\) matrix of loadings
     - \(\mu\) is the \(p \times 1\) vector of gene-means

  7. Compute the squared correlation of loadings on each PC to \(R\)

  #+BEGIN_SRC ipython
    class Simulation:
      def __init__(self, num_genes, fold_change=2, seed=0):
        np.random.seed(seed)
        self.num_genes = num_genes
        rel_expr_1 = np.flip(np.sort(np.random.lognormal(size=self.num_genes)), axis=-1)
        rel_expr_2 = rel_expr_1.copy()
        rel_expr_2[0] *= fold_change
        self.rel_expr = [rel_expr_1, rel_expr_2]
        for r in self.rel_expr:
          r /= r.sum()

      def generate_log_cpm(self, num_samples, library_sizes, detection_rates=None):
        libsize = [np.random.choice(library_sizes, num_samples // 2) for _ in range(2)]
        if detection_rates is not None:
          det_rate = [np.random.choice(detection_rates, num_samples)]
        counts = np.array([np.random.multinomial(n, p) for sizes, p in zip(libsize, self.rel_expr) for n in sizes]).reshape(num_samples, self.num_genes)
        if detection_rates is not None:
          mask = np.array([np.random.uniform() < d for d in det_rate for _ in self.rel_expr[0]]).reshape(num_samples, self.num_genes)
          counts *= mask
        total_counts = counts.sum(axis=1)
        psuedocount = .5 * total_counts / total_counts.mean()
        log_cpm = np.log(counts + psuedocount.reshape(-1, 1)) - np.log(total_counts + 2 * psuedocount).reshape(-1, 1) + 6 * np.log(10)
        return log_cpm

      def pca_corr(num_components=10, num_trials=10):
        corr = []
        for _ in range(num_trials):
          ppca = skd.PCA(n_components=num_components)
          loadings = ppca.fit_transform(log_cpm)
          corr.append([st.pearsonr(x.ravel(), total_counts.ravel())[0] for x in loadings.T])
        return corr
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[124]:
  :END:

  Look at a draw of the relative expression values:

  #+BEGIN_SRC ipython
    sim = Simulation(num_genes=1000)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[125]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-rel-expr.png
    plt.clf()
    plt.hist(sim.rel_expr[0], density=True, bins=50)
    plt.xlabel('Relative expression')
    plt.ylabel('Density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[105]:
  : Text(0,0.5,'Density')
  [[file:figure/dim-reduction.org/simulated-rel-expr.png]]
  :END:

  Look at a draw of log CPM:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-log-cpm.png
    quantiles = np.linspace(0, 1, 9)[:-1]
    simulated_log_cpm = sim.generate_log_cpm(num_samples=50, library_sizes=annotations['molecules'])
    simulated_log_cpm = simulated_log_cpm[:,(quantiles * sim.num_genes).astype(int)]
    jitter = np.random.normal(scale=.01, size=(50, 1)) + quantiles.reshape(1, -1)
    plt.clf()
    plt.scatter(jitter.ravel(), simulated_log_cpm.ravel())
    plt.xlabel('Rank of relative gene expression')
    plt.ylabel('log CPM')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[139]:
  : Text(0,0.5,'log CPM')
  [[file:figure/dim-reduction.org/simulated-log-cpm.png]]
  :END:

  Although the first principal component has non-zero correlation with library
  size, our simulation does not produce correlations comparable to the
  correlation we see in the actual data. This result suggests that explanation
  (1), not centering the data, does not fully explain the correlation.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-pca.png :async t
    corr = simulate_pca_log_cpm(500, 1000, annotations['molecules'], fold_change=2, num_trials=20, seed=1)
    plt.clf()
    plt.boxplot(np.square(corr))
    plt.xlabel('Principal component')
    plt.ylabel('Squared correlation with library size')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[80]:
  : Text(0,0.5,'Squared correlation with library size')
  [[file:figure/dim-reduction.org/simulated-pca.png]]
  :END:

  In the simulation, the correlation does not appear to depend on the true fold
  change (i.e., proportion of variance explained by biological factors). This
  result could be explained if the proportion of variance explained by the true
  fold change were small compared to the proportion of variance explained by
  library size.

  #+BEGIN_SRC ipython :async t
    corr_vs_fold_change = [
      simulate_pca_log_cpm(500, 1000, annotations['molecules'], fold_change=fold_change, num_components=1, num_trials=20, seed=0)
      for fold_change in np.linspace(1.1, 2, 10)]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[128]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-pca-vs-fold-change.png
    plt.clf()
    plt.boxplot(np.square(np.array(corr_vs_fold_change).mean(axis=-1)).T, positions=np.arange(10))
    plt.xticks(np.arange(10), ['{:.3f}'.format(x) for x in np.linspace(1.1, 2, 10)])
    plt.xlabel('True fold change')
    plt.ylabel('Squared correlation with PC1')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[129]:
  : Text(0,0.5,'Squared correlation with PC1')
  [[file:figure/dim-reduction.org/simulated-pca-vs-fold-change.png]]
  :END:

  If (not) centering the data does not explain the correlation, does dropout
  explain the correlation? We simulated dropout after drawing molecule counts
  \(x_{ij}\) as follows:

  1. Draw gene detection rates \(q_i\) from the observed gene detection rates
  2. Draw \(h_{ij} \sim Bernoulli(q_i)\) iid.
  3. Use \(X^* = X \circ H\) as the observed count matrix

  Surprisingly, the correlation with the first PC goes away, likely because
  this dropout model is not correct.

  #+BEGIN_SRC ipython :async t
    corr_with_dropout = simulate_pca_log_cpm(500, 10000, annotations['molecules'], detection_rates=annotations['detect_hs'] / 2e4, fold_change=2, num_trials=20, seed=1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-pca-vs-dropout.png
    plt.clf()
    _ = plt.boxplot(np.square(corr_with_dropout))
    plt.xlabel('Principal component')
    plt.ylabel('Squared correlation with library size')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[84]:
  : Text(0,0.5,'Squared correlation with library size')
  [[file:figure/dim-reduction.org/simulated-pca-vs-dropout.png]]
  :END:

  Hicks et al also claim that the entire distribution of non-zero measurements
  depends on detection rate. They show this by plotting the percentiles of
  non-zero expressed genes in each cell versus detection rate in that cell.

  #+BEGIN_SRC ipython
    def plot_quantiles_vs_detection(umi, annotations, quantiles=None, pseudocount=None):
      if quantiles is None:
        quantiles = np.linspace(0, 1, 5)
      else:
        assert (quantiles >= 0).all()
        assert (quantiles <= 1).all()
      vals = np.nanpercentile(np.ma.masked_equal(umi.values, 0).astype(float).filled(np.nan), 100 * quantiles, axis=0, interpolation='higher')
      if pseudocount is None:
        # log CPM with per-cell pseudocount
        total_counts = umi.sum()
        pseudocount = .5 * total_counts / total_counts.mean()
        label = 'log CPM'
        vals = np.log(vals + pseudocount.values.reshape(1, -1)) - np.log(total_counts + 2 * pseudocount).values.reshape(1, -1) + 6 * np.log(10)
      else:
        vals = np.log(vals + pseudocount)
        label = '$\log(\mathrm{UMI} + {:.3g})$'.format(pseudocount)

      plt.clf()
      for q, v in zip(quantiles, vals):
        plt.scatter(annotations['detect_hs'] / umi.shape[0], v, c=colorcet.cm['inferno'](q), label='{:.3f}'.format(q))
      plt.legend(bbox_to_anchor=(1.04, .5), loc='center left')
      plt.xlabel('Detection rate')
      plt.ylabel(label)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[55]:
  :END:

  We recapitulate the main result of Hicks et al in our data.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/umi-quantiles-vs-detection.png
    plot_quantiles_vs_detection(umi, annotations)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[56]:
  [[file:figure/dim-reduction.org/umi-quantiles-vs-detection.png]]
  :END:

  log CPM as defined in ~edgeR~ uses a pseudocount which depends on library
  size, but the derivation in Hicks et al is for \(\log(X + \epsilon)\) where
  \(\epsilon\) is constant across cells. 

  Using constant \(\epsilon\) changes the shape of the relationship between
  quantiles of non-zero expression and detection rate, but does not remove the
  relationship.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/umi-quantiles-vs-detection-1.png
    plot_quantiles_vs_detection(umi, annotations, pseudocount=1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[57]:
  [[file:figure/dim-reduction.org/umi-quantiles-vs-detection-1.png]]
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/umi-quantiles-vs-detection-1e-3.png
    plot_quantiles_vs_detection(umi, annotations, pseudocount=1e-3)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[58]:
  [[file:figure/dim-reduction.org/umi-quantiles-vs-detection-1e-3.png]]
  :END:

