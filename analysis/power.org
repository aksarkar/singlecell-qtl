#+TITLE: Power to detect QTLs in single cell data
#+SETUPFILE: setup.org
#+OPTIONS: toc:t

* Introduction

  We [[file:qtl-mapping.org][previously found]] that our study lost power to detect eQTLs, and was
  underpowered to directly detect dispersion-QTLs.

  Here, we estimate power to detect eQTLs and dispersion-QTLs as a function of:

  - number of cells per individual
  - number of molecules per cell
  - number of individuals

* Setup                                                            :noexport:
 
  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "~/.emacs.d/org-templates/library.org")
    (org-babel-lob-ingest "/project2/mstephens/aksarkar/projects/singlecell-qtl/analysis/zinb.org")
    (org-babel-lob-ingest "/project2/mstephens/aksarkar/projects/singlecell-qtl/analysis/qtl-mapping.org")
  #+END_SRC

  #+RESULTS:
  : 16

  #+CALL: ipython3(memory="8G", venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 47401086

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])

    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  #+NAME: power-imports
  #+BEGIN_SRC ipython
    import functools
    import numpy as np
    import pandas as pd
    import pickle
    import rpy2.robjects.numpy2ri
    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri
    import scipy.integrate as si
    import scipy.linalg as sl
    import scipy.optimize as so
    import scipy.stats as st
    import tabix

    rpy2.robjects.numpy2ri.activate()
    rpy2.robjects.pandas2ri.activate()
    ashr = rpy2.robjects.packages.importr('ashr')
    mashr = rpy2.robjects.packages.importr('mashr')
  #+END_SRC

  #+RESULTS: power-imports
  :RESULTS:
  # Out[4]:
  :END:

* Differential dispersion

  Perform a nested model comparison for each gene \(k\), comparing the model:

  \[ r_{ijk} \sim \mathrm{ZINB}(\pi_{ik}, \mu_{ik}, \phi_{ik}) \]

  against the model:

  \[ r_{ijk} \sim \mathrm{ZINB}(\pi_{ik}, \mu_{ik}, \phi_{i}) \]

  #+NAME: lrt-impl
  #+BEGIN_SRC ipython :eval never
    def lrt(umi, onehot, design, size_factor):
      _, _, _, llik0 = fit(
        umi=umi.values.T.astype(np.float32),
        onehot=onehot.astype(np.float32),
        design=design.astype(np.float32),
        size_factor=size_factor.astype(np.float32),
        fit_null=True,
        return_llik=True,
        learning_rate=5e-2,
        max_epochs=4000)
      _, _, _, llik1 = fit(
        umi=umi.values.T.astype(np.float32),
        onehot=onehot.astype(np.float32),
        design=design.astype(np.float32),
        size_factor=size_factor.astype(np.float32),
        return_llik=True,
        learning_rate=5e-2,
        max_epochs=4000)
      T = 2 * (llik1 - llik0)
      return T, st.chi2(1).logsf(T)
  #+END_SRC

** Null calibration via parametric bootstrap

   Sample null data from the model, using empirical estimates of the parameters
   in the observed data.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null-parametric.py
     <<zinb-imports>>
     <<tf-imports>>
     <<tf-zinb-impl>>
     <<lrt-impl>>
     <<sim-impl>>

     log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     log_phi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', index_col=0, sep=' ')
     logodds = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     params = pd.DataFrame({'log_mu': log_mu['NA18507'],
                            'log_phi': log_phi['NA18507'],
                            'logodds': logodds['NA18507']},
                           index=log_mu.index)
     params = params[params['log_mu'] > -10]
     n = 100
     umi = pd.DataFrame([simulate(2 * n, size=1e5, log_mu=log_mu, log_phi=log_phi, logodds=logodds)[0][:,0]
                         for _, (log_mu, log_phi, logodds) in params.iterrows() if log_mu > -10], index=params.index)
     onehot = np.zeros((2 * n, 2))
     onehot[:n,0] = 1
     onehot[n:,1] = 1
     design = np.zeros((2 * n, 1))
     size_factor = 1e5 * np.ones((2 * n, 1))
     T, P = lrt(umi, onehot, design, size_factor)
     pd.DataFrame({'chi2': T, 'logp': P}, index=umi.index).to_csv('null-calibration-p.txt.gz', sep='\t', compression='gzip')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
     sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-lrt-null --output=tf-lrt-null-parametric.out
     #!/bin/bash
     source activate scqtl
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null-parametric.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 46322399

   Read the results.

   #+BEGIN_SRC ipython
     null_lrt = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/null-calibration-p.txt.gz')
     N = null_lrt.shape[0]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[2]:
   :END:

   Report how many genes were simulated.

   #+BEGIN_SRC ipython
     N
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[3]:
   : 1832
   :END:

   Estimate bootstrap CIs for the quantiles.

   #+BEGIN_SRC ipython
     B = np.sort(st.chi2(1).rvs((N, 100)), axis=0)
     ci = np.percentile(B, [2.5, 97.5], axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[18]:
   :END:

   Plot the QQ-plot.

   #+BEGIN_SRC ipython :ipyfile figure/power.org/null-calibration-p.png
     plt.clf()
     plt.gcf().set_size_inches(4, 4)
     grid = st.chi2(1).ppf(np.linspace(0, 1 - 1 / N, N))
     plt.scatter(grid, null_lrt['chi2'].sort_values(), c='k', s=2)
     plt.fill_between(grid, ci[0], ci[1], color='k', alpha=0.1)
     lim = [0, 1.01 * grid.max()]
     plt.plot(lim, lim, c='r', lw=1)
     plt.xlim(lim)
     plt.xlabel('Expected $\chi^2$ statistic')
     _ = plt.ylabel('Observed $\chi^2$ statistic')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[40]:
   [[file:figure/power.org/null-calibration-p.png]]
   :END:

** Null calibration via nonparametric bootstrap                    :noexport:

   Assuming the true single cells all came from one individual, generate new
   single cells by sampling counts for each gene iid. with replacement. This
   method assumes genes are independent, which is justified only if we perform
   analysis one gene at a time.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null.py
     <<zinb-imports>>
     <<tf-imports>>
     <<tf-zinb-impl>>
     <<lrt-impl>>

     def downsample_counts(umi, size_factor, num_mols):
       """Return a downsampled matrix of UMI counts

       This is needed so that every cell has the same number of molecules in
       expectation

       """
       p = num_mols / size_factor
       assert (p <= 1).all()
       return umi.apply(lambda x: np.random.binomial(x, p), axis=1)

     def sample_cells(umi, n=1):
       """Return a sampled matrix of UMI counts

       Assume the input matrix is UMI for a single individal. Sample gene counts
       i.i.d. from the empirical distribution of counts

       """
       result = np.zeros((umi.shape[0], n))
       for i, row in enumerate(umi.values):
         result[i] = np.random.choice(row, size=n, replace=True)
       return pd.DataFrame(result, index=umi.index)

     <<read-data-qc-impl>>
     log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     keep_cells = np.logical_and(
       annotations['chip_id'] == 'NA18507',
       annotations['mol_hs'] > 1e5,
     )
     umi = umi.loc[log_mu['NA18507'] >= -10,keep_cells.values]

     # Generate counts at equal number of molecules (to simplify bootstrapping)
     np.random.seed(1)
     umi = downsample_counts(umi, annotations['mol_hs'], 1e5)

     # Generate two groups under the null
     n = 100
     umi = sample_cells(umi, n=2 * n)
     onehot = np.zeros((2 * n, 2))
     onehot[:n,0] = 1
     onehot[n:,1] = 1
     design = np.zeros((2 * n, 1))
     size_factor = num_mols.min() * np.ones((2 * n, 1))

     T, P = lrt(umi, onehot, design, size_factor)
     pd.DataFrame({'chi2': T, 'logp': P}, index=umi.index).to_csv('null-calibration.txt.gz', sep='\t', compression='gzip')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
     sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-lrt-null --output=tf-lrt-null.out
     #!/bin/bash
     source activate scqtl
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 46323093

   Plot the QQ-plot.

   #+BEGIN_SRC ipython :ipyfile figure/power.org/null-calibration.png
     null_lrt = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/null-calibration.txt.gz')
     N = null_lrt.shape[0]
     plt.clf()
     plt.gcf().set_size_inches(4, 4)
     plt.scatter(st.chi2(1).ppf(np.linspace(0, 1 - 1 / N, N)), null_lrt['chi2'].sort_values(), c='k', s=2, alpha=0.25)
     lim = [0, 1.1 * null_lrt['chi2'].max()]
     plt.plot(lim, lim, c='r', ls=':', lw=1)
     plt.xlim(lim)
     plt.ylim(lim)
     plt.xlabel('Expected chi-square statistic')
     _ = plt.ylabel('Observed chi-square statistic')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   [[file:figure/power.org/null-calibration.png]]
   :END:

** Power

   Sample from the assumed model.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-power.py
     <<zinb-imports>>
     <<tf-imports>>
     <<tf-zinb-impl>>
     <<lrt-impl>>
     <<sim-impl>>

     log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     log_phi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', index_col=0, sep=' ')
     logodds = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     params = pd.DataFrame({'log_mu': log_mu['NA18507'],
                            'log_phi': log_phi['NA18507'],
                            'logodds': logodds['NA18507']},
                           index=log_mu.index)
     params = params[params['log_mu'] > -10].sample(n=50)

     sample_sizes = np.geomspace(1e2, 1e5, 5).astype(int)
     log_fold_changes = np.log(np.geomspace(1.1, 2, 5))
     depths = np.geomspace(1e4, 1e6, 5)

     result = []
     for num_mols in depths:
       for log_fc in log_fold_changes:
         for num_samples in sample_sizes:
           umi = []
           for _, (log_mu, log_phi, logodds) in params.iterrows():
             umi.append(np.hstack([
               simulate(num_samples, size=num_mols, log_mu=log_mu, log_phi=log_phi, logodds=logodds)[0][:,0],
               simulate(num_samples, size=num_mols, log_mu=log_mu, log_phi=log_phi + log_fc, logodds=logodds)[0][:,0]
             ]))
           umi = pd.DataFrame(umi, index=params.index)
           onehot = np.zeros((umi.shape[1], 2))
           onehot[:num_samples,0] = 1
           onehot[num_samples:,1] = 1
           design = np.zeros((umi.shape[1], 1))
           size_factor = num_mols * np.ones((umi.shape[1], 1))
           T, P = lrt(umi, onehot, design, size_factor)
           result.append(pd.DataFrame({
             'num_mols': num_mols,
             'num_samples': num_samples,
             'log_fold_change': log_fc,
             'chi2': T,
             'logp': P}))
     pd.concat(result).to_csv('lrt-power.txt.gz', sep='\t', compression='gzip') 
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
     sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-lrt-power --output=tf-lrt-power.out
     #!/bin/bash
     source activate scqtl
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-power.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 46367157

   #+BEGIN_SRC sh
     sacct -j 46367157 -o Elapsed
   #+END_SRC

   #+RESULTS:
   :    Elapsed 
   : ---------- 
   :   06:59:58 
   :   06:59:58 
   :   06:59:58 

   Move the results to permanent storage.

   #+BEGIN_SRC sh
     rsync -FFau /scratch/midway2/aksarkar/singlecell/power/ /project2/mstephens/aksarkar/projects/singlecell-qtl/data/power/
   #+END_SRC

   #+RESULTS:

   Read the results.

   #+BEGIN_SRC ipython
     lrt_results = (pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/power/lrt-power.txt.gz', index_col=0)
                  .reindex())
     features = ['num_mols', 'num_samples', 'log_fold_change']
     lrt_power = (lrt_results
                  .groupby(features)
                  .apply(lambda x: (np.exp(x['logp']) < 0.05).sum() / x.shape[0])
                  .to_frame()
                  .reset_index())
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[3]:
   :END:

   Plot the results.

   #+BEGIN_SRC ipython :ipyfile figure/power.org/lrt-power.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(5.5, 3)

     num_samples = 100
     groups = sorted(set(lrt_power['num_mols']))
     for i, n in enumerate(groups):
       color = colorcet.cm['kbc']((i + .5) / len(groups))
       subset = lrt_power[np.logical_and(lrt_power['num_mols'] == n, lrt_power['num_samples'] == num_samples)]
       ax[0].plot(np.exp(subset['log_fold_change']), subset[0], lw=1, marker='.', ms=8, c=color, label='$10^{{{:.1f}}}$'.format(np.log(n) / np.log(10)))
     ax[0].legend(title='# molecules', frameon=False)
     ax[0].set_xlabel('Fold change in dispersion')
     ax[0].set_ylabel('Power at level 0.05')
     ax[0].set_title('100 cells')

     num_mols = 1e5
     grid = np.log(np.linspace(1.1, 2, 100))
     groups = sorted(set(lrt_power['num_samples']))
     for i, n in enumerate(groups):
       color = colorcet.cm['kgy']((i + .5) / len(groups))
       subset = lrt_power[np.logical_and(lrt_power['num_mols'] == num_mols, lrt_power['num_samples'] == n)]
       ax[1].plot(np.exp(subset['log_fold_change']), subset[0], lw=1, marker='.', ms=8, c=color, label='$10^{{{:.1f}}}$'.format(np.log(n) / np.log(10)))
     ax[1].legend(title='# samples', frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     ax[1].set_xlabel('Fold change in dispersion')
     ax[1].set_title('$10^5$ molecules')

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   [[file:figure/power.org/lrt-power.png]]
   :END:

* QTL discovery

  We assume the phenotype is generated from \(m\) causal effects (out of \(p\)
  variants) following a linear model:

  \[ \theta_i = \sum_j X_{ij} \beta_j + \epsilon_i \]

  \[ \mathbb{E}[x_i] = 0, \mathbb{V}[x_i] = 1 \]

  \[ \beta_j \sim N(0, 1) \text{ if \(j\) causal}\]

  \[ \epsilon_i \sim N(0, \mathbb{V[X \beta]} (1 / h^2 - 1)) \]

  #+NAME: eqtl-sim-impl
  #+BEGIN_SRC ipython
    def read_dosage(vcf, row, window=100000):
      records = list(vcf.query(row['#Chr'], row['start'] - window, row['start'] + window))
      if records:
        x = np.array([record[9:] for record in records]).astype(np.float).T
        x -= x.mean(axis=0)
        x /= x.std(axis=0)
        x = pd.DataFrame(x, columns=[record[2] for record in records])
        return x
      else:
        return None

    def read_vcf_geno(vcf, chrom, start, end):
      records = list(vcf.query(chrom, start, end))
      if records:
        x = (np.array([[_.split('/') for record in records for _ in record[9:]]])
             .reshape(len(records), -1, 2)
             .astype(float)
             .sum(axis=-1))
        return np.array(x).T
      else:
        return None

    def generate_pheno(x, pve, m=None):
      x = x[:,np.logical_and(x.mean(axis=0) / 2 > 0.05, x.std(axis=0) > 0)]
      n, p = x.shape
      if p == 0:
        return None
      if m is None:
        theta = np.random.normal(size=p)
      else:
        theta = np.zeros(p)
        theta[np.random.choice(p, m, replace=False)] = np.random.normal(size=m)
      x -= x.mean(axis=0)
      x /= x.std(axis=0)
      y = x.dot(theta)
      y += np.random.normal(scale=np.sqrt(y.var() * (1 / pve - 1)), size=n)
      y -= y.mean()
      y /= y.std()
      return y.reshape(-1, 1)

    def lm(x, y):
      n = y.shape[0]
      beta = x.T.dot(y) / n
      df = n - 1
      rss = ((y ** 2).sum() - beta ** 2 * n)
      sigma2 = rss / df
      se = np.sqrt(sigma2 / n)
      return beta, se

    _sf = st.chi2(1).sf

    def nominal_test(x, y):
      beta, se = lm(x, y)
      return _sf((beta / se) ** 2)

    def beta_llik(theta, x):
      return -st.beta.logpdf(x, *theta).mean()

    def permutation_test(x, y, num_trials=100):
      pval = nominal_test(x, y).min()
      null_pheno = y.copy()
      null_pvals = []
      for _ in range(num_trials):
        np.random.shuffle(null_pheno)
        null_pvals.append(nominal_test(x, null_pheno).min())
      null_pvals = np.array(null_pvals)
      theta = np.ones(2)
      opt = so.minimize(beta_llik, x0=theta, args=(null_pvals,))
      if opt.success:
        theta = opt.x
      else:
        # Method of moments
        theta = np.array([1, (1 / null_pvals.mean() - 1)])
        theta *= np.square(null_pvals.mean()) * ((1 - null_pvals.mean()) / null_pvals.var() - 1)
      return st.beta.cdf(pval, *theta)

    def evaluate(vcf, eqtls, num_individuals, num_causal, pve, num_genes=100):
      query = eqtls.sample(n=num_genes)
      result = []
      for _, record in query.iterrows():
        # Important: GEUVADIS chromosomes are coded 1-22
        x = read_vcf_geno(vcf, record['chr'][3:], record['start'] - 100000, record['start'] + 100000)
        if x is None:
          continue
        y = None
        while y is None:
          # Important: rejection sampling is needed to get a subset of individuals
          # with enough variable SNPs
          keep = np.random.choice(x.shape[0], num_individuals, replace=False)
          z = x[keep]
          y = generate_pheno(z, pve=pve, m=num_causal)
        pval = permutation_test(z, y)
        result.append({
          'gene': record.name,
          'num_individuals': z.shape[0],
          'num_snps': z.shape[1],
          'num_causal': num_causal,
          'pve': pve,
          'pval': pval})
      return pd.DataFrame.from_dict(result)
  #+END_SRC

  #+RESULTS: eqtl-sim-impl
  :RESULTS:
  # Out[154]:
  :END:

  QC the GEUVADIS genotypes.

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl --mem=4G --job-name=plink -a 1-22
    #!/bin/bash
    plink --memory 4000 --vcf /project/compbio/geuvadis/genotypes/GEUVADIS.chr${SLURM_ARRAY_TASK_ID}.*.vcf.gz --maf 0.05 --geno 0.01 --make-bed --out geuvadis-$SLURM_ARRAY_TASK_ID
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46479117

  Write out and index the QC'ed VCF.

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl --mem=4G --job-name=geuvadis --out=geuvadis.out
    #!/bin/bash
    set -e
    module load parallel
    source activate scqtl
    parallel echo geuvadis-{} ::: $(seq 1 22) >merge.txt
    plink --memory 4000 --merge-list merge.txt --recode vcf-iid --out geuvadis
    bgzip geuvadis.vcf
    tabix geuvadis.vcf.gz
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46404341

  Run the power calculation on 28 CPUs.

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/eqtl-power.py
    <<power-imports>>
    <<eqtl-sim-impl>>

    vcf = tabix.open('/scratch/midway2/aksarkar/singlecell/power/geuvadis.vcf.gz')
    # Restrict to genes where we previously successfully mapped eQTLs
    eqtls = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/pooled.txt.gz', sep=' ', index_col=0).dropna()

    args = [(vcf, eqtls, n, m, pve)
            for n in (53, 100, 200, 300, 400)
            for m in (1, None)
            for pve in np.linspace(0.01, 0.1, 10)]

    np.random.seed(0)
    result = [evaluate(*a) for a in args]
    pd.concat(result).to_csv('eqtl-power.txt.gz', compression='gzip', sep='\t')
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl -n1 -c28 --exclusive --time=3:00:00 --mem=16G --job-name=eqtl-power --output=eqtl-power.out
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/eqtl-power.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46541021

  Read the results.

  #+BEGIN_SRC ipython
    # Important: we used None for infinitesimal, which gets parsed as missing
    qtl_results = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/power/eqtl-power.txt.gz', index_col=0).fillna(-1)
    qtl_results['sig'] = qtl_results.apply(lambda x: x['pval'] < 0.05, axis=1).astype(int)
    qtl_power = (qtl_results
                 .groupby(['num_individuals', 'num_causal', 'pve'])['sig']
                 .agg([np.mean, np.std])
                 .reset_index()
                 .rename(columns={'mean': 'power', 'std': 'se'}))
    qtl_power['se'] /= 10
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[51]:
  :END:

  Plot the results.

  #+BEGIN_SRC ipython
    colorcet.cm.keys()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[23]:
  : odict_keys(['cyclic_grey_15_85_c0', 'cyclic_grey_15_85_c0_r', 'cyclic_grey_15_85_c0_s25', 'cyclic_grey_15_85_c0_s25_r', 'cyclic_mrybm_35_75_c68', 'cyclic_mrybm_35_75_c68_r', 'cyclic_mrybm_35_75_c68_s25', 'cyclic_mrybm_35_75_c68_s25_r', 'cyclic_mygbm_30_95_c78', 'cyclic_mygbm_30_95_c78_r', 'cyclic_mygbm_30_95_c78_s25', 'cyclic_mygbm_30_95_c78_s25_r', 'colorwheel', 'colorwheel_r', 'cyclic_wrwbw_40_90_c42', 'cyclic_wrwbw_40_90_c42_r', 'cyclic_wrwbw_40_90_c42_s25', 'cyclic_wrwbw_40_90_c42_s25_r', 'diverging_isoluminant_cjm_75_c23', 'diverging_isoluminant_cjm_75_c23_r', 'diverging_isoluminant_cjm_75_c24', 'diverging_isoluminant_cjm_75_c24_r', 'diverging_isoluminant_cjo_70_c25', 'diverging_isoluminant_cjo_70_c25_r', 'diverging_linear_bjr_30_55_c53', 'diverging_linear_bjr_30_55_c53_r', 'diverging_linear_bjy_30_90_c45', 'diverging_linear_bjy_30_90_c45_r', 'bjy', 'bjy_r', 'diverging_rainbow_bgymr_45_85_c67', 'diverging_rainbow_bgymr_45_85_c67_r', 'diverging_bkr_55_10_c35', 'diverging_bkr_55_10_c35_r', 'bkr', 'bkr_r', 'diverging_bky_60_10_c30', 'diverging_bky_60_10_c30_r', 'bky', 'bky_r', 'diverging_bwr_40_95_c42', 'diverging_bwr_40_95_c42_r', 'coolwarm', 'coolwarm_r', 'diverging_bwr_55_98_c37', 'diverging_bwr_55_98_c37_r', 'diverging_cwm_80_100_c22', 'diverging_cwm_80_100_c22_r', 'diverging_gkr_60_10_c40', 'diverging_gkr_60_10_c40_r', 'diverging_gwr_55_95_c38', 'diverging_gwr_55_95_c38_r', 'diverging_gwv_55_95_c39', 'diverging_gwv_55_95_c39_r', 'gwv', 'gwv_r', 'isoluminant_cgo_70_c39', 'isoluminant_cgo_70_c39_r', 'isoluminant_cgo_80_c38', 'isoluminant_cgo_80_c38_r', 'isolum', 'isolum_r', 'isoluminant_cm_70_c39', 'isoluminant_cm_70_c39_r', 'linear_bgy_10_95_c74', 'linear_bgy_10_95_c74_r', 'bgy', 'bgy_r', 'linear_bgyw_15_100_c67', 'linear_bgyw_15_100_c67_r', 'linear_bgyw_15_100_c68', 'linear_bgyw_15_100_c68_r', 'bgyw', 'bgyw_r', 'linear_blue_5_95_c73', 'linear_blue_5_95_c73_r', 'kbc', 'kbc_r', 'linear_blue_95_50_c20', 'linear_blue_95_50_c20_r', 'blues', 'blues_r', 'linear_bmw_5_95_c86', 'linear_bmw_5_95_c86_r', 'linear_bmw_5_95_c89', 'linear_bmw_5_95_c89_r', 'bmw', 'bmw_r', 'linear_bmy_10_95_c71', 'linear_bmy_10_95_c71_r', 'linear_bmy_10_95_c78', 'linear_bmy_10_95_c78_r', 'inferno', 'inferno_r', 'linear_gow_60_85_c27', 'linear_gow_60_85_c27_r', 'linear_gow_65_90_c35', 'linear_gow_65_90_c35_r', 'linear_green_5_95_c69', 'linear_green_5_95_c69_r', 'kgy', 'kgy_r', 'linear_grey_0_100_c0', 'linear_grey_0_100_c0_r', 'gray', 'gray_r', 'linear_grey_10_95_c0', 'linear_grey_10_95_c0_r', 'dimgray', 'dimgray_r', 'linear_kry_5_95_c72', 'linear_kry_5_95_c72_r', 'linear_kry_5_98_c75', 'linear_kry_5_98_c75_r', 'linear_kryw_0_100_c71', 'linear_kryw_0_100_c71_r', 'fire', 'fire_r', 'linear_kryw_5_100_c64', 'linear_kryw_5_100_c64_r', 'linear_kryw_5_100_c67', 'linear_kryw_5_100_c67_r', 'linear_ternary_blue_0_44_c57', 'linear_ternary_blue_0_44_c57_r', 'kb', 'kb_r', 'linear_ternary_green_0_46_c42', 'linear_ternary_green_0_46_c42_r', 'kg', 'kg_r', 'linear_ternary_red_0_50_c52', 'linear_ternary_red_0_50_c52_r', 'kr', 'kr_r', 'rainbow_bgyr_35_85_c72', 'rainbow_bgyr_35_85_c72_r', 'rainbow_bgyr_35_85_c73', 'rainbow_bgyr_35_85_c73_r', 'rainbow', 'rainbow_r', 'rainbow_bgyrm_35_85_c69', 'rainbow_bgyrm_35_85_c69_r', 'rainbow_bgyrm_35_85_c71', 'rainbow_bgyrm_35_85_c71_r'])
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/eqtl-power.png
    plt.clf()
    fig, ax = plt.subplots(1, 2, sharey=True)
    fig.set_size_inches(5.5, 3)
    groups = sorted(set(qtl_power['num_individuals']))
    for i, m in enumerate([1, -1]):
      for n in groups:
        subset = np.logical_and(qtl_power['num_individuals'] == n, qtl_power['num_causal'] == m)
        color = colorcet.cm['linear_kry_5_95_c72']((n - 53) / (max(groups) - 25))
        ax[i].plot(qtl_power.loc[subset, 'pve'],
                   qtl_power.loc[subset, 'power'],
                   marker='.', c=color, lw=1, ms=8, label=n)
        ax[i].fill_between(qtl_power.loc[subset, 'pve'],
                           qtl_power.loc[subset, 'power'] - 1.96 * qtl_power.loc[subset, 'se'],
                           qtl_power.loc[subset, 'power'] + 1.96 * qtl_power.loc[subset, 'se'],
                           color=color, alpha=0.1)
    ax[0].set_title('1 causal')
    ax[1].set_title('Infinitesimal')
    ax[0].set_ylabel('Power at level 0.05')
    ax[-1].legend(title='# individuals', frameon=False, loc='center left', bbox_to_anchor=(1, .5))

    for a in ax:
      a.set_xlabel('Proportion of variance explained')
      a.set_xlim(0.005, .105)

    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[35]:
  [[file:figure/power.org/eqtl-power.png]]
  :END:

* Distribution of effect sizes.

  Use ~ash~ to estimate the distribution of true effect sizes, borrowing
  information across genes.

  We need to compute regression standard errors ourselves.

  #+NAME: nominal-pass-def
  #+BEGIN_SRC ipython
    def run_nominal_pass(vcf, pheno, header, cov=None):
      # Annihilator matrix I - X X^+
      M = None
      result = []
      for _, record in pheno.iterrows():
        x = read_dosage(vcf, record)
        if x is not None:
          x.index = header
          y = record
          x, y = x.align(y, axis='index', join='inner')
          y = y.astype(float)
          if M is None and cov is not None:
            cov = cov.T.align(y, axis='index', join='inner')[0]
            C = np.array(cov - cov.mean(axis=0))
            M = np.eye(C.shape[0]) - C.dot(np.linalg.pinv(C))
          if M is not None:
            y = M.dot(y)
          y -= y.mean()
          y /= y.std()
          beta, se = lm(x, y)
          result.append(pd.DataFrame({'gene': record.name, 'beta': beta, 'se': se}))
      return pd.concat(result)
  #+END_SRC

  #+RESULTS: nominal-pass-def
  :RESULTS:
  # Out[24]:
  :END:

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py
    <<power-imports>>
    import argparse
    <<eqtl-sim-impl>>
    <<nominal-pass-def>>

    parser = argparse.ArgumentParser()
    parser.add_argument('--vcf', required=True)
    parser.add_argument('--pheno', required=True)
    parser.add_argument('--cov')
    parser.add_argument('--out', default=None)
    args = parser.parse_args()

    vcf = tabix.open(args.vcf)
    header = pd.read_table(args.vcf, skiprows=2, nrows=1, header=0).columns[9:]
    pheno = pd.read_table(args.pheno, index_col=4)
    if args.cov is not None:
      cov = pd.read_table(args.cov, sep=r'\s+', engine='python', index_col=0)
    else:
      cov = None

    result = run_nominal_pass(vcf, pheno, header, cov)
    if args.out is None:
      out = 'nominal-pass.txt.gz'
    else:
      out = args.out
    result.to_csv(out, sep='\t', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl -n1 -c28 --exclusive --job-name=nominal --out nominal.out --time=60:00
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /scratch/midway2/aksarkar/singlecell/scqtl-mapping/log_phi.bed.gz --out log_phi.txt.gz
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/mean.bed.gz --cov /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/mean-covars.txt --out mean.txt.gz
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk.bed.gz --cov /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk-covars.txt --out bulk.txt.gz
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 47404147

  #+BEGIN_SRC sh
    rsync -au /scratch/midway2/aksarkar/singlecell/power/ /project2/mstephens/aksarkar/projects/singlecell-qtl/data/power/
  #+END_SRC

  #+RESULTS:

  Compare standard error computations.

  #+BEGIN_SRC ipython
    def jacknife(x, y):
      n = y.shape[0]
      # pandas is too clever here
      xty = x.values.T * y.values
      beta = xty.sum(axis=1) / n
      se = np.array([(n * beta - xty[:,i]) / (n - 1) for i in range(y.shape[0])]).std(axis=0)
      return beta, se

    def bootstrap(x, y, b):
      n = y.shape[0]
      beta = x.values.T.dot(y.values) / n
      B = []
      for _ in range(b):
        index = np.random.choice(n, n, replace=True)
        B.append(x.iloc[index].values.T.dot(y.iloc[index]) / n)
      se = np.array(B).std(axis=0)
      return beta, se

    def run_se_pass(vcf, pheno, header):
      np.random.seed(0)
      result = []
      for _, record in pheno.iterrows():
        x = read_dosage(vcf, record)
        if x is not None:
          x.index = header
          y = record
          x, y = x.align(y, axis='index', join='inner')
          y = y.astype(float)
          beta0, se0 = lm(x, y)
          _, se1 = jacknife(x, y)
          _, se2 = bootstrap(x, y, 100)
          result.append(pd.DataFrame({'gene': record.name, 'beta0': beta0, 'se0': se0, 'se1': se1, 'se2': se2}))
      return pd.concat(result)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[57]:
  :END:

  #+BEGIN_SRC ipython
    header = pd.read_table('/scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz', skiprows=2, nrows=1, header=0).columns[9:]
    vcf = tabix.open('/scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz')
    pheno = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk.bed.gz', index_col=4)
    cov = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk-covars.txt', sep=r'\s+', engine='python', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[54]:
  :END:

  #+BEGIN_SRC ipython
    res = run_se_pass(vcf, pheno.sample(n=100), header)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/standard-errors.png
    ticks = ['Analytic', 'Jacknife', 'Bootstrap']
    plt.clf()
    fig, ax = plt.subplots(3, 3, sharex=True, sharey=True)
    fig.set_size_inches(8, 8)
    for y in range(3):
      for x in range(3):
        if y <= x:
          ax[y, x].set_axis_off()
        else:
          ax[y, x].scatter(res['se{}'.format(x)], res['se{}'.format(y)], s=2, c='k', alpha=0.25)
          ax[y, x].plot([0, .4], [0, .4], c='r', ls=':', lw=1)
    for y in range(3):
      ax[y, 0].set_ylabel(ticks[y])
    for x in range(3):
      ax[-1, x].set_xlabel(ticks[x])
    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[59]:
  [[file:figure/power.org/standard-errors.png]]
  :END:

  Downsample variants per gene like in [[http://dx.doi.org/10.1101/096552][Urbut et al 2016]] to make the estimation
  problem easier.

  #+NAME: downsample-impl
  #+BEGIN_SRC ipython
    def downsample(x, n):
      assert n >= 0
      if x.shape[0] >= n:
        return x.loc[np.random.choice(x.index, n, replace=False)]
      else:
        return x

    def fit_ash(summary_stats):
      summary_stats = pd.read_table(summary_stats).groupby('gene').apply(downsample, n=10)
      return ashr.ash(summary_stats['beta'], summary_stats['se'], method='fdr', mixcompdist='normal')
  #+END_SRC

  #+RESULTS: downsample-impl
  :RESULTS:
  # Out[56]:
  :END:

  Fit ~ash~.

  #+BEGIN_SRC ipython :async t
    ash_results = {x: fit_ash('/scratch/midway2/aksarkar/singlecell/power/{}.txt.gz'.format(x))
                   for x in ('log_phi', 'mean', 'bulk')}
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[60]:
  :END:

  Serialize the results.

  #+BEGIN_SRC ipython
    with open('/scratch/midway2/aksarkar/singlecell/power/ash-results.pkl', 'wb') as f:
      pickle.dump(ash_results, f)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[62]:
  :END:

  Read the results.

  #+BEGIN_SRC ipython
    with open('/scratch/midway2/aksarkar/singlecell/power/ash-results.pkl', 'rb') as f:
      ash_results = pickle.load(f)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[104]:
  :END:

  Plot the estimated distributions of effect sizes.

  #+BEGIN_SRC ipython :ipyfile figure/power.org/ash-fitted-g.png
    plt.clf()
    plt.gcf().set_size_inches(3, 3)
    grid = np.linspace(-.5, .5, num=1000)
    for k, c, l in zip(ash_results, ['r', 'b', 'k'], ['Dispersion', 'Mean', 'Bulk']):
      y = np.array(ashr.cdf_ash(ash_results[k], grid).rx2('y')).ravel()
      plt.plot(grid, y, c=c, lw=1, label=l)
    plt.xlabel('Effect size')
    plt.ylabel('Cumulative density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[63]:
  : Text(0,0.5,'Cumulative density')
  [[file:figure/power.org/ash-fitted-g.png]]
  :END:

  Run nominal passes on variance phenotypes.

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl -n1 -c28 --exclusive --job-name=nominal --out nominal.out --time=60:00
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /scratch/midway2/aksarkar/singlecell/scqtl-mapping/variance.bed.gz --cov /scratch/midway2/aksarkar/singlecell/scqtl-mapping/variance-covars.txt --out variance.txt.gz
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/fano.bed.gz --out fano.txt.gz
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/cv.bed.gz --out cv.txt.gz
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 47405002

  Fit ~mash~.

  #+BEGIN_SRC ipython
    def fit_mash(df, g=None):
      betahat = df.iloc[:,:df.shape[1] // 2]
      se = df.iloc[:,df.shape[1] // 2:]
      data = mashr.mash_set_data(betahat, se)
      U = mashr.cov_canonical(data)
      if g is None:
        V = mashr.estimate_null_correlation(data)
        data = mashr.mash_set_data(betahat, se, V=V)
        return mashr.mash(data, U, verbose=True)
      else:
        return mashr.mash(data, verbose=True, fixg=True, g=g)

    def train_test_split(betahat, se, train_subsample=10, test_thresh=9):
      df = pd.DataFrame(np.concatenate([betahat, se], axis=-1))
      training_set = df.groupby(stats[0]['gene']).apply(downsample, n=train_subsample)
      test_set = df[(np.square(betahat / se) > test_thresh).any(axis=1)]
      return training_set, test_set

    def get_pairwise_sharing(stats):
      common = functools.reduce(lambda x, y: x.merge(y, on=['id', 'gene'], how='inner')[['id', 'gene']], stats)
      betahat = np.vstack([s.merge(common, on=['id', 'gene'], how='inner')['beta'] for s in stats]).T
      se = np.vstack([s.merge(common, on=['id', 'gene'], how='inner')['se'] for s in stats]).T
      training_set, test_set = train_test_split(betahat, se)
      mash_result0 = fit_mash(training_set)
      mash_result1 = fit_mash(test_set, g=mash_result0.rx2('fitted_g'))
      return np.array(mashr.get_pairwise_sharing(mash_result1))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[11]:
  :END:

  #+BEGIN_SRC ipython
    stats = [pd.read_table('/scratch/midway2/aksarkar/singlecell/power/{}.txt.gz'.format(f)).rename(columns={'Unnamed: 0': 'id'})
             for f in ('bulk', 'log_mu','variance','fano','cv')]
    # Bulk gene names need to be munged
    stats[0]['gene'] = [x.split('.')[0] for x in stats[0]['gene']]
    # CV effect sizes are expected to be anti-correlated to all others
    stats[-1]['beta'] *= -1
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[5]:
  :END:

  #+BEGIN_SRC ipython
    res = get_pairwise_sharing(stats)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[12]:
  #+BEGIN_EXAMPLE
    array([[1.        , 0.94283656, 0.94073996, 0.94026625, 0.94153343],
    [0.94283656, 1.        , 0.99865977, 0.9935249 , 0.97356074],
    [0.94073996, 0.99865977, 1.        , 0.9997103 , 0.9710184 ],
    [0.94026625, 0.9935249 , 0.9997103 , 1.        , 0.97188708],
    [0.94153343, 0.97356074, 0.9710184 , 0.97188708, 1.        ]])
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    pd.options.display.float_format = '{:.3g}'.format
    ticks = ['Bulk', 'Abundance', 'Variance', 'Fano', 'CV']
    pd.DataFrame(100 * res, columns=ticks, index=ticks)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[16]:
  #+BEGIN_EXAMPLE
    Bulk  Abundance  Variance  Fano   CV
    Bulk        100       94.3      94.1    94 94.2
    Abundance  94.3        100      99.9  99.4 97.4
    Variance   94.1       99.9       100   100 97.1
    Fano         94       99.4       100   100 97.2
    CV         94.2       97.4      97.1  97.2  100
  #+END_EXAMPLE
  :END:

* Compare experimental variance to biological variance

  Let \hat{\theta}_i be an estimate of some phenotype \(\theta_i\). Assume:

  \[ \hat{\theta}_i = \theta_i + u_i \]

  where \(u_i\) is a random effect capturing experimental noise, finite
  sampling variance, and estimator sampling variance.

  Above, we estimated the scale of the finite sampling variance and estimator
  sampling variance. Here, we relate it to the genetic variance.

  #+CALL: recode-impl()

  #+RESULTS:
  :RESULTS:
  # Out[10]:
  :END:

  Extract the genotypes and UMI counts.

  #+NAME: extract-impl
  #+BEGIN_SRC ipython
    def parse_vcf_dosage(record):
      geno = [float(g) for g in record[9:]]
      return pd.Series(geno)

    def extract_geno(qtls, dosages):
      header = pd.read_table(dosages, skiprows=2, nrows=1, header=0).columns[9:]
      genotypes = tabix.open(dosages)
      X = np.round(
        qtls
        .apply(lambda x: parse_vcf_dosage(next(genotypes.query(x['chr'], int(x['var_start']) - 1, int(x['var_start'])))), axis=1)
        .rename(columns={i: ind for i, ind in enumerate(header)}))
      return X

    def extract_counts(qtls, counts):
      X = pd.concat(
        [chunk.align(qtls, axis='index', join='inner')[0]
         for chunk in pd.read_table(counts, index_col=0, chunksize=1000)])
      return X
  #+END_SRC

  #+RESULTS: extract-impl
  :RESULTS:
  # Out[4]:
  :END:

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  :END:

  Read the data.

  #+NAME: read-qtls
  #+BEGIN_SRC ipython
    qtls = (pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/mean.txt.gz', sep=' ', index_col=0)
            .sort_values('p_beta')
            .head(n=100))
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
  #+END_SRC

  #+RESULTS: read-qtls
  :RESULTS:
  # Out[84]:
  :END:

  #+RESULTS:
  :RESULTS:
  # Out[75]:
  :END:

  Extract the subset we want.

  #+NAME: extract-counts
  #+BEGIN_SRC ipython
    counts = extract_counts(qtls, '/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz')
    annotations = annotations.loc[keep_samples.values.ravel()].reset_index(drop=True)
    counts = counts.loc[:,keep_samples.values.ravel()]
    geno = extract_geno(qtls, '/scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz')
    geno = geno.loc[:,list(set(annotations['chip_id']))]
  #+END_SRC

  #+RESULTS: extract-counts
  :RESULTS:
  # Out[85]:
  :END:

  Bootstrap the subset.

  #+NAME: bootstrap-impl
  #+BEGIN_SRC ipython
    def bootstrap(counts, annotations):
      idx = np.hstack({k: np.random.choice(g, size=len(g))
                       for k, g in annotations.groupby('chip_id').groups.items()}.values())
      return counts.iloc[:,idx], annotations.iloc[idx]
  #+END_SRC

  #+RESULTS: bootstrap-impl
  :RESULTS:
  # Out[108]:
  :END:

  Recover the derived mean and variance.

  #+CALL: point-gamma-moments()

  #+RESULTS:
  :RESULTS:
  # Out[6]:
  :END:

  Partition the sum of squares.

  #+BEGIN_SRC ipython
    def ss(y):
      return np.square(y - y.mean()).sum()

    def partition(x, y):
      total = ss(y)
      within = y.groupby(x.values).apply(ss).sum()
      between = total - within
      return total, between, within

    X, Y = geno.align(mean_by_ind, join='inner')
    var_comps = pd.DataFrame([partition(X.loc[i], Y.loc[i]) for i in Y.index])
    var_comps.index = X.index
    var_comps.columns = ['total', 'between', 'within']
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[95]:
  :END:

  Look at the ANOVA for weak eQTLs.

  #+BEGIN_SRC ipython
    var_comps.tail()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[102]:
  #+BEGIN_EXAMPLE
    total       between        within
    gene
    ENSG00000163961  3.681769e-10  9.298483e-11  2.751920e-10
    ENSG00000151806  1.530030e-10  5.271346e-11  1.002895e-10
    ENSG00000116260  3.854593e-10  6.561917e-11  3.198401e-10
    ENSG00000109775  3.234854e-10  1.171794e-10  2.063060e-10
    ENSG00000164758  2.571498e-10  7.245402e-11  1.846958e-10
  #+END_EXAMPLE
  :END:

  Compare against the estimated sampling variance from the simulation.

  #+BEGIN_SRC ipython
    data[np.logical_and(data['num_samples'] == 95, data['num_mols'] == 114026)]['latent_mean_hat'].describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[105]:
  #+BEGIN_EXAMPLE
    count    8.000000e+01
    mean     9.264424e-09
    std      2.667586e-08
    min      1.169715e-11
    25%      1.685388e-10
    50%      9.926134e-10
    75%      4.742833e-09
    max      1.403023e-07
    Name: latent_mean_hat, dtype: float64
  #+END_EXAMPLE
  :END:

  Estimate bootstrap SEs on a GPU.

  #+NAME: tf-zinb.py
  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb-se.py
    <<zinb-imports>>
    <<tf-imports>>
    import tabix
    import scipy.linalg as sl

    <<tf-zinb-impl>>
    <<recode-impl>>
    <<extract-impl>>
    <<bootstrap-impl>>

    <<read-qtls>>
    <<extract-counts>>

    results = []
    for trial in range(100):
      C, A = bootstrap(counts, annotations)
      onehot = recode(annotations, 'chip_id')
      chip = recode(annotations, 'experiment')
      chip -= chip.mean(axis=0)

      results.append(fit(
        umi=C.values.T.astype(np.float32),
        onehot=onehot.astype(np.float32),
        design=chip.astype(np.float32),
        size_factor=annotations['mol_hs'].astype(np.float32).values.reshape(-1, 1),
        learning_rate=5e-2,
        max_epochs=4000))

    estimates = {
      'mean': np.array([np.exp(r[0] - np.log1p(np.exp(r[2]))) for r in results]),
      'disp': np.array([r[1] for r in results])
    }

    for k in estimates:
      (pd.DataFrame(estimates[k].var(axis=0).T, columns=sorted(set(annotations['chip_id'])), index=counts.index)
       .to_csv('/scratch/midway2/aksarkar/singlecell/power/{}-se.txt.gz'.format(k), sep='\t', compression='gzip'))
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --job-name=tf-zinb-se --out=tf-zinb-se.out
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb-se.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 47382783

  Read the results.

  #+BEGIN_SRC ipython
    mean_sampling_var = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/mean-se.txt.gz', index_col=0)
    disp_sampling_var = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/disp-se.txt.gz', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[5]:
  :END:

  Estimate the reduction in effective PVE for eQTLs.

  #+BEGIN_SRC ipython
    S, M = mean_sampling_var.align(mean_by_ind, join='inner', axis='index')
    pd.DataFrame({'prop': S.mean(axis=1) / M.var(axis=1), 'factor': M.var(axis=1) / np.ma.masked_less((M.var(axis=1) - S.mean(axis=1)).values, 0)}).describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[41]:
  #+BEGIN_EXAMPLE
    factor        prop
    count  75.000000  100.000000
    mean    3.365234    0.698366
    std     4.746555    0.407146
    min     1.109361    0.098580
    25%     1.458076    0.380033
    50%     2.043664    0.626091
    75%     3.229525    0.978929
    max    34.946118    2.004026
  #+END_EXAMPLE
  :END:

  Estimate how many more cells would be required so the median factor is 1.1.

  \[ (\sigma^2_y + \sigma^2_u) / \sigma^2_y \leq 1.1 \]

  \[ \sigma^2_u \leq .1 \sigma^2 y \]

  #+BEGIN_SRC ipython
    (S.mean(axis=1) / (.1 * np.ma.masked_less((M.var(axis=1) - S.mean(axis=1)).values, 0))).describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[42]:
  #+BEGIN_EXAMPLE
    count     75.000000
    mean      23.652338
    std       47.465547
    min        1.093611
    25%        4.580756
    50%       10.436642
    75%       22.295255
    max      339.461176
    dtype: float64
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    (M.var(axis=1) / np.ma.masked_less((M.var(axis=1) - S.mean(axis=1) / 2.82).values, 0)).describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[43]:
  #+BEGIN_EXAMPLE
    count    100.000000
    mean       1.398247
    std        0.388211
    min        1.036224
    25%        1.155754
    50%        1.285377
    75%        1.531795
    max        3.455993
    dtype: float64
  #+END_EXAMPLE
  :END:

  Do the same for dQTLs.

  #+BEGIN_SRC ipython
    S, M = disp_sampling_var.align(log_phi, join='inner', axis='index')
    pd.DataFrame({'prop': S.mean(axis=1) / M.var(axis=1), 'factor': M.var(axis=1) / np.ma.masked_less((M.var(axis=1) - S.mean(axis=1)).values, 0)}).describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[44]:
  #+BEGIN_EXAMPLE
    factor        prop
    count  92.000000  100.000000
    mean    2.346648    0.383063
    std     3.382984    0.342855
    min     1.041327    0.039686
    25%     1.117332    0.108174
    50%     1.281986    0.228490
    75%     1.914243    0.636540
    max    23.984483    1.541178
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    (S.mean(axis=1) / (.1 * np.ma.masked_less((M.var(axis=1) - S.mean(axis=1)).values, 0))).describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[45]:
  #+BEGIN_EXAMPLE
    count     92.000000
    mean      13.466479
    std       33.829841
    min        0.413266
    25%        1.173316
    50%        2.819856
    75%        9.142426
    max      229.844830
    dtype: float64
  #+END_EXAMPLE
  :END:
* Distribution of PVE

  We can use shrunk marginal effect size estimates to bound the proportion of
  phenotypic variance explained. We claim:

  \[ h^2 \geq \arg\max_j E[\beta_j \mid \hat\beta]^2 \]

  *Frequentist argument.* If the maximizer \(j\) is causal, this is a lower
  bound on PVE because \(h^2 = \sum_k \beta_k^2\) and other variants \(k\)
  could be causal, and because the estimate is shrunk towards zero.

  If it is not causal, then the true marginal effect size is \(\sum_i R_{ij}
  \beta_i\), where \(R_{ij} = \mathrm{Corr}(X_i, X_j)\). \(R_ij \leq 1\), so
  this is still a lower bound

  *Bayesian argument.* 

  \[ h^2 \geq \beta_j^2 \text{ for all \(j\)} \]

  \[ E[h^2 \mid \hat\beta] \geq E[\beta_j^2 \mid \hat\beta] \text{ for all \(j\)} \]

  \[ E[h^2 \mid \hat\beta] \geq \arg\max_j E[\beta_j^2 \mid \hat\beta] \]

  #+NAME: get-pve-impl
  #+BEGIN_SRC ipython
    def get_pve(summary_stats, ash_result):
      res = ashr.ash(summary_stats['beta'], summary_stats['se'], fixg=True,
                     g=ash_result.rx2('fitted_g'))
      pve = np.square(pd.Series(np.square(ashr.get_sd(res)) + ashr.get_pm(res),
                                index=summary_stats['gene']))
      return pve.groupby(level=0).agg(max)
  #+END_SRC

  #+RESULTS: get-pve-impl
  :RESULTS:
  # Out[64]:
  :END:

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/estimate-pve.py
    <<power-imports>>
    <<downsample-impl>>
    <<get-pve-impl>>
    np.random.seed(0)
    stats = {x: (pd.read_table('/scratch/midway2/aksarkar/singlecell/power/{}.txt.gz'.format(x))
                 .groupby('gene')
                 .agg(lambda x: x.loc[x['beta'].idxmax()])
                 .reset_index())
             for x in ('log_phi', 'log_mu', 'bulk')}
    # Munge gene names in bulk summary statistics
    stats['bulk']['gene'] = [x.split('.')[0] for x in stats['bulk']['gene']]
    with open('/scratch/midway2/aksarkar/singlecell/power/ash-results.pkl', 'rb') as f:
      ash_results = pickle.load(f)
    (pd.DataFrame({k: get_pve(v, ash_results[k]) for k, v in stats.items()})
     .to_csv('estimated-pve.txt.gz', sep='\t', compression='gzip'))
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl --time=10:00 --mem=4G -n1 -c28 --exclusive --job-name estimate-pve --out estimate-pve.out
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/estimate-pve.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 47408762

  Read the results.

  #+BEGIN_SRC ipython
    estimated_pve = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/estimated-pve.txt.gz', index_col=0)
    estimated_pve.columns = ['Bulk', 'Mean', 'Dispersion']
    estimated_pve = estimated_pve[['Dispersion', 'Mean', 'Bulk']]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[67]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/estimated-pve.png
    plt.clf()
    plt.gcf().set_size_inches(3, 3)
    M = 0.1
    grid = np.linspace(0, M, 100)
    for k, c in zip(estimated_pve, ['r', 'b', 'k']):
      f = st.gaussian_kde(estimated_pve[k].dropna())
      plt.plot(grid, f(grid), c=c, lw=1, label=k)
      plt.fill_between(grid, f(grid), color=c, alpha=0.1)
    plt.legend(frameon=False)
    plt.xlim(0, M)
    plt.ylim(0, plt.ylim()[1])
    plt.xlabel('Proportion of variance explained')
    plt.ylabel('Density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[68]:
  : Text(0,0.5,'Density')
  [[file:figure/power.org/estimated-pve.png]]
  :END:

  Plot the expected density of mean expression PVE assuming measurement noise
  from the simulation.

  #+BEGIN_SRC ipython
    (estimated_pve['Mean'] / estimated_pve['Bulk']).describe()  
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[42]:
  #+BEGIN_EXAMPLE
    count    8462.000000
    mean        1.587743
    std         3.075250
    min         0.002746
    25%         0.268501
    50%         0.672904
    75%         1.638187
    max        62.349105
    dtype: float64
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/effective-pve.png
    plt.clf()
    plt.gcf().set_size_inches(3, 3)
    M = 0.1
    grid = np.linspace(0, M, 100)
    f0 = st.gaussian_kde(estimated_pve['Bulk'].dropna())
    plt.plot(grid, f0(grid), c='b', lw=1, label='Bulk')
    plt.fill_between(grid, f0(grid), color='b', alpha=0.1)

    f1 = st.gaussian_kde(.67 * estimated_pve['Bulk'].dropna())
    plt.plot(grid, f1(grid), c='c', lw=1, label='Bulk + noise')
    plt.fill_between(grid, f1(grid), color='c', alpha=0.1)

    f2 = st.gaussian_kde(estimated_pve['Mean'].dropna())
    plt.plot(grid, f2(grid), c='k', lw=1, label='SC')
    plt.fill_between(grid, f2(grid), color='k', alpha=0.1)

    plt.legend(frameon=False)
    plt.xlim(0, M)
    plt.ylim(0, plt.ylim()[1])
    plt.xlabel('Proportion of variance explained')
    plt.ylabel('Density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[43]:
  : Text(0,0.5,'Density')
  [[file:figure/power.org/effective-pve.png]]
  :END:

  Estimate the proportion of large effect dQTLs.

  #+BEGIN_SRC ipython
    f = st.gaussian_kde(estimated_pve['Dispersion'].dropna())
    si.quad(f, .01, 1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[84]:
  : (0.06961646614211667, 4.412386762494795e-09)
  :END:

  Estimate the same for eQTLs.

  #+BEGIN_SRC ipython
    f = st.gaussian_kde(estimated_pve['Bulk'].dropna())
    si.quad(f, 0.01, 1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[85]:
  : (0.7296000671917134, 1.95708035203864e-12)
  :END:

  Compare the distribution of PVE for iPSCs against DGN whole blood ([[http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006423][Wheeler et
  al 2016]]). The results are [[https://github.com/WheelerLab/GenArchDB][available on github]].

  #+BEGIN_SRC ipython
    import sqlite3
    with sqlite3.connect('/project2/mstephens/aksarkar/.local/src/GenArchDB/genarch.db') as conn:
      dgn_pve = pd.read_sql('select * from results where tissue == "DGN-WB";', conn)
      gtex_pve = pd.read_sql('select * from results where tissue == "WholeBlood_TW" and pve != "NA";', conn)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[27]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/ipsc-vs-dgn-pve.png
    plt.clf()
    plt.gcf().set_size_inches(3, 3)
    M = 0.1
    grid = np.linspace(0, 1, 100)
    f0 = st.gaussian_kde(estimated_pve['Bulk'].dropna())
    plt.plot(grid, f0(grid), c='k', lw=1, label='iPSC')
    plt.fill_between(grid, f0(grid), color='k', alpha=0.1)

    f1 = st.gaussian_kde(dgn_pve['pve'].dropna())
    plt.plot(grid, f1(grid), c='g', lw=1, label='Whole blood (DGN)')
    plt.fill_between(grid, f1(grid), color='g', alpha=0.1)

    f2 = st.gaussian_kde(gtex_pve['pve'].dropna())
    plt.plot(grid, f2(grid), c='c', lw=1, label='Whole blood (GTEx)')
    plt.fill_between(grid, f2(grid), color='c', alpha=0.1)

    plt.legend(frameon=False)
    plt.xlim(0, 1)
    plt.ylim(0, plt.ylim()[1])
    plt.xlabel('Proportion of variance explained')
    plt.ylabel('Density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[29]:
  : Text(0,0.5,'Density')
  [[file:figure/power.org/ipsc-vs-dgn-pve.png]]
  :END:

  Compute the expected power over the estimated distribution of PVE.

  #+BEGIN_SRC ipython
    def average_power(g, f):
      x = g['pve']
      return si.trapz(g['power'] * f(x), x)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[46]:
  :END:

  Estimate the distribution of effective PVE by correcting for the median
  reduction in PVE (at the current experiment size), and then assuming a large
  enough experiment to only reduce effective PVE by 10%..

  #+BEGIN_SRC ipython
    f = st.gaussian_kde(.9 * estimated_pve['Bulk'].dropna())
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[49]:
  :END:

  Estimate the average power as a function of the number of individuals and
  number of causal variants.

  #+BEGIN_SRC ipython
    (qtl_power.groupby(['num_individuals', 'num_causal']).apply(average_power, f=f)).reset_index()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[52]:
  #+BEGIN_EXAMPLE
    num_individuals  num_causal         0
    0               53        -1.0  0.067524
    1               53         1.0  0.078898
    2              100        -1.0  0.104900
    3              100         1.0  0.114457
    4              200        -1.0  0.152676
    5              200         1.0  0.201122
    6              300        -1.0  0.250451
    7              300         1.0  0.280708
    8              400        -1.0  0.276784
    9              400         1.0  0.303322
  #+END_EXAMPLE
  :END:

  Repeat the analysis for dQTLs.

  #+BEGIN_SRC ipython
    (qtl_power.groupby(['num_individuals', 'num_causal']).apply(average_power, f=st.gaussian_kde(estimated_pve['Dispersion'].dropna() * .9 * 1.28))).reset_index()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[54]:
  #+BEGIN_EXAMPLE
    num_individuals  num_causal         0
    0               53        -1.0  0.008282
    1               53         1.0  0.007657
    2              100        -1.0  0.014693
    3              100         1.0  0.015392
    4              200        -1.0  0.016902
    5              200         1.0  0.018266
    6              300        -1.0  0.029801
    7              300         1.0  0.031696
    8              400        -1.0  0.030865
    9              400         1.0  0.037992
  #+END_EXAMPLE
  :END:

* Estimate the error variance

  The single cell experiment size determines the standard error of the
  estimator, which can be thought of as measurement error.

  \[ \hat\theta \sim N(\theta, \sigma^2) \]

  This is the same as changing effective PVE:

  \[ h^2_{\mathrm{eff}} = \frac{h^2}{1 + \sigma^2} \]

  We estimate the standard error as a function of the experiment size from the
  simulation.

  #+BEGIN_SRC ipython
    sim_results = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/simulation.txt.gz', index_col=0)
    sim_results['latent_mean'] = np.exp(sim_results['log_mu'] - np.log1p(np.exp(sim_results['logodds'])))
    sim_results['latent_mean_hat'] = np.exp(sim_results['log_mu_hat'] - np.log1p(np.exp(sim_results['logodds_hat'])))
    sim_results['latent_var'] = np.exp(2 * sim_results['log_mu'] + sim_results['log_phi'] - np.log1p(np.exp(sim_results['logodds']))) + np.exp(-np.log1p(np.exp(sim_results['logodds'])) - np.log1p(np.exp(-sim_results['logodds'])) + 2 * sim_results['log_mu'])
    sim_results['latent_var_hat'] = np.exp(2 * sim_results['log_mu_hat'] + sim_results['log_phi_hat'] - np.log1p(np.exp(sim_results['logodds_hat']))) + np.exp(-np.log1p(np.exp(sim_results['logodds_hat'])) - np.log1p(np.exp(-sim_results['logodds_hat'])) + 2 * sim_results['log_mu_hat'])
    mu_pass = sim_results['log_mu'] > -10
    pi_pass = sim_results['logodds'] <= 0
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[14]:
  :END:

  Restrict the analysis to \(\ln\mu > -10, \mathrm{logit}(\pi) < 0\), because
  this is the part of the parameter space we can reliably estimate in.

  #+BEGIN_SRC ipython
    data = sim_results[np.logical_and(mu_pass, pi_pass)].groupby(['num_samples', 'num_mols', 'latent_mean', 'latent_var'])[['latent_mean_hat', 'latent_var_hat']].agg(np.var).reset_index()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[15]:
  :END:

  For \(x_1, \ldots, x_n \sim N(\mu, \sigma^2)\), we know:

  \[ \bar{x} \sim N(\mu, \sigma^2 / n) \]

  So a priori, we might expect the sampling variance of the single cell latent
  mean to decrease as \(1 / n\).

  Similarly, we know:

  \[ (n - 1)S^2 / \sigma^2 \sim \chi^2(n - 1) \]

  Therefore,

  \[ V[S^2] = 2 (n - 1) sigma^4 / (n - 1)^2 \]

  And a priori, we might also expect the sampling variance of the single cell
  latent variance to decrease as \(1 / n\).

  From the parametric simulation, we can estimate these relationships directly
  by fitting multiplicative models.

  #+BEGIN_SRC ipython
    def mlm(x, y):
      x -= x.mean(axis=0)
      y -= y.mean()
      beta = np.linalg.pinv(x).dot(y)
      rss = np.square(y - x.dot(beta)).sum()
      sigma2 = rss / (y.shape[0] - 1)
      se = np.sqrt(sigma2 / np.einsum('ij,ij->j', x, x))
      return beta, se

    def ci(beta, se):
      return np.array([beta + 1.96 * se, beta - 1.96 * se]).reshape(2, -1).T
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[172]:
  :END:

  #+BEGIN_SRC ipython
    ci(*mlm(np.log(data[['num_samples', 'num_mols', 'latent_mean', 'latent_var']]), np.log(data['latent_mean_hat'])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[173]:
  #+BEGIN_EXAMPLE
    array([[-0.9984868 , -1.03276978],
    [-0.01299114, -0.08496202],
    [-0.01580837, -0.07270787],
    [ 1.01267559,  0.98504366]])
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    ci(*mlm(np.log(data[['num_samples', 'num_mols', 'latent_mean', 'latent_var']]), np.log(data['latent_var_hat'])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[174]:
  #+BEGIN_EXAMPLE
    array([[-0.94347526, -1.02803879],
    [-0.07134441, -0.24887015],
    [ 0.26219289,  0.12184272],
    [ 1.92936491,  1.86120709]])
  #+END_EXAMPLE
  :END:

