#+TITLE: Power to detect QTLs in single cell data
#+SETUPFILE: setup.org
#+OPTIONS: toc:t

* Introduction

  We [[file:qtl-mapping.org][previously found]] that our study lost power to detect eQTLs, and was
  underpowered to directly detect dispersion-QTLs.

  Here, we estimate power to detect eQTLs and dispersion-QTLs as a function of:

  - number of cells per individual
  - number of molecules per cell
  - number of individuals

* Setup                                                            :noexport:
 
  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "~/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="8G", venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 46541646

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])

    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+NAME: power-imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import pickle
    import rpy2.robjects.numpy2ri
    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri
    import scipy.optimize as so
    import scipy.stats as st
    import tabix

    rpy2.robjects.numpy2ri.activate()
    rpy2.robjects.pandas2ri.activate()
    ashr = rpy2.robjects.packages.importr('ashr')
  #+END_SRC

  #+RESULTS: power-imports
  :RESULTS:
  # Out[2]:
  :END:

* Differential dispersion

  Perform a nested model comparison for each gene \(k\), comparing the model:

  \[ r_{ijk} \sim \mathrm{ZINB}(\pi_{ik}, \mu_{ik}, \phi_{ik}) \]

  against the model:

  \[ r_{ijk} \sim \mathrm{ZINB}(\pi_{ik}, \mu_{ik}, \phi_{i}) \]

  #+NAME: lrt-impl
  #+BEGIN_SRC ipython :eval never
    def lrt(umi, onehot, design, size_factor):
      _, _, _, llik0 = fit(
        umi=umi.values.T.astype(np.float32),
        onehot=onehot.astype(np.float32),
        design=design.astype(np.float32),
        size_factor=size_factor.astype(np.float32),
        fit_null=True,
        return_llik=True,
        learning_rate=5e-2,
        max_epochs=4000)
      _, _, _, llik1 = fit(
        umi=umi.values.T.astype(np.float32),
        onehot=onehot.astype(np.float32),
        design=design.astype(np.float32),
        size_factor=size_factor.astype(np.float32),
        return_llik=True,
        learning_rate=5e-2,
        max_epochs=4000)
      T = 2 * (llik1 - llik0)
      return T, st.chi2(1).logsf(T)
  #+END_SRC

** Null calibration via parametric bootstrap

   Sample null data from the model, using empirical estimates of the parameters
   in the observed data.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null-parametric.py
     <<zinb-imports>>
     <<tf-imports>>
     <<tf-zinb-impl>>
     <<lrt-impl>>
     <<sim-impl>>

     log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     log_phi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', index_col=0, sep=' ')
     logodds = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     params = pd.DataFrame({'log_mu': log_mu['NA18507'],
                            'log_phi': log_phi['NA18507'],
                            'logodds': logodds['NA18507']},
                           index=log_mu.index)
     params = params[params['log_mu'] > -10]
     n = 100
     umi = pd.DataFrame([simulate(2 * n, size=1e5, log_mu=log_mu, log_phi=log_phi, logodds=logodds)[0][:,0]
                         for _, (log_mu, log_phi, logodds) in params.iterrows() if log_mu > -10], index=params.index)
     onehot = np.zeros((2 * n, 2))
     onehot[:n,0] = 1
     onehot[n:,1] = 1
     design = np.zeros((2 * n, 1))
     size_factor = 1e5 * np.ones((2 * n, 1))
     T, P = lrt(umi, onehot, design, size_factor)
     pd.DataFrame({'chi2': T, 'logp': P}, index=umi.index).to_csv('null-calibration-p.txt.gz', sep='\t', compression='gzip')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
     sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-lrt-null --output=tf-lrt-null-parametric.out
     #!/bin/bash
     source activate scqtl
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null-parametric.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 46322399

   Read the results.

   #+BEGIN_SRC ipython
     null_lrt = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/null-calibration-p.txt.gz')
     N = null_lrt.shape[0]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[2]:
   :END:

   Report how many genes were simulated.

   #+BEGIN_SRC ipython
     N
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[3]:
   : 1832
   :END:

   Estimate bootstrap CIs for the quantiles.

   #+BEGIN_SRC ipython
     B = np.sort(st.chi2(1).rvs((N, 100)), axis=0)
     ci = np.percentile(B, [2.5, 97.5], axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[18]:
   :END:

   Plot the QQ-plot.

   #+BEGIN_SRC ipython :ipyfile figure/power.org/null-calibration-p.png
     plt.clf()
     plt.gcf().set_size_inches(4, 4)
     grid = st.chi2(1).ppf(np.linspace(0, 1 - 1 / N, N))
     plt.scatter(grid, null_lrt['chi2'].sort_values(), c='k', s=2)
     plt.fill_between(grid, ci[0], ci[1], color='k', alpha=0.1)
     lim = [0, 1.01 * grid.max()]
     plt.plot(lim, lim, c='r', lw=1)
     plt.xlim(lim)
     plt.xlabel('Expected $\chi^2$ statistic')
     _ = plt.ylabel('Observed $\chi^2$ statistic')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[40]:
   [[file:figure/power.org/null-calibration-p.png]]
   :END:

** Null calibration via nonparametric bootstrap                    :noexport:

   Assuming the true single cells all came from one individual, generate new
   single cells by sampling counts for each gene iid. with replacement. This
   method assumes genes are independent, which is justified only if we perform
   analysis one gene at a time.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null.py
     <<zinb-imports>>
     <<tf-imports>>
     <<tf-zinb-impl>>
     <<lrt-impl>>

     def downsample_counts(umi, size_factor, num_mols):
       """Return a downsampled matrix of UMI counts

       This is needed so that every cell has the same number of molecules in
       expectation

       """
       p = num_mols / size_factor
       assert (p <= 1).all()
       return umi.apply(lambda x: np.random.binomial(x, p), axis=1)

     def sample_cells(umi, n=1):
       """Return a sampled matrix of UMI counts

       Assume the input matrix is UMI for a single individal. Sample gene counts
       i.i.d. from the empirical distribution of counts

       """
       result = np.zeros((umi.shape[0], n))
       for i, row in enumerate(umi.values):
         result[i] = np.random.choice(row, size=n, replace=True)
       return pd.DataFrame(result, index=umi.index)

     <<read-data-qc-impl>>
     log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     keep_cells = np.logical_and(
       annotations['chip_id'] == 'NA18507',
       annotations['mol_hs'] > 1e5,
     )
     umi = umi.loc[log_mu['NA18507'] >= -10,keep_cells.values]

     # Generate counts at equal number of molecules (to simplify bootstrapping)
     np.random.seed(1)
     umi = downsample_counts(umi, annotations['mol_hs'], 1e5)

     # Generate two groups under the null
     n = 100
     umi = sample_cells(umi, n=2 * n)
     onehot = np.zeros((2 * n, 2))
     onehot[:n,0] = 1
     onehot[n:,1] = 1
     design = np.zeros((2 * n, 1))
     size_factor = num_mols.min() * np.ones((2 * n, 1))

     T, P = lrt(umi, onehot, design, size_factor)
     pd.DataFrame({'chi2': T, 'logp': P}, index=umi.index).to_csv('null-calibration.txt.gz', sep='\t', compression='gzip')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
     sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-lrt-null --output=tf-lrt-null.out
     #!/bin/bash
     source activate scqtl
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-null.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 46323093

   Plot the QQ-plot.

   #+BEGIN_SRC ipython :ipyfile figure/power.org/null-calibration.png
     null_lrt = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/null-calibration.txt.gz')
     N = null_lrt.shape[0]
     plt.clf()
     plt.gcf().set_size_inches(4, 4)
     plt.scatter(st.chi2(1).ppf(np.linspace(0, 1 - 1 / N, N)), null_lrt['chi2'].sort_values(), c='k', s=2, alpha=0.25)
     lim = [0, 1.1 * null_lrt['chi2'].max()]
     plt.plot(lim, lim, c='r', ls=':', lw=1)
     plt.xlim(lim)
     plt.ylim(lim)
     plt.xlabel('Expected chi-square statistic')
     _ = plt.ylabel('Observed chi-square statistic')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   [[file:figure/power.org/null-calibration.png]]
   :END:

** Power

   Sample from the assumed model.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-power.py
     <<zinb-imports>>
     <<tf-imports>>
     <<tf-zinb-impl>>
     <<lrt-impl>>
     <<sim-impl>>

     log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     log_phi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', index_col=0, sep=' ')
     logodds = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
     params = pd.DataFrame({'log_mu': log_mu['NA18507'],
                            'log_phi': log_phi['NA18507'],
                            'logodds': logodds['NA18507']},
                           index=log_mu.index)
     params = params[params['log_mu'] > -10].sample(n=50)

     sample_sizes = np.geomspace(1e2, 1e5, 5).astype(int)
     log_fold_changes = np.log(np.geomspace(1.1, 2, 5))
     depths = np.geomspace(1e4, 1e6, 5)

     result = []
     for num_mols in depths:
       for log_fc in log_fold_changes:
         for num_samples in sample_sizes:
           umi = []
           for _, (log_mu, log_phi, logodds) in params.iterrows():
             umi.append(np.hstack([
               simulate(num_samples, size=num_mols, log_mu=log_mu, log_phi=log_phi, logodds=logodds)[0][:,0],
               simulate(num_samples, size=num_mols, log_mu=log_mu, log_phi=log_phi + log_fc, logodds=logodds)[0][:,0]
             ]))
           umi = pd.DataFrame(umi, index=params.index)
           onehot = np.zeros((umi.shape[1], 2))
           onehot[:num_samples,0] = 1
           onehot[num_samples:,1] = 1
           design = np.zeros((umi.shape[1], 1))
           size_factor = num_mols * np.ones((umi.shape[1], 1))
           T, P = lrt(umi, onehot, design, size_factor)
           result.append(pd.DataFrame({
             'num_mols': num_mols,
             'num_samples': num_samples,
             'log_fold_change': log_fc,
             'chi2': T,
             'logp': P}))
     pd.concat(result).to_csv('lrt-power.txt.gz', sep='\t', compression='gzip') 
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
     sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-lrt-power --output=tf-lrt-power.out
     #!/bin/bash
     source activate scqtl
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-lrt-power.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 46367157

   #+BEGIN_SRC sh
     sacct -j 46367157 -o Elapsed
   #+END_SRC

   #+RESULTS:
   :    Elapsed 
   : ---------- 
   :   06:59:58 
   :   06:59:58 
   :   06:59:58 

   Move the results to permanent storage.

   #+BEGIN_SRC sh
     rsync -FFau /scratch/midway2/aksarkar/singlecell/power /project2/mstephens/aksarkar/projects/singlecell-qtl/data/power/
   #+END_SRC

   #+RESULTS:

   Read the results.

   #+BEGIN_SRC ipython
     lrt_results = (pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/power/lrt-power.txt.gz', index_col=0)
                  .reindex())
     features = ['num_mols', 'num_samples', 'log_fold_change']
     lrt_power = (lrt_results
                  .groupby(features)
                  .apply(lambda x: (np.exp(x['logp']) < 0.05).sum() / x.shape[0])
                  .to_frame()
                  .reset_index())
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[3]:
   :END:

   Plot the results.

   #+BEGIN_SRC ipython :ipyfile figure/power.org/lrt-power.png
     plt.clf()
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(5.5, 3)

     num_samples = 100
     groups = sorted(set(lrt_power['num_mols']))
     for i, n in enumerate(groups):
       color = colorcet.cm['kbc']((i + .5) / len(groups))
       subset = lrt_power[np.logical_and(lrt_power['num_mols'] == n, lrt_power['num_samples'] == num_samples)]
       ax[0].plot(np.exp(subset['log_fold_change']), subset[0], lw=1, marker='.', ms=8, c=color, label='$10^{{{:.1f}}}$'.format(np.log(n) / np.log(10)))
     ax[0].legend(title='# molecules', frameon=False)
     ax[0].set_xlabel('Fold change in dispersion')
     ax[0].set_ylabel('Power at level 0.05')
     ax[0].set_title('100 cells')

     num_mols = 1e5
     grid = np.log(np.linspace(1.1, 2, 100))
     groups = sorted(set(lrt_power['num_samples']))
     for i, n in enumerate(groups):
       color = colorcet.cm['kgy']((i + .5) / len(groups))
       subset = lrt_power[np.logical_and(lrt_power['num_mols'] == num_mols, lrt_power['num_samples'] == n)]
       ax[1].plot(np.exp(subset['log_fold_change']), subset[0], lw=1, marker='.', ms=8, c=color, label='$10^{{{:.1f}}}$'.format(np.log(n) / np.log(10)))
     ax[1].legend(title='# samples', frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     ax[1].set_xlabel('Fold change in dispersion')
     ax[1].set_title('$10^5$ molecules')

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   [[file:figure/power.org/lrt-power.png]]
   :END:

* QTL discovery

  We assume the phenotype is generated from \(m\) causal effects (out of \(p\)
  variants) following a linear model:

  \[ \theta_i = \sum_j X_{ij} \beta_j + \epsilon_i \]

  \[ \mathbb{E}[x_i] = 0, \mathbb{V}[x_i] = 1 \]

  \[ \beta_j \sim N(0, 1) \text{ if \(j\) causal}\]

  \[ \epsilon_i \sim N(0, \mathbb{V[X \beta]} (1 / h^2 - 1)) \]

  #+NAME: eqtl-sim-impl
  #+BEGIN_SRC ipython
    def read_dosage(vcf, row, window=100000):
      records = list(vcf.query(row['#Chr'], row['start'] - window, row['start'] + window))
      if records:
        x = np.array([record[9:] for record in records]).astype(np.float).T
        x -= x.mean(axis=0)
        x /= x.std(axis=0)
        x = pd.DataFrame(x, columns=[record[2] for record in records])
        return x
      else:
        return None

    def read_vcf_geno(vcf, chrom, start, end):
      records = list(vcf.query(chrom, start, end))
      if records:
        x = (np.array([[_.split('/') for record in records for _ in record[9:]]])
             .reshape(len(records), -1, 2)
             .astype(float)
             .sum(axis=-1))
        return np.array(x).T
      else:
        return None

    def generate_pheno(x, pve, m=None):
      x = x[:,np.logical_and(x.mean(axis=0) / 2 > 0.05, x.std(axis=0) > 0)]
      n, p = x.shape
      if p == 0:
        return None
      if m is None:
        theta = np.random.normal(size=p)
      else:
        theta = np.zeros(p)
        theta[np.random.choice(p, m, replace=False)] = np.random.normal(size=m)
      x -= x.mean(axis=0)
      x /= x.std(axis=0)
      y = x.dot(theta)
      y += np.random.normal(scale=np.sqrt(y.var() * (1 / pve - 1)), size=n)
      y -= y.mean()
      y /= y.std()
      return y.reshape(-1, 1)

    def lm(x, y):
      n = y.shape[0]
      beta = x.T.dot(y) / n
      df = n - 1
      rss = ((y ** 2).sum() - beta ** 2 * n)
      sigma2 = rss / df
      se = np.sqrt(sigma2 / n)
      return beta, se

    _sf = st.chi2(1).sf

    def nominal_test(x, y):
      beta, se = lm(x, y)
      return _sf((beta / se) ** 2)

    def beta_llik(theta, x):
      return -st.beta.logpdf(x, *theta).mean()

    def permutation_test(x, y, num_trials=100):
      pval = nominal_test(x, y).min()
      null_pheno = y.copy()
      null_pvals = []
      for _ in range(num_trials):
        np.random.shuffle(null_pheno)
        null_pvals.append(nominal_test(x, null_pheno).min())
      null_pvals = np.array(null_pvals)
      theta = np.ones(2)
      opt = so.minimize(beta_llik, x0=theta, args=(null_pvals,))
      if opt.success:
        theta = opt.x
      else:
        # Method of moments
        theta = np.array([1, (1 / null_pvals.mean() - 1)])
        theta *= np.square(null_pvals.mean()) * ((1 - null_pvals.mean()) / null_pvals.var() - 1)
      return st.beta.cdf(pval, *theta)

    def evaluate(vcf, eqtls, num_individuals, num_causal, pve, num_genes=100):
      query = eqtls.sample(n=num_genes)
      result = []
      for _, record in query.iterrows():
        # Important: GEUVADIS chromosomes are coded 1-22
        x = read_vcf_geno(vcf, record['chr'][3:], record['start'] - 100000, record['start'] + 100000)
        if x is None:
          continue
        y = None
        while y is None:
          # Important: rejection sampling is needed to get a subset of individuals
          # with enough variable SNPs
          keep = np.random.choice(x.shape[0], num_individuals, replace=False)
          z = x[keep]
          y = generate_pheno(z, pve=pve, m=num_causal)
        pval = permutation_test(z, y)
        result.append({
          'gene': record.name,
          'num_individuals': z.shape[0],
          'num_snps': z.shape[1],
          'num_causal': num_causal,
          'pve': pve,
          'pval': pval})
      return pd.DataFrame.from_dict(result)
  #+END_SRC

  #+RESULTS: eqtl-sim-impl
  :RESULTS:
  # Out[44]:
  :END:

  QC the GEUVADIS genotypes.

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl --mem=4G --job-name=plink -a 1-22
    #!/bin/bash
    plink --memory 4000 --vcf /project/compbio/geuvadis/genotypes/GEUVADIS.chr${SLURM_ARRAY_TASK_ID}.*.vcf.gz --maf 0.05 --geno 0.01 --make-bed --out geuvadis-$SLURM_ARRAY_TASK_ID
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46479117

  Write out and index the QC'ed VCF.

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl --mem=4G --job-name=geuvadis --out=geuvadis.out
    #!/bin/bash
    set -e
    module load parallel
    source activate scqtl
    parallel echo geuvadis-{} ::: $(seq 1 22) >merge.txt
    plink --memory 4000 --merge-list merge.txt --recode vcf-iid --out geuvadis
    bgzip geuvadis.vcf
    tabix geuvadis.vcf.gz
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46404341

  Run the power calculation on 28 CPUs.

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/eqtl-power.py
    <<power-imports>>
    <<eqtl-sim-impl>>

    vcf = tabix.open('/scratch/midway2/aksarkar/singlecell/power/geuvadis.vcf.gz')
    # Restrict to genes where we previously successfully mapped eQTLs
    eqtls = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/pooled.txt.gz', sep=' ', index_col=0).dropna()

    args = [(vcf, eqtls, n, m, pve)
            for n in (53, 100, 200, 300, 400)
            for m in (1, None)
            for pve in np.linspace(0.01, 0.1, 10)]

    np.random.seed(0)
    result = [evaluate(*a) for a in args]
    pd.concat(result).to_csv('eqtl-power.txt.gz', compression='gzip', sep='\t')
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl -n1 -c28 --exclusive --time=3:00:00 --mem=16G --job-name=eqtl-power --output=eqtl-power.out
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/eqtl-power.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46541021

  Read the results.

  #+BEGIN_SRC ipython
    # Important: we used None for infinitesimal, which gets parsed as missing
    qtl_results = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/power/eqtl-power.txt.gz', index_col=0).fillna(-1)
    qtl_results['sig'] = qtl_results.apply(lambda x: x['pval'] < 0.05, axis=1).astype(int)
    qtl_power = (qtl_results
                 .groupby(['num_individuals', 'num_causal', 'pve'])['sig']
                 .agg([np.mean, np.std])
                 .reset_index()
                 .rename(columns={'mean': 'power', 'std': 'se'}))
    qtl_power['se'] /= 10
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[15]:
  :END:

  Plot the results.

  #+BEGIN_SRC ipython
    colorcet.cm.keys()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[23]:
  : odict_keys(['cyclic_grey_15_85_c0', 'cyclic_grey_15_85_c0_r', 'cyclic_grey_15_85_c0_s25', 'cyclic_grey_15_85_c0_s25_r', 'cyclic_mrybm_35_75_c68', 'cyclic_mrybm_35_75_c68_r', 'cyclic_mrybm_35_75_c68_s25', 'cyclic_mrybm_35_75_c68_s25_r', 'cyclic_mygbm_30_95_c78', 'cyclic_mygbm_30_95_c78_r', 'cyclic_mygbm_30_95_c78_s25', 'cyclic_mygbm_30_95_c78_s25_r', 'colorwheel', 'colorwheel_r', 'cyclic_wrwbw_40_90_c42', 'cyclic_wrwbw_40_90_c42_r', 'cyclic_wrwbw_40_90_c42_s25', 'cyclic_wrwbw_40_90_c42_s25_r', 'diverging_isoluminant_cjm_75_c23', 'diverging_isoluminant_cjm_75_c23_r', 'diverging_isoluminant_cjm_75_c24', 'diverging_isoluminant_cjm_75_c24_r', 'diverging_isoluminant_cjo_70_c25', 'diverging_isoluminant_cjo_70_c25_r', 'diverging_linear_bjr_30_55_c53', 'diverging_linear_bjr_30_55_c53_r', 'diverging_linear_bjy_30_90_c45', 'diverging_linear_bjy_30_90_c45_r', 'bjy', 'bjy_r', 'diverging_rainbow_bgymr_45_85_c67', 'diverging_rainbow_bgymr_45_85_c67_r', 'diverging_bkr_55_10_c35', 'diverging_bkr_55_10_c35_r', 'bkr', 'bkr_r', 'diverging_bky_60_10_c30', 'diverging_bky_60_10_c30_r', 'bky', 'bky_r', 'diverging_bwr_40_95_c42', 'diverging_bwr_40_95_c42_r', 'coolwarm', 'coolwarm_r', 'diverging_bwr_55_98_c37', 'diverging_bwr_55_98_c37_r', 'diverging_cwm_80_100_c22', 'diverging_cwm_80_100_c22_r', 'diverging_gkr_60_10_c40', 'diverging_gkr_60_10_c40_r', 'diverging_gwr_55_95_c38', 'diverging_gwr_55_95_c38_r', 'diverging_gwv_55_95_c39', 'diverging_gwv_55_95_c39_r', 'gwv', 'gwv_r', 'isoluminant_cgo_70_c39', 'isoluminant_cgo_70_c39_r', 'isoluminant_cgo_80_c38', 'isoluminant_cgo_80_c38_r', 'isolum', 'isolum_r', 'isoluminant_cm_70_c39', 'isoluminant_cm_70_c39_r', 'linear_bgy_10_95_c74', 'linear_bgy_10_95_c74_r', 'bgy', 'bgy_r', 'linear_bgyw_15_100_c67', 'linear_bgyw_15_100_c67_r', 'linear_bgyw_15_100_c68', 'linear_bgyw_15_100_c68_r', 'bgyw', 'bgyw_r', 'linear_blue_5_95_c73', 'linear_blue_5_95_c73_r', 'kbc', 'kbc_r', 'linear_blue_95_50_c20', 'linear_blue_95_50_c20_r', 'blues', 'blues_r', 'linear_bmw_5_95_c86', 'linear_bmw_5_95_c86_r', 'linear_bmw_5_95_c89', 'linear_bmw_5_95_c89_r', 'bmw', 'bmw_r', 'linear_bmy_10_95_c71', 'linear_bmy_10_95_c71_r', 'linear_bmy_10_95_c78', 'linear_bmy_10_95_c78_r', 'inferno', 'inferno_r', 'linear_gow_60_85_c27', 'linear_gow_60_85_c27_r', 'linear_gow_65_90_c35', 'linear_gow_65_90_c35_r', 'linear_green_5_95_c69', 'linear_green_5_95_c69_r', 'kgy', 'kgy_r', 'linear_grey_0_100_c0', 'linear_grey_0_100_c0_r', 'gray', 'gray_r', 'linear_grey_10_95_c0', 'linear_grey_10_95_c0_r', 'dimgray', 'dimgray_r', 'linear_kry_5_95_c72', 'linear_kry_5_95_c72_r', 'linear_kry_5_98_c75', 'linear_kry_5_98_c75_r', 'linear_kryw_0_100_c71', 'linear_kryw_0_100_c71_r', 'fire', 'fire_r', 'linear_kryw_5_100_c64', 'linear_kryw_5_100_c64_r', 'linear_kryw_5_100_c67', 'linear_kryw_5_100_c67_r', 'linear_ternary_blue_0_44_c57', 'linear_ternary_blue_0_44_c57_r', 'kb', 'kb_r', 'linear_ternary_green_0_46_c42', 'linear_ternary_green_0_46_c42_r', 'kg', 'kg_r', 'linear_ternary_red_0_50_c52', 'linear_ternary_red_0_50_c52_r', 'kr', 'kr_r', 'rainbow_bgyr_35_85_c72', 'rainbow_bgyr_35_85_c72_r', 'rainbow_bgyr_35_85_c73', 'rainbow_bgyr_35_85_c73_r', 'rainbow', 'rainbow_r', 'rainbow_bgyrm_35_85_c69', 'rainbow_bgyrm_35_85_c69_r', 'rainbow_bgyrm_35_85_c71', 'rainbow_bgyrm_35_85_c71_r'])
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/eqtl-power.png
    plt.clf()
    fig, ax = plt.subplots(1, 2, sharey=True)
    fig.set_size_inches(5.5, 3)
    groups = sorted(set(qtl_power['num_individuals']))
    for i, m in enumerate([1, -1]):
      for n in groups:
        subset = np.logical_and(qtl_power['num_individuals'] == n, qtl_power['num_causal'] == m)
        color = colorcet.cm['linear_kry_5_95_c72']((n - 53) / (max(groups) - 25))
        ax[i].plot(qtl_power.loc[subset, 'pve'],
                   qtl_power.loc[subset, 'power'],
                   marker='.', c=color, lw=1, ms=8, label=n)
        ax[i].fill_between(qtl_power.loc[subset, 'pve'],
                           qtl_power.loc[subset, 'power'] - 1.96 * qtl_power.loc[subset, 'se'],
                           qtl_power.loc[subset, 'power'] + 1.96 * qtl_power.loc[subset, 'se'],
                           color=color, alpha=0.1)
    ax[0].set_title('1 causal')
    ax[1].set_title('Infinitesimal')
    ax[0].set_ylabel('Power at level 0.05')
    ax[-1].legend(title='# individuals', frameon=False, loc='center left', bbox_to_anchor=(1, .5))

    for a in ax:
      a.set_xlabel('Proportion of variance explained')
      a.set_xlim(0.005, .105)

    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[35]:
  [[file:figure/power.org/eqtl-power.png]]
  :END:

  The single cell experiment size determines the standard error of the
  estimator, which can be thought of as measurement error.

  \[ \hat\theta \sim N(\theta, \sigma^2) \]

  This is the same as changing effective PVE:

  \[ h^2_{\mathrm{eff}} = \frac{h^2}{1 + \sigma^2} \]

  We estimate the standard error as a function of the experiment size from the
  simulation.

  #+BEGIN_SRC ipython
    sim_results = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/simulation.txt.gz', index_col=0)
    mu_pass = sim_results['log_mu'] > -10
    pi_pass = sim_results['logodds'] <= 0
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[28]:
  :END:

  #+BEGIN_SRC ipython
    sim_results[pi_pass].groupby(['num_samples', 'num_mols', 'log_mu']).apply(lambda x: pd.Series(1 / (1 + np.var(x['log_mu_hat'])))).reset_index()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[98]:
  #+BEGIN_EXAMPLE
    num_samples  num_mols  log_mu         0
    0            95    114026     -12  0.943735
    1            95    114026     -11  0.975032
    2            95    114026     -10  0.984130
    3            95    114026      -9  0.988944
    4            95    114026      -8  0.990610
    5            95    114026      -7  0.995300
    6            95    114026      -6  0.992922
    7         10000    114026     -12  0.970102
    8         10000    114026     -11  0.980160
    9         10000    114026     -10  0.992239
    10        10000    114026      -9  0.995632
    11        10000    114026      -8  0.998242
    12        10000    114026      -7  0.999954
    13        10000    114026      -6  0.999969
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    sim_results[np.logical_and(mu_pass, pi_pass)].groupby(['num_samples', 'num_mols', 'log_phi']).apply(lambda x: 1 / (1 + np.var(x['log_phi_hat']))).reset_index()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[46]:
  #+BEGIN_EXAMPLE
    num_samples  num_mols  log_phi         0
    0           95    114026       -4  0.867270
    1           95    114026       -3  0.916724
    2           95    114026       -2  0.950310
    3           95    114026       -1  0.969675
    4           95    114026        0  0.904745
    5        10000    114026       -4  0.996827
    6        10000    114026       -3  0.999260
    7        10000    114026       -2  0.999752
    8        10000    114026       -1  0.999300
    9        10000    114026        0  0.933499
  #+END_EXAMPLE
  :END:

* Distribution of effect sizes.

  Use ~ash~ to estimate the distribution of true effect sizes, borrowing
  information across genes.

  We need to compute regression standard errors ourselves.

  #+NAME: nominal-pass-def
  #+BEGIN_SRC ipython
    def run_nominal_pass(vcf, pheno, header, cov=None):
      # Annihilator matrix I - X X^+
      M = None
      result = []
      for _, record in pheno.iterrows():
        x = read_dosage(vcf, record)
        if x is not None:
          x.index = header
          y = record
          x, y = x.align(y, axis='index', join='inner')
          y = y.astype(float)
          if M is None and cov is not None:
            cov = cov.T.align(y, axis='index', join='inner')[0]
            C = np.array(cov - cov.mean(axis=0))
            M = np.eye(C.shape[0]) - C.dot(np.linalg.pinv(C))
          if M is not None:
            y = M.dot(y)
          y -= y.mean()
          y /= y.std()
          beta, se = lm(x, y)
          result.append(pd.DataFrame({'gene': record.name, 'beta': beta, 'se': se}))
      return pd.concat(result)
  #+END_SRC

  #+RESULTS: nominal-pass-def
  :RESULTS:
  # Out[24]:
  :END:

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py
    <<power-imports>>
    import argparse
    <<eqtl-sim-impl>>
    <<nominal-pass-def>>

    parser = argparse.ArgumentParser()
    parser.add_argument('--vcf', required=True)
    parser.add_argument('--pheno', required=True)
    parser.add_argument('--cov')
    parser.add_argument('--out', default=None)
    args = parser.parse_args()

    vcf = tabix.open(args.vcf)
    header = pd.read_table(args.vcf, skiprows=2, nrows=1, header=0).columns[9:]
    pheno = pd.read_table(args.pheno, index_col=4)
    if args.cov is not None:
      cov = pd.read_table(args.cov, sep=r'\s+', engine='python', index_col=0)
    else:
      cov = None

    result = run_nominal_pass(vcf, pheno, header, cov)
    if args.out is None:
      out = 'nominal-pass.txt.gz'
    else:
      out = args.out
    result.to_csv(out, sep='\t', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl -n1 -c28 --exclusive --job-name=nominal --out nominal.out --time=60:00
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /scratch/midway2/aksarkar/singlecell/scqtl-mapping/log_phi.bed.gz --out log_phi.txt.gz
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/log_mu.bed.gz --cov /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/log_mu-covars.txt --out log_mu.txt.gz
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/qtl-nominal.py --vcf /scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz --pheno /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk.bed.gz --cov /project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk-covars.txt --out bulk.txt.gz
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46516912

  Compare standard error computations.

  #+BEGIN_SRC ipython
    def jacknife(x, y):
      n = y.shape[0]
      # pandas is too clever here
      xty = x.values.T * y.values
      beta = xty.sum(axis=1) / n
      se = np.array([(n * beta - xty[:,i]) / (n - 1) for i in range(y.shape[0])]).std(axis=0)
      return beta, se

    def bootstrap(x, y, b):
      n = y.shape[0]
      beta = x.values.T.dot(y.values) / n
      B = []
      for _ in range(b):
        index = np.random.choice(n, n, replace=True)
        B.append(x.iloc[index].values.T.dot(y.iloc[index]) / n)
      se = np.array(B).std(axis=0)
      return beta, se

    def run_se_pass(vcf, pheno, header):
      np.random.seed(0)
      result = []
      for _, record in pheno.iterrows():
        x = read_dosage(vcf, record)
        if x is not None:
          x.index = header
          y = record
          x, y = x.align(y, axis='index', join='inner')
          y = y.astype(float)
          beta0, se0 = lm(x, y)
          _, se1 = jacknife(x, y)
          _, se2 = bootstrap(x, y, 100)
          result.append(pd.DataFrame({'gene': record.name, 'beta0': beta0, 'se0': se0, 'se1': se1, 'se2': se2}))
      return pd.concat(result)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[57]:
  :END:

  #+BEGIN_SRC ipython
    header = pd.read_table('/scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz', skiprows=2, nrows=1, header=0).columns[9:]
    vcf = tabix.open('/scratch/midway2/aksarkar/singlecell/scqtl-mapping/yri-120-dosages.vcf.gz')
    pheno = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk.bed.gz', index_col=4)
    cov = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-mapping/bulk-covars.txt', sep=r'\s+', engine='python', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[54]:
  :END:

  #+BEGIN_SRC ipython
    res = run_se_pass(vcf, pheno.sample(n=100), header)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/standard-errors.png
    ticks = ['Analytic', 'Jacknife', 'Bootstrap']
    plt.clf()
    fig, ax = plt.subplots(3, 3, sharex=True, sharey=True)
    fig.set_size_inches(8, 8)
    for y in range(3):
      for x in range(3):
        if y <= x:
          ax[y, x].set_axis_off()
        else:
          ax[y, x].scatter(res['se{}'.format(x)], res['se{}'.format(y)], s=2, c='k', alpha=0.25)
          ax[y, x].plot([0, .4], [0, .4], c='r', ls=':', lw=1)
    for y in range(3):
      ax[y, 0].set_ylabel(ticks[y])
    for x in range(3):
      ax[-1, x].set_xlabel(ticks[x])
    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[59]:
  [[file:figure/power.org/standard-errors.png]]
  :END:

  Downsample variants per gene like in [[http://dx.doi.org/10.1101/096552][Urbut et al 2016]] to make the estimation
  problem easier.

  #+NAME: downsample-impl
  #+BEGIN_SRC ipython
    def downsample(x, n):
      assert n >= 0
      if x.shape[0] >= n:
        return x.loc[np.random.choice(x.index, n, replace=False),['beta', 'se']]
      else:
        return x.loc[:,['beta', 'se']]

    def fit_ash(summary_stats):
      summary_stats = pd.read_table(summary_stats).groupby('gene').apply(downsample, n=10)
      return ashr.ash(summary_stats['beta'], summary_stats['se'], method='fdr', mixcompdist='normal')
  #+END_SRC

  #+RESULTS: downsample-impl
  :RESULTS:
  # Out[36]:
  :END:

  #+RESULTS:
  :RESULTS:
  # Out[60]:
  :END:

  Fit ~ash~.

  #+BEGIN_SRC ipython
    ash_results = {x: fit_ash('/scratch/midway2/aksarkar/singlecell/power/{}.txt.gz'.format(x))
                   for x in ('log_phi', 'log_mu', 'bulk')}
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  Serialize the results.

  #+BEGIN_SRC ipython
    with open('/scratch/midway2/aksarkar/singlecell/power/ash-results.pkl', 'wb') as f:
      pickle.dump(ash_results, f)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[39]:
  :END:

  Read the results.

  #+BEGIN_SRC ipython
    with open('/scratch/midway2/aksarkar/singlecell/power/ash-results.pkl', 'rb') as f:
      ash_results = pickle.load(f)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[95]:
  :END:

  Plot the estimated distributions of effect sizes.

  #+BEGIN_SRC ipython :ipyfile figure/power.org/ash-fitted-g.png
    plt.clf()
    plt.gcf().set_size_inches(3, 3)
    grid = np.linspace(-.5, .5, num=1000)
    for k, c, l in zip(ash_results, ['r', 'b', 'k'], ['Dispersion', 'Abundance', 'Bulk']):
      plt.plot(grid, np.array(ashr.cdf_ash(ash_results[k], grid).rx2('y')).ravel(), c=c, lw=1, label=l)
    plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
    plt.xlabel('Effect size')
    plt.ylabel('Cumulative density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[52]:
  : Text(0,0.5,'Cumulative density')
  [[file:figure/power.org/ash-fitted-g.png]]
  :END:

* Distribution of PVE

  We can use shrunk marginal effect size estimates to bound the proportion of
  phenotypic variance explained. We claim:

  \[ h^2 \geq \arg\max_j E[\beta_j \mid \hat\beta]^2 \]

  *Frequentist argument.* If the maximizer \(j\) is causal, this is a lower
  bound on PVE because \(h^2 = \sum_k \beta_k^2\) and other variants \(k\)
  could be causal, and because the estimate is shrunk towards zero.

  If it is not causal, then the true marginal effect size is \(R_ij beta_i\),
  where \(i\) is causal and \(R_ij = \mathrm{Corr}(X_i, X_j)\). \(R_ij <= 1\),
  so this is still a lower bound

  *Bayesian argument.* 

  \[ h^2 \geq \beta_j^2 \text{ for all \(j\)} \]

  \[ E[h^2 \mid \hat\beta] \geq E[\beta_j^2 \mid \hat\beta] \text{ for all \(j\)} \]

  \[ E[h^2 \mid \hat\beta] \geq \arg\max_j E[\beta_j^2 \mid \hat\beta] \]

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/estimate-pve.py
    <<power-imports>>
    <<downsample-impl>>

    def get_pve(summary_stats, ash_result):
      res = ashr.ash(summary_stats['beta'], summary_stats['se'], fixg=True,
                     g=ash_result.rx2('fitted_g'))
      pve = np.square(pd.Series(np.square(ashr.get_sd(res)) + ashr.get_pm(res),
                                index=summary_stats['gene']))
      return pve.groupby(level=0).agg(max)

    np.random.seed(0)
    stats = {x: (pd.read_table('/scratch/midway2/aksarkar/singlecell/power/{}.txt.gz'.format(x))
                 .groupby('gene')
                 .agg(lambda x: x.loc[x['beta'].idxmax()])
                 .reset_index())
             for x in ('log_phi', 'log_mu', 'bulk')}
    # Munge gene names in bulk summary statistics
    stats['bulk']['gene'] = [x.split('.')[0] for x in stats['bulk']['gene']]
    with open('/scratch/midway2/aksarkar/singlecell/power/ash-results.pkl', 'rb') as f:
      ash_results = pickle.load(f)
    (pd.DataFrame({k: get_pve(v, ash_results[k]) for k, v in stats.items()})
     .to_csv('estimated-pve.txt.gz', sep='\t', compression='gzip'))
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/power/
    sbatch --partition=broadwl --time=10:00 --mem=4G -n1 -c28 --exclusive --job-name estimate-pve --out estimate-pve.out
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/estimate-pve.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 46541723

  Read the results.

  #+BEGIN_SRC ipython
    estimated_pve = pd.read_table('/scratch/midway2/aksarkar/singlecell/power/estimated-pve.txt.gz', index_col=0)
    estimated_pve.columns = ['Bulk', 'Abundance', 'Dispersion']
    estimated_pve = estimated_pve[['Dispersion', 'Abundance', 'Bulk']]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[54]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/power.org/estimated-pve.png
    plt.clf()
    plt.gcf().set_size_inches(3, 3)
    M = 0.1
    grid = np.linspace(0, M, 100)
    for k, c in zip(estimated_pve, ['r', 'b', 'k']):
      f = st.gaussian_kde(estimated_pve[k].dropna())
      plt.plot(grid, f(grid), c=c, lw=1, label=k)
      plt.fill_between(grid, f(grid), color=c, alpha=0.1)
    plt.legend(frameon=False)
    plt.xlim(0, M)
    plt.ylim(0, plt.ylim()[1])
    plt.xlabel('Proportion of variance explained')
    plt.ylabel('Density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[55]:
  : Text(0,0.5,'Density')
  [[file:figure/power.org/estimated-pve.png]]
  :END:

  Estimate the quantiles of the distribution of dispersion PVE.

  #+BEGIN_SRC ipython
    import scipy.integrate as si
    f = st.gaussian_kde(estimated_pve['Dispersion'].dropna())
    x = np.linspace(0, 1, 1000)
    x[np.where(si.cumtrapz(f(x), x) > 0.9)[0].min()]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[85]:
  : 0.01001001001001001
  :END:
