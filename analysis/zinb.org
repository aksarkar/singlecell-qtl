#+TITLE: Mean/dispersion estimation
#+SETUPFILE: setup.org

* Introduction

  The simplest approach to call mean/variance QTLs is modular:

  1. Estimate a mean and a dispersion for each individual
  2. Treat the mean/dispersion as continuous phenotypes and perform QTL mapping

  Here, we solve (1), and [[file:qtl-mapping.org][use QTLtools]] to solve (2). We perform the following
  analyses:

  1. [[*Tensorflow implementation][We implement GPU-based]] maximum likelihood estimation of the parameters
  2. [[*Quantile-quantile diagnostic plot][We develop visual diagnostics]] of the fitted models to check goodness of fit
  3. [[*Fit NB][We fit a model without dropout]] and show it does not fit the data
  4. [[*Fit ZINB][We fit a model assuming dropout per gene]]
  5. [[*Fit ZINB2][We fit a model assuming dropout per individual per gene]]

* Model specification

  Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
  \(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell. As a
  first pass, define \(R_{ij} = \sum_k r_{ijk}\).

  Following Hilbe 2012, we derive the negative binomial as a Poisson-Gamma
  mixture:

  \[ r_{ijk} \sim \text{Pois}(R_{ij} \mu_{ik} u_{ijk}) \]

  \[ u_{ijk} \sim \text{Gamma}(\phi_{ik}^{-1}, \phi_{ik}^{-1}) \]

  Here, \(\mu_{ik}u_{ijk}\) denotes relative expression
  ([[https://arxiv.org/abs/1104.3889][Pachter 2011]]). Marginalizing out \(u\)
  yields the negative binomial distribution, with log likelihood:

  \[ \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}) = r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]

  We have multiple data points (30-200 cells) per mean/dispersion parameter, so
  simply minimizing the negative log likelihood should give reasonable
  estimates.

  We can additionally account for zero-inflation, by letting \(\pi_{\cdot}\)
  denote the probability of a "technical zero" (i.e., not arising from the
  negative-binomial).

  As a first pass, estimate dropout assuming parameters \(\pi_k\) are shared
  across cells (and individuals) for each gene. This assumption allows us to
  directly estimate the parameter from the data without requiring
  shrinkage/regularization to avoid overfitting.

  Then, the log-likelihood of the data is:

  \[ \ln p(r_{ijk} \mid r_{ijk} = 0, \cdot) = -\ln(\pi_\cdot + (1 - \pi_{\cdot})
  \exp(\ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}))) \]

  \[ \ln p(r_{ijk} \mid r_{ijk} > 0, \cdot) = \ln(1 - \pi_{\cdot}) + \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik})) \]

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/dim-reduction.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="16G",venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 43362111

  #+NAME: zinb-imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.stats as st
    import scipy.special as sp
  #+END_SRC

  #+RESULTS: zinb-imports
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+NAME: tf-imports
  #+BEGIN_SRC ipython :eval never
    import tensorflow as tf
  #+END_SRC

  #+NAME: list-local-devices
  #+BEGIN_SRC ipython :eval never
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS: list-local-devices
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 3101117888233869158, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 1
     }
     incarnation: 4298943332142850272
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:08:00.0"]
  #+END_EXAMPLE
  :END:

* Read the data

  Read the QC'ed data.

  #+CALL: read-data-qc()

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  Compute size factors.

  #+BEGIN_SRC ipython
    size = umi.agg(np.sum)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  :END:

  Onehot-encode the samples.

  #+NAME: onehot-qc
  #+BEGIN_SRC ipython
    individuals = sorted(annotations['chip_id'].unique())
    onehot = np.zeros((umi.shape[1], len(individuals)), dtype=np.float32)
    onehot[np.arange(onehot.shape[0]),annotations['chip_id'].apply(lambda x: individuals.index(x))] = 1
    onehot = pd.DataFrame(onehot, columns=individuals, index=umi.columns)
    onehot.shape
  #+END_SRC

  #+RESULTS: onehot-qc
  :RESULTS:
  # Out[5]:
  : (4985, 54)
  :END:

  Check that one-hot encoding is OK:

  #+BEGIN_SRC ipython :async t
    for _ in range(100):
      gene = np.random.choice(umi.index)
      ind = np.random.choice(individuals)
      idx = individuals.index(ind)
      assert (umi.loc[gene, (annotations['chip_id'] == ind).values] == 
              umi.loc[gene, onehot.dot(np.eye(onehot.shape[1])[idx]).astype(bool)]).all()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[5]:
  :END:

* Dask implementation                                              :noexport:

  Optimize each pair of mean/dispersion parameters by sequentially considering
  each subset of the data (set of cells per gene per individual). Use
  ~multiprocessing~ to parallelize this over chunks of genes.

  To actually fit the models, don't read the entire data in. Instead use ~dask~
  to process chunks at a time.

  #+NAME: chunk-data
  #+BEGIN_SRC ipython :eval never
    import dask.dataframe
    keep_samples = pd.read_table('/home/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    keep_genes = pd.read_table('/home/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
    onehot = pd.read_table('/scratch/midway2/aksarkar/singlecell/onehot.txt.gz')
    umi = dd.read_csv('/home/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz')
  #+END_SRC

  #+BEGIN_SRC ipython :eval never :tangle nb.py :noweb yes :exports none
    <<zinb-imports>>
    import multiprocessing as mp
    import scipy.optimize
    import scipy.special

    <<read-data-qc>>
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[102]:
  :END:

  Optimize the negative log-likelihood using BFGS.

  #+BEGIN_SRC ipython :tangle nb.py
    def zinb(theta, x, size):
      theta, dropout = theta[:2], scipy.special.expit(theta[2])
      case_zero = -np.log(dropout + (1 - dropout) * np.exp(nll(theta, x, size)))
      case_non_zero = np.log(1 - dropout) + nll(theta, x, size)
      return np.where(x < 1e-8, case_zero, case_non_zero).mean()

    def nll(theta, x, size):
      mean, inv_disp = np.exp(theta)
      mean *= size
      assert mean.shape == x.shape
      return -(x * np.log(mean / inv_disp + 1e-8) -
               x * np.log(1 + mean / inv_disp + 1e-8) -
               inv_disp * np.log(1 + mean / inv_disp + 1e-8) +
               scipy.special.gammaln(x + inv_disp) -
               scipy.special.gammaln(inv_disp) -
               scipy.special.gammaln(x + 1)).mean()

    def fit_scipy(x, size):
      res = scipy.optimize.minimize(fun=nll, x0=[0, 0], args=(x, size), tol=1e-4)
      if res.success:
        return tuple(res.x)
      else:
        return (float('nan'), float('nan'))

    def process_chunk(x, size):
      res = pd.DataFrame([(gene, k) + fit_scipy(row.loc[g].values, size.loc[g].values)
                           for gene, row in x.iterrows()
                           for k, g in row.groupby(annotations['chip_id'].values).groups.items()])
      res.columns = ['gene', 'individual', 'mean', 'dispersion']
      res.set_index('gene', inplace=True)
      return res
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[24]:
  :END:

  Output record-like data for each parameter to make reassembling the result
  easier.

  #+BEGIN_SRC ipython
    process_chunk(umi.iloc[:5], size).head()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[90]:
  #+BEGIN_EXAMPLE
    individual      mean  dispersion
    gene
    ENSG00000000003    NA18519 -8.223728    2.564775
    ENSG00000000003    NA18862 -8.406216    2.263670
    ENSG00000000003    NA19093 -8.339645    2.393590
    ENSG00000000003    NA19128 -8.479335    2.293822
    ENSG00000000003    NA18852 -8.609513    2.454818
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    Out[90].pivot(columns='individual', values='mean') 
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[99]:
  #+BEGIN_EXAMPLE
    individual        NA18519   NA18852   NA18862   NA19093   NA19128
    gene
    ENSG00000000003 -8.223728 -8.609513 -8.406216 -8.339645 -8.479335
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :eval never :tangle nb.py
    f = functools.partial(process_chunk, size=umi.agg(np.sum))
    with mp.Pool(maxtasksperchild=10) as pool:
      result = pd.concat(pool.map(f, np.array_split(umi, 1000)))

    log_mean = result.pivot(columns='individual', values='mean')
    log_disp = result.pivot(columns='individual', values='dispersion') 

    log_mean.to_csv('/scratch/midway2/aksarkar/singlecell/mean.txt.gz', sep=' ', compression='gzip')
    log_disp.to_csv('/scratch/midway2/aksarkar/singlecell/disp.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  # Tangle the code from this file and submit requesting multiple cores.

  #+BEGIN_SRC emacs-lisp :exports none
    (org-babel-tangle)
  #+END_SRC

  #+RESULTS:
  | zinb.py | tensorflow-nb.py | nb.py |

  #+BEGIN_SRC sh :exports none
    sbatch --partition="broadwl" -N1 -c16 --mem=32G --out=nb.out
    #!/bin/bash
    source activate scqtl
    python nb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 42917334

* Tensorflow implementation

  We optimize all of the parameters together, using one-hot encoding to map
  parameters to data points. This makes inference more amenable to running on
  the GPU. Although this is slower for the NB estimation problem, it will be
  faster for the ZINB estimation problem when parameters are shared and we need
  to operate on the entire count matrix.

  Use ~tensorflow~ to automatically differentiate the negative log likelihood and
  perform gradient descent.

  #+NAME: zinb-impl
  #+BEGIN_SRC ipython :eval never
    def sigmoid(x):
      """Numerically safe sigmoid"""
      return tf.clip_by_value(tf.sigmoid(x), -13, 13)

    def log(x):
      """Numerically safe log"""
      return tf.log(x + 1e-8)

    def nb_llik(x, mean, inv_disp):
      """Log likelihood of x distributed as NB

      See Hilbe 2012, eq. 8.10

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)

      """
      return (x * log(mean / inv_disp) -
              x * log(1 + mean / inv_disp) -
              inv_disp * log(1 + mean / inv_disp) +
              tf.lgamma(x + inv_disp) -
              tf.lgamma(inv_disp) -
              tf.lgamma(x + 1))

    def zinb_llik(x, mean, inv_disp, logodds, eps=1e-8):
      """Log likelihood of x distributed as ZINB

      See Hilbe 2012, eq. 11.12, 11.13

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)
      logodds - dropout log odds

      """
      case_zero = -log(sigmoid(-logodds) + sigmoid(logodds) * tf.exp(nb_llik(x, mean, inv_disp)))
      case_non_zero = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
      return tf.where(tf.less(x, 1e-8), case_zero, case_non_zero)

    def fit(umi, onehot, size_factor, gene_dropout=False, ind_dropout=False, learning_rate=1e-2, max_epochs=1000):
      """Return estimated log mean and log dispersion. 

      If fitting a zero-inflated model, additionally return dropout log odds.

      umi - count matrix (n x p; float32)
      onehot - mapping of individuals to cells (m x n; float32)
      size_factor - size factor vector (n x 1; float32)

      Returns:

      log_mean - log mean parameter (m x p)
      log_disp - log dispersion parameter (m x p)
      dropout - if zero_inflation, dropout log odds (n x p)

      """
      n, p = umi.shape
      _, m = onehot.shape

      graph = tf.Graph()
      with graph.as_default(), graph.device('/gpu:*'):
        size_factor = tf.Variable(size_factor, trainable=False)
        umi = tf.Variable(umi, trainable=False)
        onehot = tf.Variable(onehot, trainable=False)

        mean = tf.exp(tf.Variable(tf.zeros([m, p])))
        inv_disp = tf.exp(tf.Variable(tf.zeros([m, p])))

        if gene_dropout:
          if ind_dropout:
            dropout_params = tf.Variable(tf.zeros([m, p]))
            dropout = tf.matmul(onehot, dropout_params)
          else:
            dropout_params = tf.Variable(tf.zeros([1, p]))
            dropout = dropout_params
          llik = tf.reduce_mean(
            zinb_llik(umi, size_factor * tf.matmul(onehot, mean),
                      tf.matmul(onehot, inv_disp), dropout))
        elif ind_dropout:
          raise ValueError('Cannot specify individual-specific dropout without gene-specific dropout')
        else:
          llik = tf.reduce_sum(
            nb_llik(umi, size_factor * tf.matmul(onehot, mean),
                    tf.matmul(onehot, inv_disp)))

        train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-llik)
        opt = [tf.log(mean), -tf.log(inv_disp)]
        if gene_dropout:
          opt.append(dropout_params)
        curr = float('-inf')
        with tf.Session() as sess:
          sess.run(tf.global_variables_initializer())
          for i in range(max_epochs):
            _, update = sess.run([train, llik])
            if not np.isfinite(update):
              raise tf.train.NanLossDuringTrainingError
            if not i % 100:
              print(i, update)
          return sess.run(opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Quantile-quantile diagnostic plot

  The challenge in visualizing the fitted distributions is that the
  observations \(r_{ijk}\) are not drawn iid. from a distribution
  \(g_{ik}(\cdot)\). 

  Instead, we have \(r_{ijk} \sim g_{ijk}(\cdot)\), and we have used maximum
  likelihood to estimate distributions \(\hat{g}_{ijk}\).

  We can use an idea from ~ashr~: Let \(\hat{G}_{ijk}\) denote the CDF of
  \(\hat{g}_{ijk}\). Then, the distribution of values
  \(\hat{G}_{ijk}(r_{ijk})\) should be uniform.

  In the case of dropout, the PDF has a point mass at zero of size \(\pi\) and
  therefore the distribution of values of the CDF is no longer uniform. To
  handle this, plot randomized quantiles \(\tilde{G}_{ijk}(r_{ijk}) =
  \hat{G}_{ijk}(r_{ijk}) + \pi U\), where \(U \sim \mathrm{Unif}(0, 1)\).

  #+BEGIN_SRC ipython
    def estimated_cdf(x, mean, disp, size, onehot, gene_dropout=None, ind_dropout=None):
      n = onehot.dot(np.exp(-disp.values.T) + 1e-8)
      p = 1 / (1 + size.to_frame().values * onehot.dot(np.exp(mean.values + disp.values).T))
      assert (n.values > 0).all()
      assert (p.values >= 0).all()
      assert (p.values <= 1).all()
      G = st.nbinom(n=n, p=p).cdf(x)
      if gene_dropout is not None:
        G *= sp.expit(-gene_dropout.values.T)
        G += np.random.uniform(size=G.shape) * sp.expit(gene_dropout.values.T)
      elif ind_dropout is not None:
        G *= onehot.dot(sp.expit(-ind_dropout.values.T))
        G += np.random.uniform(size=G.shape) * onehot.dot(sp.expit(ind_dropout.values.T))
      assert (G <= 1).all()
      return G

    def diagnostic(umi, mean, disp, size, onehot, **kwargs):
      q = estimated_cdf(umi.values.T, mean, disp, size, onehot, **kwargs)
      plt.clf()
      plt.scatter(x=np.linspace(0, 1, q.shape[0]),
                  y=sorted(q.ravel()),
                  s=0.5)
      plt.plot([[0, 0], [1, 1]], c='black')
      plt.title(umi.index[0])
      plt.xlabel('Expected quantile')
      plt.ylabel('Estimated quantile')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[57]:
  :END:

  Look at the quantiles of the QQ plots over all genes:

  #+BEGIN_SRC ipython
    def diagnostic_quantiles(umi, mean, disp, size, onehot, quantiles=None, **kwargs):
      if quantiles is None:
        quantiles = np.array([.05, .25, .5, .75, .95])
      else:
        quantiles = np.array(quantiles)
        assert (0 <= quantiles <= 1).all()
      cdf = np.sort(estimated_cdf(umi.values.T, mean, disp, size, onehot, **kwargs).T)
      cdf_quantiles = np.percentile(cdf, 100 * quantiles, interpolation='higher', axis=0)

      plt.clf()
      for q, row in zip(quantiles, cdf_quantiles):
        plt.scatter(x=np.linspace(0, 1, row.shape[0]), y=row, c=colorcet.cm['kbc'](q), s=.5, label=q)
      plt.plot([0, 1], [0, 1], c='r', ls='dashed')
      plt.legend()
      plt.xlabel('Expected quantile')
      plt.ylabel('Estimated quantile')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[7]:
  :END:

* Fit NB

  Estimate means and dispersions assuming no dropout.

  #+BEGIN_SRC ipython :eval never :tangle tensorflow-nb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/mean2.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=120 --job-name=tfnb --output=tfnb.out --error=tfnb.err
    #!/bin/bash
    source activate scqtl
    python tensorflow-nb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43203399

  #+BEGIN_SRC sh :exports none
    sacct -j 43203399 -o Elapsed
  #+END_SRC

  #+RESULTS:
  |    Elapsed |
  | ---------- |
  |   00:38:18 |
  |   00:38:18 |

  Check the goodness of fit.

  #+BEGIN_SRC ipython
    mean = pd.read_table('/scratch/midway2/aksarkar/singlecell/mean2.txt.gz', sep=' ', index_col=0)
    disp = pd.read_table('/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz', sep=' ', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[59]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/nb-quantiles.png
    diagnostic(umi.iloc[:1], mean.iloc[:1], disp.iloc[:1], size, onehot)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[60]:
  [[file:figure/zinb.org/nb-quantiles.png]]
  :END:

  The estimated quantiles are higher than the expected quantiles, suggesting
  that the estimated distributions have too much density at lower values. This
  result is explained by the fact that means are biased downwards and
  dispersions are biased upwards due to zero-inflation. Accordingly, we expect
  to find some genes which depart even more from uniform quantiles.

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/nb-quantiles-dist.png
    diagnostic_quantiles(umi, mean, disp, size, onehot)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[61]:
  [[file:figure/zinb.org/nb-quantiles-dist.png]]
  :END:

* Fit ZINB

  Estimate the parameters of the zero-inflated model assuming dropout per gene.

  #+BEGIN_SRC ipython :eval never :tangle zinb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=120 --job-name=zinb --output=zinb.out --error=zinb.err
    #!/bin/bash
    source activate scqtl
    python zinb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43244397

  #+BEGIN_SRC sh :exports none
    sacct -j 43244397 -o Elapsed,MaxRSS,MaxVMSize
  #+END_SRC

  #+RESULTS:
  |    Elapsed | MaxRSS     | MaxVMSize  |
  | ---------- | ---------- | ---------- |
  |   01:05:22 |            |            |
  |   01:05:22 | 3957908K   | 162203788K |

  Plot the diagnostic for the model.

  #+BEGIN_SRC ipython
    zi_mean = pd.read_table('/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz', sep=' ', index_col=0)
    zi_disp = pd.read_table('/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz', sep=' ', index_col=0)
    zi_dropout = pd.read_table('/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz', sep=' ', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[8]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/zinb-diagnostic.png
    diagnostic_quantiles(umi, zi_mean, zi_disp, size, onehot, gene_dropout=zi_dropout)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[63]:
  [[file:figure/zinb.org/zinb-diagnostic.png]]
  :END:

* Fit ZINB2

  Estimate the parameters of the zero-inflated model assuming dropout per
  individual and gene.

  #+BEGIN_SRC ipython :eval never :tangle zinb2.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      ind_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dropout.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=120 --job-name=zinb2 --output=zinb2.out --error=zinb2.err
    #!/bin/bash
    source activate scqtl
    python zinb2.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43299007

  #+BEGIN_SRC sh :exports none
    sacct -j 43299007 -o Elapsed,MaxRSS
  #+END_SRC

  #+RESULTS:
  |    Elapsed | MaxRSS     |
  | ---------- | ---------- |
  |   01:22:47 |            |
  |   01:22:47 | 6317612K   |

  Plot the data and fitted distribution.

  #+BEGIN_SRC ipython
    zi2_mean = pd.read_table('/scratch/midway2/aksarkar/singlecell/zi2-mean.txt.gz', sep=' ', index_col=0)
    zi2_disp = pd.read_table('/scratch/midway2/aksarkar/singlecell/zi2-dispersion.txt.gz', sep=' ', index_col=0)
    zi2_dropout = pd.read_table('/scratch/midway2/aksarkar/singlecell/zi2-dropout.txt.gz', sep=' ', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[24]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/zinb2-diagnostic.png
    diagnostic_quantiles(umi, zi2_mean, zi2_disp, size, onehot, ind_dropout=zi2_dropout)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[64]:
  [[file:figure/zinb.org/zinb2-diagnostic.png]]
  :END:
