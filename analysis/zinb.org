#+TITLE: ZINB estimation
#+SETUPFILE: setup.org

* Introduction

  Here, we estimate means, variances, and dropout rates of the single cell data
  using a hierarchical zero-inflated negative binomial (ZINB) model.

  The basic idea is that after fitting the model, we will recover an
  individual-by-gene mean matrix and individual-by-gene dispersion matrix. We
  can use the columns of this matrix as continuous phenotypes to plug into
  existing QTL mapping software.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/sc-vs-bulk.org")
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/qtl-mapping.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  This has to be run on the ~gpu2~ partition since it depends on ~libcuda.so~

  #+CALL: ipython3(venv="scqtl", memory="8G", partition="gpu2")

  #+RESULTS:
  : 1

  #+BEGIN_SRC ipython
    %matplotlib inline

    import edward as ed
    import numpy as np
    import pandas as pd
    import tensorflow as tf
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Read the data                                                    :noexport:

  #+CALL: load-data-defs()

  #+CALL: load-data()

  #+CALL: umi-qc()

  #+BEGIN_SRC ipython
    onehot = 
  #+END_SRC

* Model specification and inference

  We implement ZINB in [[http://edwardlib.org/][Edward]] (not shown). This allows us to succinctly
  describe the model below:

  #+NAME: zinb-def
  #+BEGIN_SRC ipython :exports none
    def logit(x):
      return tf.reciprocal(1 + tf.exp(-x))

    class _ZINB(ds.Distribution):
      """Zero-inflated negative binomial random variable

      """
      def __init__(self, dropout, mean, dispersion, validate_args=False,
                   allow_nan_stats=False, name='ZINB'):
        parameters = locals()
        with tf.name_scope(name, values=[dropout, mean, dispersion]):
          self._dropout = tf.identity(dropout, name="dropout")
          self._mean = tf.exp(tf.identity(mean, name="mean"))
          self._dispersion = tf.exp(tf.identity(dispersion, name="dispersion"))
        super(_ZINB, self).__init__(
            allow_nan_stats=allow_nan_stats,
            dtype=self._mean.dtype,
            graph_parents=[self._dropout, self._mean, self._dispersion],
            name=name,
            parameters=parameters,
            reparameterization_type=ds.FULLY_REPARAMETERIZED,
            validate_args=validate_args,
        )

      def nb_llik(self, x):
        """Return log likelihood of x under NB model

        c.f. Hilbe 2012, Eq. 8.10

        """
        theta = tf.reciprocal(self._dispersion)
        return (tf.lgamma(x + theta) -
                tf.lgamma(x + 1) -
                tf.lgamma(theta) +
                theta * (tf.log(theta) - tf.log(theta + self._mean)) +
                x * (tf.log(self._mean) - tf.log(theta + self._mean)))

      def _log_prob(self, x):
        """Return log likelihood of x under ZINB model

        x ~ dropout \delta_0(x) + (1 - dropout) NB(x; mean, dispersion)

        c.f. Hilbe 2012, Eqs. 11.12, 11.13

        """
        zeros = tf.cast(tf.equal(x, 0.), tf.float32)
        case_zero = (tf.nn.softplus(-logit(self._dropout) + self.nb_llik(x)) -
                     tf.nn.softplus(-logit(self._dropout)))
        case_nonzero = (tf.log(self._dropout) + self.nb_llik(x))
        return zeros * case_zero + (1 - zeros) * case_nonzero

      def _sample_n(self, n):
        raise NotImplementedError

    class ZINB(ed.models.RandomVariable, _ZINB):
      def __init__(self, *args, **kwargs):
        if 'mean' in kwargs:
            mean = kwargs['mean']
        else:
            mean = args[1]
        if 'value' not in kwargs:
            kwargs['value'] = tf.zeros_like(mean)
        RandomVariable.__init__(self, *args, **kwargs)
  #+END_SRC

  #+BEGIN_SRC ipython
    p, n = umi.shape
    _, m = onehot.shape

    shared_dropout = ed.models.Normal(tf.constant(0.), scale=tf.constant(1.))
    dropout = ed.models.Normal(loc=tf.fill([n, p], shared_dropout), scale=tf.constant(1.))
    shared_mean = ed.models.Normal(tf.constant(0.), tf.constant(1.))
    mean = ed.models.Normal(loc=tf.fill([m, p], shared_mean), scale=tf.constant(1.))
    shared_dispersion = ed.models.Normal(tf.constant(0.), tf.constant(1.))
    dispersion = ed.models.Normal(loc=tf.fill([m, p], shared_mean), scale=tf.constant(1.))
    y = ZINB(dropout=dropout,
             mean=tf.matmul(onehot, mean),
             dispersion=tf.matmul(onehot, dispersion))
  #+END_SRC

  We fit the model using variational inference.

  #+BEGIN_SRC ipython
    q_shared_dropout = ed.models.Normal(tf.Variable(tf.zeros([1])), tf.Variable(tf.zeros([1])))
    q_shared_mean = ed.models.Normal(tf.Variable(tf.zeros([1])), tf.Variable(tf.zeros([1])))
    q_shared_dispersion = ed.models.Normal(tf.Variable(tf.zeros([1])), tf.Variable(tf.zeros([1])))
    q_dropout = ed.models.Normal(tf.Variable(tf.zeros([n, p])), tf.Variable(tf.zeros([n, p])))
    q_mean = ed.models.Normal(tf.Variable(tf.zeros([m, p])), tf.Variable(tf.zeros([m, p])))
    q_dispersion = ed.models.Normal(tf.Variable(tf.zeros([m, p])), tf.Variable(tf.zeros([m, p])))
  #+END_SRC

  #+BEGIN_SRC ipython :async t
    # IMPORTANT: we have to explicitly make sure Edward does not use analytical
    # KL because it does not DTRT
    inf = ed.ReparameterizationKLqp(
      latent_vars={
        shared_dropout: q_shared_dropout,
        shared_mean: q_shared_mean,
        shared_dispersion: q_shared_dispersion,
        dropout: q_dropout,
        mean: q_mean,
        dispersion: q_dispersion,
      },
      data={y: counts})
    inf.run(n_samples=10)
  #+END_SRC
