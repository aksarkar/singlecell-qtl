#+TITLE: ZINB estimation
#+SETUPFILE: setup.org

* Introduction

  Here, we estimate means, variances, and dropout rates of the single cell data
  using a hierarchical zero-inflated negative binomial (ZINB) model.

  The key issues are:

  1. *This model does not use genotype information.* The basic idea is that
     after fitting the model, we will recover an individual-by-gene mean matrix
     and individual-by-gene dispersion matrix. We can use the columns of this
     matrix as continuous phenotypes to plug into existing QTL mapping
     software.

     Related, this model does not use covariates (either measured or inferred)
     to estimate any of the parameters.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/sc-vs-bulk.org")
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/qtl-mapping.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  This has to be run on the ~gpu2~ partition since it depends on ~libcuda.so~

  #+CALL: ipython3(venv="scqtl", memory="32G", partition="gpu2")

  #+RESULTS:
  : Submitted batch job 40967674

  #+BEGIN_SRC ipython
    %matplotlib inline

    import edward as ed
    import functools
    import numpy as np
    import pandas as pd
    import tensorflow as tf
    import tensorflow.contrib.distributions as ds
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Read the data                                                    :noexport:

  #+CALL: load-data-defs()

  #+RESULTS:
  :RESULTS:
  :END:

  #+CALL: load-data()

  #+RESULTS:
  :RESULTS:
  : ((20327, 2261), (34608, 15))
  :END:

  #+CALL: umi-qc()

  #+RESULTS:
  :RESULTS:
  : (15636, 1861)
  :END:

* Model specification and inference

  We implement ZINB in [[http://edwardlib.org/][Edward]] (not shown). This allows us to succinctly
  describe the model below:

  #+NAME: zinb-def
  #+BEGIN_SRC ipython :exports none
    def logit(x):
      return tf.reciprocal(1 + tf.exp(-x))

    class _ZINB(ds.Distribution):
      """Zero-inflated negative binomial random variable

      """
      def __init__(self, dropout, mean, dispersion, validate_args=False,
                   allow_nan_stats=False, name='ZINB'):
        parameters = locals()
        with tf.name_scope(name, values=[dropout, mean, dispersion]):
          self._dropout = tf.identity(dropout, name="dropout")
          self._mean = tf.exp(tf.identity(mean, name="mean"))
          self._dispersion = tf.exp(tf.identity(dispersion, name="dispersion"))
        super(_ZINB, self).__init__(
            allow_nan_stats=allow_nan_stats,
            dtype=self._mean.dtype,
            graph_parents=[self._dropout, self._mean, self._dispersion],
            name=name,
            parameters=parameters,
            reparameterization_type=ds.FULLY_REPARAMETERIZED,
            validate_args=validate_args,
        )

      def nb_llik(self, x):
        """Return log likelihood of x under NB model

        c.f. Hilbe 2012, Eq. 8.10

        """
        theta = tf.reciprocal(self._dispersion)
        return (tf.lgamma(x + theta) -
                tf.lgamma(x + 1) -
                tf.lgamma(theta) +
                theta * (tf.log(theta) - tf.log(theta + self._mean)) +
                x * (tf.log(self._mean) - tf.log(theta + self._mean)))

      def _log_prob(self, x):
        """Return log likelihood of x under ZINB model

        x ~ dropout \delta_0(x) + (1 - dropout) NB(x; mean, dispersion)

        c.f. Hilbe 2012, Eqs. 11.12, 11.13

        """
        zeros = tf.cast(tf.equal(x, 0.), tf.float32)
        case_zero = (tf.nn.softplus(-logit(self._dropout) + self.nb_llik(x)) -
                     tf.nn.softplus(-logit(self._dropout)))
        case_nonzero = (tf.log(self._dropout) + self.nb_llik(x))
        return zeros * case_zero + (1 - zeros) * case_nonzero

      def _sample_n(self, n):
        raise NotImplementedError

    class ZINB(ed.models.RandomVariable, _ZINB):
      def __init__(self, *args, **kwargs):
        if 'mean' in kwargs:
            mean = kwargs['mean']
        else:
            mean = args[1]
        if 'value' not in kwargs:
            kwargs['value'] = tf.zeros_like(mean)
        ed.models.RandomVariable.__init__(self, *args, **kwargs)
  #+END_SRC

  #+RESULTS: zinb-def
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    Normal = ed.models.Normal
    p, n = umi_qc.shape
    _, m = onehot.shape

    shared_dropout = Normal(tf.zeros([1]), scale=tf.ones([1]))
    shared_mean = Normal(tf.zeros([1]), scale=tf.ones([1]))
    shared_dispersion = Normal(tf.zeros([1]), scale=tf.ones([1]))

    gene_dropout = Normal(tf.ones([1, p]) * shared_dropout, scale=tf.ones([1, p]))
    gene_mean = Normal(tf.ones([1, p]) * shared_mean, scale=tf.ones([1, p]))
    gene_dispersion = Normal(tf.ones([1, p]) * shared_dispersion, scale=tf.ones([1, p]))

    dropout = Normal(loc=tf.ones([n, 1]) * gene_dropout, scale=tf.constant(1.))
    mean = Normal(loc=tf.ones([m, 1]) * gene_mean, scale=tf.constant(1.))
    dispersion = Normal(loc=tf.ones([m, 1]) * gene_dispersion, scale=tf.constant(1.))

    y = ZINB(dropout=dropout,
             mean=tf.exp(tf.matmul(onehot, mean)),
             dispersion=tf.exp(tf.matmul(onehot, dispersion)))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  We fit the model using variational inference.

  #+BEGIN_SRC ipython
    q_shared_dropout = Normal(tf.Variable(tf.zeros([1, p])), tf.Variable(tf.zeros([1])))
    q_shared_mean = Normal(tf.Variable(tf.zeros([1, p])), tf.Variable(tf.zeros([1])))
    q_shared_dispersion = Normal(tf.Variable(tf.zeros([1, p])), tf.Variable(tf.zeros([1])))

    q_dropout = Normal(tf.Variable(tf.zeros([n, p])), tf.Variable(tf.zeros([n, p])))
    q_mean = Normal(tf.Variable(tf.zeros([m, p])), tf.Variable(tf.zeros([m, p])))
    q_dispersion = Normal(tf.Variable(tf.zeros([m, p])), tf.Variable(tf.zeros([m, p])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :async t
    # IMPORTANT: we have to explicitly make sure Edward does not use analytical
    # KL because it does not DTRT
    inf = ed.ReparameterizationKLqp(
      latent_vars={
        shared_dropout: q_shared_dropout,
        shared_mean: q_shared_mean,
        shared_dispersion: q_shared_dispersion,
        dropout: q_dropout,
        mean: q_mean,
        dispersion: q_dispersion,
      },
      data={y: umi_qc.values.T})
    inf.run(n_samples=10)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  486e4f90-66f8-4f28-94a5-eef5ca89d28e
  :END:
