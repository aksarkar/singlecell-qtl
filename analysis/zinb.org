#+TITLE: Mean/dispersion estimation
#+SETUPFILE: setup.org

* Introduction

  We take a modular approach to call QTLs:

  1. Estimate a mean and a dispersion for each individual
  2. Treat the mean/dispersion as continuous phenotypes and perform QTL mapping

  Here, we solve (1).

  1. [[*numpy/scipy implementation][We implement CPU-based ML estimation]]
  2. [[*Genome-wide distribution of dispersion][We estimate per-gene indexes of dispersion]] accounting for the fact that
     data came from multiple individuals

* Model specification

  Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
  \(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell. As a
  first pass, define \(R_{ij} = \sum_k r_{ijk}\).

  Following Hilbe 2012, we derive the negative binomial as a Poisson-Gamma
  mixture:

  \[ r_{ijk} \sim \text{Pois}(R_{ij} \mu_{ik} u_{ijk}) \]

  \[ u_{ijk} \sim \text{Gamma}(\phi_{ik}^{-1}, \phi_{ik}^{-1}) \]

  Here, \(\mu_{ik}u_{ijk}\) denotes relative expression
  ([[https://arxiv.org/abs/1104.3889][Pachter 2011]]). Marginalizing out \(u\)
  yields the negative binomial distribution, with log likelihood:

  \[ \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}) = r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]

  We have multiple data points (30-200 cells) per mean/dispersion parameter, so
  simply minimizing the negative log likelihood should give reasonable
  estimates.

  We additionally account for zero-inflation, by letting \(\pi_{ik}\) denote
  the probability of observing an excess zero (not arising from the
  negative-binomial).

  Then, the log-likelihood of the data is:

  \[ \ln p(r_{ijk} \mid \cdot) = \ln(\pi_{ik} + (1 -  \pi_{ik}) p(r_{ijk}
    \mid R_{ij}, \mu_{ik}, \phi_{ik}))\ \text{if}\ r_{ijk} = 0 \]
  \[ \ln p(r_{ijk} \mid \cdot) = \ln(1 - \pi_{ik}) + \ln p(r_{ijk} \mid
    R_{ij}, \mu_{ik}, \phi_{ik})\ \text{otherwise} \]

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/project2/mstephens/aksarkar/projects/singlecell-qtl/analysis/dim-reduction.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="4G",venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 44351667

  #+NAME: zinb-imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.stats as st
    import scipy.special as sp
    import sqlite3
  #+END_SRC

  #+RESULTS: zinb-imports
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[5]:
  :END:

  #+NAME: tf-imports
  #+BEGIN_SRC ipython :eval never
    import tensorflow as tf
  #+END_SRC

  #+NAME: list-local-devices
  #+BEGIN_SRC ipython :eval never
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS: list-local-devices
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 3101117888233869158, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 1
     }
     incarnation: 4298943332142850272
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:08:00.0"]
  #+END_EXAMPLE
  :END:

* Tensorflow implementation

  We optimize all of the parameters together, using one-hot encoding to map
  parameters to data points. This makes inference more amenable to running on
  the GPU.

  Use ~tensorflow~ to automatically differentiate the negative log likelihood and
  perform gradient descent.

  #+NAME: zinb-impl
  #+BEGIN_SRC ipython :eval never
    def sigmoid(x):
      """Numerically safe sigmoid"""
      return tf.clip_by_value(tf.sigmoid(x), -13, 13)

    def log(x):
      """Numerically safe log"""
      return tf.log(x + 1e-8)

    def nb_llik(x, mean, inv_disp):
      """Log likelihood of x distributed as NB

      See Hilbe 2012, eq. 8.10

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)

      """
      return (x * log(mean / inv_disp) -
              x * log(1 + mean / inv_disp) -
              inv_disp * log(1 + mean / inv_disp) +
              tf.lgamma(x + inv_disp) -
              tf.lgamma(inv_disp) -
              tf.lgamma(x + 1))

    def zinb_llik(x, mean, inv_disp, logodds, eps=1e-8):
      """Log likelihood of x distributed as ZINB

      See Hilbe 2012, eq. 11.12, 11.13

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)
      logodds - dropout log odds

      """
      case_zero = log(sigmoid(-logodds) + sigmoid(logodds) * tf.exp(nb_llik(x, mean, inv_disp)))
      case_non_zero = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
      return tf.where(tf.less(x, 1e-8), case_zero, case_non_zero)

    def fit(umi, onehot, size_factor, gene_dropout=False, ind_dropout=False, learning_rate=1e-2, max_epochs=1000):
      """Return estimated log mean and log dispersion. 

      If fitting a zero-inflated model, additionally return dropout log odds.

      umi - count matrix (n x p; float32)
      onehot - mapping of individuals to cells (m x n; float32)
      size_factor - size factor vector (n x 1; float32)
      gene_dropout - fit one dropout parameter per gene
      ind_dropout - fit one dropout parameter per individual
      init_log_mean - initial value for estimated log mean (m x p; float32)
      init_log_disp - initial value for estimated log dispersion (m x p; float32)

      If ind_dropout is True, gene_dropout must be True, otherwise raises
      ArgumentError.

      Returns:

      log_mean - log mean parameter (m x p)
      log_disp - log dispersion parameter (m x p)
      dropout - dropout log odds (1 x p if gene_dropout, n x p if ind_dropout)

      """
      n, p = umi.shape
      _, m = onehot.shape

      params = locals()
      graph = tf.Graph()
      with graph.as_default(), graph.device('/gpu:*'):
        size_factor = tf.Variable(size_factor, trainable=False)
        umi = tf.Variable(umi, trainable=False)
        onehot = tf.Variable(onehot, trainable=False)

        mean = tf.exp(tf.Variable(tf.zeros([m, p])))
        inv_disp = tf.exp(tf.Variable(tf.zeros([m, p])))

        if gene_dropout:
          if ind_dropout:
            dropout_params = tf.Variable(tf.zeros([m, p]))
            dropout = tf.matmul(onehot, dropout_params)
          else:
            dropout_params = tf.Variable(tf.zeros([1, p]))
            dropout = dropout_params
          llik = tf.reduce_mean(
            zinb_llik(umi, size_factor * tf.matmul(onehot, mean),
                      tf.matmul(onehot, inv_disp), dropout))
        elif ind_dropout:
          raise ValueError('Cannot specify individual-specific dropout without gene-specific dropout')
        else:
          llik = tf.reduce_mean(
            nb_llik(umi, size_factor * tf.matmul(onehot, mean),
                    tf.matmul(onehot, inv_disp)))

        train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-llik)
        opt = [tf.log(mean), -tf.log(inv_disp)]
        if gene_dropout:
          opt.append(dropout_params)
        curr = float('-inf')
        with tf.Session() as sess:
          sess.run(tf.global_variables_initializer())
          for i in range(max_epochs):
            _, update = sess.run([train, llik])
            if not np.isfinite(update):
              raise tf.train.NanLossDuringTrainingError
            if not i % 100:
              print(i, update)
          return sess.run(opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

** Read the data

  Read the QC'ed data.

  #+CALL: read-data-qc()

  #+RESULTS:
  :RESULTS:
  # Out[61]:
  :END:

  Onehot-encode the samples.

  #+NAME: onehot-qc
  #+BEGIN_SRC ipython
    individuals = sorted(annotations['chip_id'].unique())
    onehot = np.zeros((umi.shape[1], len(individuals)), dtype=np.float32)
    onehot[np.arange(onehot.shape[0]),annotations['chip_id'].apply(lambda x: individuals.index(x))] = 1
    onehot = pd.DataFrame(onehot, columns=individuals, index=umi.columns)
    onehot.shape
  #+END_SRC

  #+RESULTS: onehot-qc
  :RESULTS:
  # Out[5]:
  : (4995, 54)
  :END:

  Check that one-hot encoding is OK:

  #+BEGIN_SRC ipython :async t
    for _ in range(100):
      gene = np.random.choice(umi.index)
      ind = np.random.choice(individuals)
      idx = individuals.index(ind)
      assert (umi.loc[gene, (annotations['chip_id'] == ind).values] == 
              umi.loc[gene, onehot.dot(np.eye(onehot.shape[1])[idx]).astype(bool)]).all()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[5]:
  :END:

** Fit NB

  Estimate means and dispersions assuming no dropout.

  #+BEGIN_SRC ipython :eval never :tangle tensorflow-nb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/mean2.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=60 --job-name=tfnb --output=tfnb.out --error=tfnb.err
    #!/bin/bash
    source activate scqtl
    python tensorflow-nb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43884214

** Fit ZINB

  Estimate the parameters of the zero-inflated model assuming dropout per gene.

  #+BEGIN_SRC ipython :eval never :tangle zinb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=150 --job-name=zinb --output=zinb.out --error=zinb.err
    #!/bin/bash
    source activate scqtl
    python zinb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43966390

** Fit ZINB2

  Estimate the parameters of the zero-inflated model assuming dropout per
  individual and gene.

  #+BEGIN_SRC ipython :eval never :tangle zinb2.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      ind_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dropout.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=120 --job-name=zinb2 --output=zinb2.out --error=zinb2.err
    #!/bin/bash
    source activate scqtl
    python zinb2.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43910744

* numpy/scipy implementation

  Optimize the negative log-likelihood.

  #+NAME: np-zinb-impl
  #+BEGIN_SRC ipython
    import scipy.optimize as so

    def log(x):
      """Numerically safe log"""
      return np.log(x + 1e-8)

    def sigmoid(x):
      """Numerically safe sigmoid"""
      lim = np.log(np.finfo(np.float64).resolution)
      return np.clip(sp.expit(x), lim, -lim)

    def zinb(theta, x, size):
      theta, dropout = theta[:2], sigmoid(theta[2])
      case_zero = log(dropout + (1 - dropout) * np.exp(-nb(theta, x, size)))
      case_non_zero = log(1 - dropout) - nb(theta, x, size)
      return -np.where(x < 1e-8, case_zero, case_non_zero).mean()

    def nb(theta, x, size):
      mean, inv_disp = np.exp(theta)
      mean *= size
      assert mean.shape == x.shape
      return -(x * log(mean / inv_disp) -
               x * log(1 + mean / inv_disp) -
               inv_disp * log(1 + mean / inv_disp) +
               sp.gammaln(x + inv_disp) -
               sp.gammaln(inv_disp) -
               sp.gammaln(x + 1)).mean()
   #+END_SRC

   #+RESULTS: np-zinb-impl
   :RESULTS:
   # Out[1]:
   :END:

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Write the data to the database. Use the pass through the count matrix to
   compute and write out size factors for each cell.

   #+BEGIN_SRC ipython :async t
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     annotations = annotations.loc[keep_samples.values.ravel()]
     annotations['sample'] = annotation.apply(lambda x: '{chip_id}.{experiment:08d}.{well}'.format(**dict(x)), axis=1)
     annotations = annotations.set_index('sample')
     annotations['size'] = np.zeros(annotations.shape[0])
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       conn.execute('drop table if exists umi;')
       for chunk in pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0, chunksize=100):
         chunk = (chunk
                  .filter(items=keep_genes[keep_genes.values.ravel()].index, axis='index')
                  .loc[:,keep_samples.values.ravel()])
         if not chunk.empty:
           annotations['size'] += chunk.sum(axis=0)
           chunk.reset_index().melt(id_vars='gene', var_name='sample').to_sql(name='umi', con=conn, index=False, if_exists='append')
       annotations[['chip_id', 'size']].to_sql(name='annotation', con=conn, if_exists='replace')
       conn.execute('create index ix_umi on umi(gene, sample);')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[99]:
   :END:

   Use this to check the parameter estimation for a particular gene/individual.

   #+BEGIN_SRC ipython
     def extract_data(ind, gene):
       with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
         umi = pd.read_sql("""select umi.value, annotation.size from umi, annotation 
         where annotation.chip_id == ? and gene == ? and 
         umi.sample == annotation.sample;""", con=conn, params=(ind, gene))
         return umi
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Shard the data to parallelize over nodes. Doing this in one pass over the
   count matrix is easier than doing this in one pass over the database table
   because we don't have to worry about accidentally splitting up data for an
   individual/gene pair across chunks.

   #+BEGIN_SRC ipython :async t
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       annotation = pd.read_sql('select * from annotation', con=conn)

     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
     keep_genes = keep_genes[keep_genes.values].index
     i = 0
     for chunk in pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0, chunksize=1000):
       print('Processing chunk {}'.format(i))
       chunk = (chunk
                .loc[:,keep_samples.values.ravel()]
                .filter(items=keep_genes, axis='index'))
       if not chunk.empty:
         chunk = (chunk
                  .reset_index()
                  .melt(id_vars='gene', var_name='sample')
                  .merge(annotation[['chip_id', 'sample', 'size']], on='sample')
                  .sort_values(['gene', 'chip_id', 'sample']))
         chunk.to_csv('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(i), columns=['gene', 'chip_id', 'sample', 'value', 'size'], compression='gzip', sep='\t')
         i += 1
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[105]:
   :END:

   Process each chunk in parallel.

   #+NAME: process-chunk-impl
   #+BEGIN_SRC ipython :eval never
     def fit_gene(chunk):
       res0 = so.minimize(nb, x0=[0, 0], args=(chunk[:,0], chunk[:,1]))
       pi0 = (chunk[:,0] == 0).sum() / chunk.shape[0] + 1e-8
       res = so.minimize(zinb, x0=list(res0.x) + [sp.logit(pi0 + 1e-8)], args=(chunk[:,0], chunk[:,1]))
       # Log likelihood of ZINB vs. NB. We need to flip signs again
       llr = chunk.shape[0] * (res0.fun - res.fun)
       return list(res0.x) + [res0.success] + list(res.x) + [res.success, llr]

     def compute_breaks(chunk, by_ind=False):
       # Each subproblem has fixed size, so we can just split on integer indices
       # (instead of grouping)
       num_genes = len(set(chunk['gene']))
       num_samples = len(set(chunk['sample']))
       breaks = num_samples * np.arange(num_genes).reshape(-1, 1)
       if by_ind:
         num_samples_per_ind = chunk.iloc[:num_samples]['chip_id'].value_counts().sort_index().values
         # This can't be written += because of broadcasting
         breaks = breaks + np.cumsum(num_samples_per_ind).reshape(1, -1)
       else:
         # We need to get the right end point of each subproblem (exclusive)
         breaks += num_samples
       return breaks.ravel()
   #+END_SRC

   #+BEGIN_SRC ipython :tangle /scratch/midway2/aksarkar/singlecell/np-zinb.py :noweb tangle
     <<zinb-imports>>
     import argparse
     import gzip
     import os
     import multiprocessing as mp
     import sqlite3
     <<np-zinb-impl>>
     <<process-chunk-impl>>

     with mp.Pool() as pool:
       chunk = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')))
       breaks = compute_breaks(chunk, by_ind=True)
       res = pool.map(fit_gene, np.split(chunk[['value', 'size']].values, breaks[:-1]))

     with gzip.open('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')), 'wt') as f:
       for b in breaks:
         gene, ind = chunk.iloc[b - 1][['gene', 'chip_id']]
         print(gene, ind, *res.pop(0), file=f)
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation
     sbatch --partition=broadwl --job-name="np-zinb" --mem=4G -a 0-20 -n1 -c28 --exclusive
     #!/bin/bash
     source activate scqtl
     python /scratch/midway2/aksarkar/singlecell/np-zinb.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 44315588

   Populate the database.

   #+BEGIN_SRC ipython :async t
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       conn.execute('drop table if exists params;')
       for i in range(20):
         for chunk in pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(i), sep=' ', header=None, chunksize=1000):
           chunk.columns = ['gene', 'ind', 'nb_log_mean', 'nb_log_disp', 'nb_success', 'zinb2_log_mean', 'zinb2_log_disp', 'zinb2_logodds', 'zinb_success', 'llr']
           chunk.to_sql(name='params', con=conn, index=False, if_exists='append')
       conn.execute('create index ix_params on params(gene, ind);')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

* Genome-wide distribution of dispersion

  The index of dispersion for observed data \(r_{ijk}\) at gene \(k\) is:

  \[ D_k = \frac{V[r_{ijk}]}{E[r_{ijk}]} \]

  where expectations (variances) are taken over individuals \(i\) and cells
  \(j\).

  Let \(g_{ijk}\) denote the zero-inflated negative binomial density as defined
  above. Then, we have:

  \[ r_{ijk} \sim \sum_{ijk} \frac{1}{N} g_{ijk}(\cdot) \]

  Fixing gene \(k\), the mixture density has expectation:

  \[ \mu_k = \frac{1}{N} \sum_{ijk} E_{g_{ijk}}[r_{ijk}] \]

  and variance ([[http://www.springer.com/us/book/9780387329093][Frühwirth-Schnatter 2006]]):

  \[ \sigma^2_k = \frac{1}{N} \sum (E_{g_{ijk}}[r_{ijk}] - \mu)^2 + V_{g_{ijk}}
  [r_{ijk}] \]

  Fixing individual \(i\) and cell \(j\), we have:

  \[ E[r_{ijk}] = (1 - \pi_{ik}) R_{ij} \mu_{ik} \]

  \[ V[r_[ijk]] = (1 - \pi_{ik}) \left(R_{ij} \mu_{ik} + (R_{ij} \mu_{ik})^2 \phi_{ik}\right) \]

  Read the estimated parameters, throwing out individuals with fewer than 50
  cells.

  Compute the index of dispersion as described above. Use the sharded results
  to parallelize. *Important: we parameterized in terms of inverse dispersion,
  so we need to flip signs here.*

  #+BEGIN_SRC ipython :eval never
  #+END_SRC

   #+BEGIN_SRC ipython :tangle /scratch/midway2/aksarkar/singlecell/np-disp.py :noweb tangle
     <<zinb-imports>>
     import multiprocessing as mp
     import sqlite3

     def process_gene(chunk):
       global annotation
       gene, ind, *_ = chunk.iloc[0]
       if ind in annotation['chip_id']:
         exp_value = annotation[annotation['chip_id'] == ind]['size'] * np.exp(chunk['log_mean'])
         var = exp_value + exp_value * exp_value * np.exp(-g['log_disp'])
         gene_mean = exp_value.mean()
         gene_var = (np.square(exp_value - gene_mean) + var).mean()
         return
       else:
         return {}

     with mp.Pool() as pool:
       with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
         annotation = pd.read_sql('select * from annotation group by chip_id having count(distinct sample) >= 50;', conn)
       chunk = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')), header=None)
       chunk.columns = ['gene', 'ind', 'nb_log_mean', 'nb_log_disp', 'nb_success', 'zinb2_log_mean', 'zinb2_log_disp', 'zinb2_logodds', 'zinb2_success', 'llr']
       chunk['log_mean'] = np.where(chunk['llr'] < 1, chunk['nb_log_mean'], chunk['zinb2_log_mean'])
       chunk['log_disp'] = np.where(chunk['llr'] < 1, chunk['nb_log_disp'], chunk['zinb2_log_disp'])
       # Subproblems have fixed size
       num_genes = len(set(chunk['gene']))
       num_inds = len(set(chunk['chip_id']))
       # Get the right endpoints (exclusive)
       breaks = num_inds * np.arange(1, num_genes + 1)
       res = pd.DataFrame.from_dict(pool.map(process_gene, np.split(chunk[['gene', 'ind', 'log_mean', 'log_disp']].values, breaks[:-1])))
       res.to_csv('/scratch/midway2/aksarkar/singlecell/density-estimation/dispersion-index-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')), compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation
     sbatch --partition=broadwl --job-name="np-zinb" --mem=4G -a 0-20 -n1 -c28 --exclusive
     #!/bin/bash
     source activate scqtl
     python /scratch/midway2/aksarkar/singlecell/np-zinb.py
   #+END_SRC

  Plot the distribution of dispersions.

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/global-dispersion-hist.png
    plt.clf()
    plt.hist(dispersion[np.isfinite(dispersion)], bins=50, color='k')
    plt.xlabel('Index of dispersion')
    _ = plt.ylabel('Number of genes')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[6]:
  [[file:figure/zinb.org/global-dispersion-hist.png]]
  :END:

  Write out the computed dispersions.

  #+BEGIN_SRC ipython
    dispersion[np.isfinite(dispersion)].to_frame().to_csv('/scratch/midway2/aksarkar/singlecell/index-dispersion-by-gene.txt.gz', compression='gzip', header=False, sep='\t')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[7]:
  :END:
