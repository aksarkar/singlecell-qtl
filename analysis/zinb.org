#+TITLE: Mean/dispersion estimation
#+SETUPFILE: setup.org

* Introduction

  We take a modular approach to call QTLs:

  1. Estimate a mean and a dispersion for each individual
  2. Treat the mean/dispersion as continuous phenotypes and perform QTL mapping

  Here, we solve (1).

  1. [[*numpy/scipy implementation][We implement CPU-based ML estimation]]
  2. [[*Per-gene dispersion][We estimate per-gene indexes of dispersion]] accounting for the fact that
     data came from multiple individuals

* Model specification

  Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
  \(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell.

  \[ r_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})\text{Poisson}(\cdot; R_{ij} \mu_{ik} u_{ijk}) \]

  \[ u_{ijk} \sim \text{Gamma}(\cdot; \phi_{ik}^{-1}, \phi_{ik}^{-1}) \]

  Here, \(\mu_{ik}\) is proportional to relative expression ([[https://arxiv.org/abs/1104.3889][Pachter 2011]]), and
  \(\phi_{ik}\) is the variance of expression noise.

  Considering just the Poisson component, marginalizing out \(u\) yields the
  log likelihood:

  \[ l(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]

  Then, marginalizing over the mixture yields the log likelihood:

  \[ \ln p(r_{ijk} \mid \cdot) = \ln(\pi_{ik} + \exp(l(\cdot)))\ \text{if}\ r_{ijk} = 0 \]

  \[ \ln p(r_{ijk} \mid \cdot) = l(\cdot)\ \text{otherwise} \]

  We have enough observations per mean/dispersion parameter that simply
  minimizing the negative log likelihood should give reasonable estimates.

  This model is equivalent to a model where we assume that the underlying rate
  is a point-Gamma mixture:

  \[ r_{ijk} \mid \lambda_{ijk} \sim \mathrm{Poisson}(\cdot; R_{ij}\lambda_{ijk}) \]

  \[ \lambda_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})
  \text{Gamma}(\lambda_{ijk}; \phi_{ik}^{-1}, \phi_{ik}^{-1}\mu_{ik}^{-1}) \]

  The Gamma component of this mixture corresponds to \(\mu_{ik}u_{ijk}\) in the
  model above. Considering just the Gamma component, marginalizing out
  \(\lambda\) yields the log likelihood:

  \[ \tilde{l}(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}} \right) + \phi_{ik}^{-1} \ln\left(\frac{\phi_{ik}^{-1}\mu_{ik}^{-1}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}}\right) + \ln\Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln\Gamma(r_{ijk} + 1) - \ln\Gamma(\phi_{ik}^{-1}) \]

  It is clear \(l = \tilde{l}\), and therefore the marginal likelihoods (over
  the mixture components) are also equal.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/project2/mstephens/aksarkar/projects/singlecell-qtl/analysis/dim-reduction.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="6G",venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 45307417

  #+NAME: zinb-imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.optimize as so
    import scipy.stats as st
    import scipy.special as sp
    import sqlite3
  #+END_SRC

  #+RESULTS: zinb-imports
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+NAME: tf-imports
  #+BEGIN_SRC ipython :eval never
    import tensorflow as tf
  #+END_SRC

  #+NAME: list-local-devices
  #+BEGIN_SRC ipython :eval never
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS: list-local-devices
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 3101117888233869158, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 1
     }
     incarnation: 4298943332142850272
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:08:00.0"]
  #+END_EXAMPLE
  :END:

* Tensorflow implementation                                        :noexport:

  We optimize all of the parameters together, using one-hot encoding to map
  parameters to data points. This makes inference more amenable to running on
  the GPU.

  Use ~tensorflow~ to automatically differentiate the negative log likelihood and
  perform gradient descent.

  #+NAME: zinb-impl
  #+BEGIN_SRC ipython :eval never
    def sigmoid(x):
      """Numerically safe sigmoid"""
      return tf.clip_by_value(tf.sigmoid(x), -13, 13)

    def log(x):
      """Numerically safe log"""
      return tf.log(x + 1e-8)

    def nb_llik(x, mean, inv_disp):
      """Log likelihood of x distributed as NB

      See Hilbe 2012, eq. 8.10

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)

      """
      return (x * log(mean / inv_disp) -
              x * log(1 + mean / inv_disp) -
              inv_disp * log(1 + mean / inv_disp) +
              tf.lgamma(x + inv_disp) -
              tf.lgamma(inv_disp) -
              tf.lgamma(x + 1))

    def zinb_llik(x, mean, inv_disp, logodds, eps=1e-8):
      """Log likelihood of x distributed as ZINB

      See Hilbe 2012, eq. 11.12, 11.13

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)
      logodds - dropout log odds

      """
      case_zero = log(sigmoid(-logodds) + sigmoid(logodds) * tf.exp(nb_llik(x, mean, inv_disp)))
      case_non_zero = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
      return tf.where(tf.less(x, 1e-8), case_zero, case_non_zero)

    def fit(umi, onehot, size_factor, gene_dropout=False, ind_dropout=False, learning_rate=1e-2, max_epochs=1000):
      """Return estimated log mean and log dispersion. 

      If fitting a zero-inflated model, additionally return dropout log odds.

      umi - count matrix (n x p; float32)
      onehot - mapping of individuals to cells (m x n; float32)
      size_factor - size factor vector (n x 1; float32)
      gene_dropout - fit one dropout parameter per gene
      ind_dropout - fit one dropout parameter per individual
      init_log_mean - initial value for estimated log mean (m x p; float32)
      init_log_disp - initial value for estimated log dispersion (m x p; float32)

      If ind_dropout is True, gene_dropout must be True, otherwise raises
      ArgumentError.

      Returns:

      log_mean - log mean parameter (m x p)
      log_disp - log dispersion parameter (m x p)
      dropout - dropout log odds (1 x p if gene_dropout, n x p if ind_dropout)

      """
      n, p = umi.shape
      _, m = onehot.shape

      params = locals()
      graph = tf.Graph()
      with graph.as_default(), graph.device('/gpu:*'):
        size_factor = tf.Variable(size_factor, trainable=False)
        umi = tf.Variable(umi, trainable=False)
        onehot = tf.Variable(onehot, trainable=False)

        mean = tf.exp(tf.Variable(tf.zeros([m, p])))
        inv_disp = tf.exp(tf.Variable(tf.zeros([m, p])))

        if gene_dropout:
          if ind_dropout:
            dropout_params = tf.Variable(tf.zeros([m, p]))
            dropout = tf.matmul(onehot, dropout_params)
          else:
            dropout_params = tf.Variable(tf.zeros([1, p]))
            dropout = dropout_params
          llik = tf.reduce_mean(
            zinb_llik(umi, size_factor * tf.matmul(onehot, mean),
                      tf.matmul(onehot, inv_disp), dropout))
        elif ind_dropout:
          raise ValueError('Cannot specify individual-specific dropout without gene-specific dropout')
        else:
          llik = tf.reduce_mean(
            nb_llik(umi, size_factor * tf.matmul(onehot, mean),
                    tf.matmul(onehot, inv_disp)))

        train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-llik)
        opt = [tf.log(mean), -tf.log(inv_disp)]
        if gene_dropout:
          opt.append(dropout_params)
        curr = float('-inf')
        with tf.Session() as sess:
          sess.run(tf.global_variables_initializer())
          for i in range(max_epochs):
            _, update = sess.run([train, llik])
            if not np.isfinite(update):
              raise tf.train.NanLossDuringTrainingError
            if not i % 100:
              print(i, update)
          return sess.run(opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

** Read the data

   Read the QC'ed data.

   #+CALL: read-data-qc()

   #+RESULTS:
   :RESULTS:
   # Out[61]:
   :END:

   Onehot-encode the samples.

   #+NAME: onehot-qc
   #+BEGIN_SRC ipython
    individuals = sorted(annotations['chip_id'].unique())
    onehot = np.zeros((umi.shape[1], len(individuals)), dtype=np.float32)
    onehot[np.arange(onehot.shape[0]),annotations['chip_id'].apply(lambda x: individuals.index(x))] = 1
    onehot = pd.DataFrame(onehot, columns=individuals, index=umi.columns)
    onehot.shape
   #+END_SRC

   #+RESULTS: onehot-qc
   :RESULTS:
   # Out[5]:
   : (4995, 54)
   :END:

   Check that one-hot encoding is OK:

   #+BEGIN_SRC ipython :async t
    for _ in range(100):
      gene = np.random.choice(umi.index)
      ind = np.random.choice(individuals)
      idx = individuals.index(ind)
      assert (umi.loc[gene, (annotations['chip_id'] == ind).values] == 
              umi.loc[gene, onehot.dot(np.eye(onehot.shape[1])[idx]).astype(bool)]).all()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

** Fit NB

   Estimate means and dispersions assuming no dropout.

   #+BEGIN_SRC ipython :eval never :tangle tensorflow-nb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/mean2.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz', sep=' ', compression='gzip')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   :END:

   #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=60 --job-name=tfnb --output=tfnb.out --error=tfnb.err
    #!/bin/bash
    source activate scqtl
    python tensorflow-nb.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 43884214

** Fit ZINB

   Estimate the parameters of the zero-inflated model assuming dropout per gene.

   #+BEGIN_SRC ipython :eval never :tangle zinb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz', sep=' ', compression='gzip')
   #+END_SRC

   #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=150 --job-name=zinb --output=zinb.out --error=zinb.err
    #!/bin/bash
    source activate scqtl
    python zinb.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 43966390

** Fit ZINB2

   Estimate the parameters of the zero-inflated model assuming dropout per
   individual and gene.

   #+BEGIN_SRC ipython :eval never :tangle zinb2.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      ind_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dropout.txt.gz', sep=' ', compression='gzip')
   #+END_SRC

   #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=120 --job-name=zinb2 --output=zinb2.out --error=zinb2.err
    #!/bin/bash
    source activate scqtl
    python zinb2.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 43910744

* numpy/scipy implementation

  Optimize the negative log-likelihood.

  #+NAME: np-zinb-impl
  #+BEGIN_SRC ipython
    def log(x):
      """Numerically safe log"""
      return np.log(x + 1e-8)

    def sigmoid(x):
      """Numerically safe sigmoid"""
      lim = np.log(np.finfo(np.float64).resolution)
      return np.clip(sp.expit(x), lim, -lim)

    def nb(theta, x, size, design=None):
      """Return the per-data point log likelihood

      x ~ Poisson(size .* design' * theta[2:] * exp(theta[0]) * u)
      u ~ Gamma(exp(theta[1]), exp(theta[1]))

      """
      mean, inv_disp = np.exp(theta[:2])
      mean *= size
      if design is not None:
        mean *= np.exp(design.dot(theta[2:]))
      assert mean.shape == x.shape
      return (x * log(mean / inv_disp) -
              x * log(1 + mean / inv_disp) -
              inv_disp * log(1 + mean / inv_disp) +
              sp.gammaln(x + inv_disp) -
              sp.gammaln(inv_disp) -
              sp.gammaln(x + 1))

    def _nb(theta, x, size, design=None):
      """Return the mean negative log likelihood of x"""
      return -nb(theta, x, size, design).mean()

    def zinb(theta, x, size, design=None):
      """Return the mean negative log likelihood of x"""
      logodds, theta = theta[0], theta[1:]
      case_non_zero = -np.log1p(np.exp(logodds)) + nb(theta, x, size, design)
      case_zero = np.logaddexp(logodds - np.log1p(np.exp(logodds)), case_non_zero)
      return -np.where(x < 1, case_zero, case_non_zero).mean()

    def _fit_gene(chunk, design=None):
      if chunk[:,0].sum() == 0:
        # Without stringent QC, we need to handle the case of all zero observations
        return [-np.inf, -np.inf, np.inf, 0, 0]
      if design is None:
        x0 = np.zeros(2)
      else:
        assert design.shape[0] == chunk.shape[0]
        x0 = np.zeros(2 + design.shape[1])
      res0 = so.minimize(_nb, x0=x0, args=(chunk[:,0], chunk[:,1], design))
      pi0 = (chunk[:,0] == 0).sum() / chunk.shape[0]
      res = so.minimize(zinb, x0=[sp.logit(pi0 + 1e-8)] + list(res0.x), args=(chunk[:,0], chunk[:,1], design))
      if res0.fun < res.fun:
        # This isn't a likelihood ratio test. Numerically, our implementation of
        # ZINB can't represent pi = 0, so we need to use a separate implementation
        # for it
        log_mu, neg_log_phi, *_ = res0.x
        logit_pi = -np.inf
      else:
        logit_pi, log_mu, neg_log_phi, *_ = res.x
      mean_by_sample = chunk[:,1].ravel() * np.exp(log_mu)
      var_by_sample = mean_by_sample + np.square(mean_by_sample) * np.exp(-neg_log_phi)
      mean_by_ind = mean_by_sample.mean()
      var_by_ind = (np.square(mean_by_sample - mean_by_ind) + var_by_sample).mean()
      return [log_mu, -neg_log_phi, logit_pi, mean_by_ind, var_by_ind]

    def fit_gene(chunk, bootstraps=100):
      orig = _fit_gene(chunk)
      B = []
      for _ in range(bootstraps):
        B.append(_fit_gene(chunk[np.random.choice(chunk.shape[0], chunk.shape[0], replace=True)]))
      se = np.array(B)[:,:2].std(axis=0)
      return orig + list(se.ravel())
  #+END_SRC

  #+RESULTS: np-zinb-impl
  :RESULTS:
  # Out[28]:
  :END:

  Computing analytic SE runs into numerical problems.

  #+BEGIN_SRC ipython
    def _pois(theta, x, size):
      mean = np.exp(theta)
      mean *= size
      return (x * log(mean) - mean - sp.gammaln(x + 1)).mean()

    def _pois_jac(theta, x, size):
      mean = np.exp(theta)
      return mean * (x / mean - size).mean()
    
    def _nb_jac(theta, x, size):
      mean, inv_disp = np.exp(theta)
      T = (1 + size * mean / inv_disp)
      return mean * (x / mean - size / inv_disp * (x + inv_disp) / T).mean()

    def check_gradients(x, f, df, args=None, num_trials=100):
      x = np.array(x)
      y = f(x, *args)
      analytic_diff = df(x, *args)
      error = []
      for i in range(num_trials):
        eps = np.random.normal(scale=1e-4, size=x.shape)
        num_diff = (f(x + eps, *args) - y) / eps
        error.append(abs(num_diff - analytic_diff))
      return np.array(error)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[92]:
  :END:

  Check the parameter estimation on simulated data.

  Assuming simulated confounders \(x\) are isotropic Gaussian, we can derive
  the scale of \(\beta\) to achieve a specified fold-change in relative
  abundance:

  \[ x \sim N(0, 1) \]

  Letting \(\tau\) denote precision:

  \[ \beta \sim N(0, \tau) \]

  \[ x\beta \sim N(0, 1 + \tau) \]

  \[ \mathbb{E}[x\beta] = y = \exp\left(\frac{1}{2 (1 + \tau)}\right) \]

  \[ \tau = \frac{1 - 2 \ln y}{2 \ln y} \]  

  #+NAME: sim-impl
  #+BEGIN_SRC ipython
    def simulate(num_samples, size=None, log_mu=None, log_phi=None, logodds=None, seed=None, design=None, fold=None):
      if seed is None:
        seed = 0
      np.random.seed(seed)
      if log_mu is None:
        log_mu = np.random.uniform(low=-12, high=-8)
      if log_phi is None:
        log_phi = np.random.uniform(low=-6, high=0)
      if size is None:
        size = 1e5
      if logodds is None:
        prob = np.random.uniform()
      else:
        prob = sp.expit(logodds)
      if design is None:
        design = np.random.normal(size=(num_samples, 1))
      else:
        assert design.shape[0] == num_samples
      if fold is None or np.isclose(fold, 1):
        beta = np.array([[0]])
      else:
        assert fold > 1
        beta = np.random.normal(size=(design.shape[1], 1), scale=2 * np.log(fold) / (1 - 2 * np.log(fold)))

      n = np.exp(-log_phi)
      p = 1 / (1 + size * np.exp(log_mu + design.dot(beta) + log_phi)).ravel()
      x = np.where(np.random.uniform(size=num_samples) < prob,
                   0,
                   np.random.negative_binomial(n=n, p=p, size=num_samples))
      return np.vstack((x, size * np.ones(num_samples))).T, design

    def batch_design_matrix(num_samples, num_batches):
      """Return a matrix of binary indicators representing batch assignment"""
      design = np.zeros((num_samples, num_batches))
      design[np.arange(num_samples), np.random.choice(num_batches, size=num_samples)] = 1
      return design

    def evaluate(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial):
      x, design = simulate(num_samples=num_samples, size=num_mols,
                           log_mu=log_mu, log_phi=log_phi,
                           logodds=logodds, design=None, fold=fold, seed=trial)
      keys = ['num_samples', 'num_mols', 'log_mu', 'log_phi', 'logodds', 'trial',
              'fold', 'log_mu_hat', 'log_phi_hat', 'logodds_hat', 'mean', 'var']
      result = [num_samples, num_mols, log_mu, log_phi, logodds, trial, fold] + list(_fit_gene(x, design))
      result = {k: v for k, v in zip(keys, result)}
      log_cpm = np.log(x[:,0] + 1) - np.log(x[:,1]) + 6 * np.log(10)
      result['mean_log_cpm'] = log_cpm.mean()
      result['var_log_cpm'] = log_cpm.var()
      return result
  #+END_SRC

  #+RESULTS: sim-impl
  :RESULTS:
  # Out[280]:
  :END:

  Check the implementation actually worked.

  #+BEGIN_SRC ipython
    x, design = simulate(num_samples=1000, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, fold=1.3)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[265]:
  :END:

  #+BEGIN_SRC ipython
    _fit_gene(x)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[266]:
  #+BEGIN_EXAMPLE
    [-7.8374613012654155,
    -0.9486405356911012,
    -2.897129862156103,
    39.46697174214084,
    642.6904819352392]
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    _fit_gene(x, design)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[267]:
  #+BEGIN_EXAMPLE
    [-8.010835294475278,
    -5.361781894730521,
    -2.8841721644494522,
    33.18474128733248,
    38.35229003613272]
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    evaluate(num_samples=1000, num_mols=1e5, log_mu=-8, log_phi=-6, logodds=-3, fold=1.3, trial=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[281]:
  #+BEGIN_EXAMPLE
    {'fold': 1.3,
    'log_mu': -8,
    'log_mu_hat': -8.010835294475278,
    'log_phi': -6,
    'log_phi_hat': -5.361781894730521,
    'logodds': -3,
    'logodds_hat': -2.8841721644494522,
    'mean': 33.18474128733248,
    'mean_log_cpm': 5.612601676299008,
    'num_mols': 100000.0,
    'num_samples': 1000,
    'trial': 0,
    'var': 38.35229003613272,
    'var_log_cpm': 1.0138767222629645}
  #+END_EXAMPLE
  :END:

  Investigate what happens as the number of confounders increases.

  #+BEGIN_SRC ipython
    design = np.random.normal(size=(300, 20))
    x, _ = simulate(num_samples=300, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, design=design, fold=1.1)
    _fit_gene(x, design)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[322]:
  #+BEGIN_EXAMPLE
    [-8.00649125343069,
    -6.410804905890429,
    -2.99891159295469,
    33.32921072894424,
    35.15509338485383]
  #+END_EXAMPLE
  :END:

  Run the simulation on 28 CPUs.

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /scratch/midway2/aksarkar/singlecell/density-estimation/sim.py
    #!/usr/bin/env python3
    <<zinb-imports>>
    import multiprocessing as mp
    import sqlite3
    <<np-zinb-impl>>
    <<sim-impl>>
    args = [(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial)
            for num_samples in np.linspace(100, 1000, 5).astype(int)
            for num_mols in 1e3 * np.linspace(100, 1000, 5).astype(int)
            for log_mu in np.linspace(-12, -6, 7)
            for log_phi in np.linspace(-6, 0, 7)
            for logodds in np.linspace(-3, 3, 7)
            for fold in np.linspace(1, 1.25, 6)
            for trial in range(10)]
    with mp.Pool() as pool:
      result = pd.DataFrame.from_dict(pool.starmap(evaluate, args))
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      result.to_sql(name='simulation', con=conn, index=False, if_exists='replace')
      conn.execute('create index ix_simulation on simulation(num_samples, num_mols);')
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation/
    sbatch --partition=broadwl --mem=8G --job-name sim -n1 -c28 --exclusive --out sim.out
    #!/bin/bash
    source activate scqtl
    python /scratch/midway2/aksarkar/singlecell/density-estimation/sim.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 45301353

  Use this to check the parameter estimation for a particular gene/individual.

  #+BEGIN_SRC ipython
    def extract_data(ind, gene):
      with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
        umi = pd.read_sql("""select umi.value, annotation.size from umi, annotation 
        where annotation.chip_id == ? and gene == ? and 
        umi.sample == annotation.sample;""", con=conn, params=(ind, gene))
        return umi
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[25]:
  :END:

  Shard the data to parallelize over nodes. During this pass, write the data
  out to the database. 

  *Important notes:* 

  1. We need to use the actual number of molecules detected as the size factor,
     not the sum of QC'ed counts
  2. We need to estimate relative abundance for all genes (to match bulk), not
     just those which passed QC
  3. We need to recode categorical covariates as binary indicators. Although
     over the entire data set the number of indicators (in particular,
     ~experiment~) might be larger than the number of observations, for each
     subproblem it will not be.

  Write the annotations to the database.

  #+BEGIN_SRC ipython
    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    annotations['sample'] = annotations.apply(lambda x: '{chip_id}.{experiment:08d}.{well}'.format(**dict(x)), axis=1)
    annotations['size'] = annotations['mol_hs']
    annotations = annotations[['sample', 'chip_id', 'size']]
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      annotations.to_sql(name='annotation', con=conn, if_exists='replace')
      conn.execute('create index ix_annotation on annotation(chip_id, sample);')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  Recode categorical variables as binary indicators.

  #+NAME: recode-impl
  #+BEGIN_SRC ipython
    def recode(annotations, key):
      n = annotations.shape[0]
      cat = sorted(set(annotations[key]))
      onehot = np.zeros((n, len(cat)))
      onehot[np.arange(n), annotations[key].apply(cat.index)] = 1
      return onehot

    def build_design_matrices(annotations, keys):
      """Return dictionary mapping individuals to design matrices"""
      design = [annotations.groupby('chip_id').apply(recode, k) for k in keys]
      design = {k: np.concatenate([d.loc[k] for d in design], axis=1) for k in set(annotations['chip_id'])}
      return design
  #+END_SRC

  #+RESULTS: recode-impl
  :RESULTS:
  # Out[32]:
  :END:

  #+RESULTS:
  :RESULTS:
  # Out[341]:
  :END:

  Check that we will have enough data points to actually estimate effects:

  #+BEGIN_SRC ipython
    annotations[keep_samples.values.ravel()].groupby('chip_id').apply(lambda x: pd.Series(recode(x, 'experiment').shape)).describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[316]:
  #+BEGIN_EXAMPLE
    0          1
    count   54.000000  54.000000
    mean    96.685185   5.500000
    std     40.056952   1.969101
    min     19.000000   2.000000
    25%     78.000000   4.000000
    50%     88.500000   5.000000
    75%    102.250000   6.750000
    max    281.000000  14.000000
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    i = 0
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      conn.execute('drop table if exists umi;')
      for chunk in pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0, chunksize=1000):
        print('Processing chunk {}'.format(i))
        chunk = (chunk
                 .loc[:,keep_samples.values.ravel()])
        if not chunk.empty:
          chunk = (chunk
                   .reset_index()
                   .melt(id_vars='gene', var_name='sample')
                   .merge(annotations, on='sample')
                   .sort_values(['gene', 'chip_id', 'sample']))
          chunk.to_csv('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(i), columns=['gene', 'chip_id', 'sample', 'value', 'size'], compression='gzip', sep='\t')
          chunk[['gene', 'sample', 'value']].to_sql(name='umi', con=conn, index=False, if_exists='append')
          i += 1
        del chunk
      conn.execute('create index ix_umi on umi(gene, sample);')
  #+END_SRC

  Process each chunk in parallel.

  #+NAME: process-chunk-impl
  #+BEGIN_SRC ipython :eval never
     def compute_breaks(chunk, by_ind=False):
       # Each subproblem has fixed size, so we can just split on integer indices
       # (instead of grouping)
       num_genes = len(set(chunk['gene']))
       num_samples = len(set(chunk['sample']))
       breaks = num_samples * np.arange(num_genes).reshape(-1, 1)
       if by_ind:
         num_samples_per_ind = chunk.iloc[:num_samples]['chip_id'].value_counts().sort_index().values
         # This can't be written += because of broadcasting
         breaks = breaks + np.cumsum(num_samples_per_ind).reshape(1, -1)
       else:
         # We need to get the right end point of each subproblem (exclusive)
         breaks += num_samples
       return breaks.ravel()
  #+END_SRC

  #+BEGIN_SRC ipython :eval never :tangle /scratch/midway2/aksarkar/singlecell/np-zinb.py :noweb tangle
    <<zinb-imports>>
    import argparse
    import gzip
    import os
    import multiprocessing as mp
    import sqlite3
    <<np-zinb-impl>>
    <<recode-impl>>
    <<process-chunk-impl>>

    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    design = build_design_matrices(annotations[keep_samples.values.ravel()], ['batch', 'experiment'])

    with mp.Pool() as pool:
      chunk = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')))
      breaks = compute_breaks(chunk, by_ind=True)
      res = pool.starmap(
        _fit_gene,
        zip(np.split(chunk[['value', 'size']].values, breaks[:-1]), 
            [design[chunk.iloc[b - 1]['chip_id']] for b in breaks]))

    with gzip.open('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')), 'wt') as f:
      for b in breaks:
        gene, ind = chunk.iloc[b - 1][['gene', 'chip_id']]
        print(gene, ind, *res.pop(0), file=f)
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation
    sbatch --partition=broadwl --job-name="np-zinb" --mem=8G -a 20 -n1 -c28 --exclusive
    #!/bin/bash
    source activate scqtl
    python /scratch/midway2/aksarkar/singlecell/np-zinb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 45303406

  Populate the database.

  #+BEGIN_SRC ipython
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       conn.execute('drop table if exists params;')
       for i in range(21):
         for chunk in pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(i), sep=' ', header=None, chunksize=1000):
           chunk.columns = ['gene', 'ind', 'log_mu', 'log_phi', 'logodds', 'mean', 'var', 'log_mu_se', 'log_phi_se']
           chunk.to_sql(name='params', con=conn, index=False, if_exists='append')
       conn.execute('create index ix_params on params(gene, ind);')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Mean-variance relationship by individual

  Read the estimated parameters.

  #+BEGIN_SRC ipython
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      params = pd.read_sql('select * from params;', conn)
  #+END_SRC  

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  :END:

  Plot the mean-variance relationship for one individual.

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/mean-var-rel.png
    subset = params[params['ind'] == 'NA18501']
    subset = subset[subset['mean'] > 0]
    grid = np.linspace(subset['mean'].min(), subset['mean'].max(), 200)
    plt.clf()
    plt.gcf().set_size_inches(6, 6)
    plt.scatter(subset['mean'], subset['var'] / subset['mean'], s=2, c='k')
    for phi in np.linspace(0, 1, 5):
      plt.plot(grid, (grid + np.square(grid) * phi) / (grid + 1e-8), label='{:.2f}'.format(phi), c=colorcet.cm['bkr'](phi), ls=':', lw=1)
    plt.legend(title='Overdispersion')
    plt.xlabel('Estimated mean')
    plt.ylabel('Estimated Fano factor')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[36]:
  : Text(0,0.5,'Estimated Fano factor')
  [[file:figure/zinb.org/mean-var-rel.png]]
  :END:

* Per-gene dispersion

  The index of dispersion for observed data \(r_{ijk}\) at gene \(k\) is:

  \[ D_k = \frac{V[r_{ijk}]}{E[r_{ijk}]} \]

  where expectations (variances) are taken over individuals \(i\) and cells
  \(j\).

  Let \(g_{ijk}\) denote the zero-inflated negative binomial density as defined
  above. Then, we have:

  \[ r_{ijk} \sim \sum_{ijk} \frac{1}{N} g_{ijk}(\cdot) \]

  Fixing gene \(k\), the mixture density has expectation:

  \[ \mu_k = \frac{1}{N} \sum E[r_{ijk}] \]

  and variance ([[http://www.springer.com/us/book/9780387329093][Frühwirth-Schnatter 2006]]):

  \[ \sigma^2_k = \frac{1}{N} \sum (E[r_{ijk}] - \mu_k)^2 + V[r_{ijk}] \]

  Fixing individual \(i\) and cell \(j\), we have:

  \[ E[r_{ijk}] = R_{ij} \mu_{ik} \]

  \[ V[r_{ijk}] = \left(R_{ij} \mu_{ik} + (R_{ij} \mu_{ik})^2 \phi_{ik}\right) \]

  Here, we ignore the factor of \((1 - \pi_{ik})\) under the assumption that
  excess zeros do not reflect biology.

  #+BEGIN_SRC ipython
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      params = pd.read_sql('select * from params;', conn)
    gene_params = params.groupby('gene').apply(lambda x: pd.Series([x['mean'].mean(), (np.square(x['mean'] - x['mean'].mean()) + x['var']).mean()]))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[37]:
  :END:

  Plot the mean-variance relationship over all genes, estimated using all of
  the data and accounting for the fact that data came from different
  individuals.

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/overall-mean-var-rel.png
    subset = gene_params[gene_params[0] > 0]
    subset.columns = ['mean', 'var']
    lim = [subset['mean'].min(), subset['mean'].max()]
    grid = np.linspace(*lim, num=200)
    plt.clf()
    plt.gcf().set_size_inches(6, 6)
    plt.scatter(subset['mean'], subset['var'] / subset['mean'], s=2, c='k')
    for phi in np.geomspace(.1, 2, 5):
      plt.plot(grid, (grid + np.square(grid) * phi) / (grid + 1e-8), label='{:.2f}'.format(phi), c=colorcet.cm['bkr'](phi), ls=':', lw=1)
    plt.legend(title='Overdispersion')
    plt.xlim(lim)
    plt.ylim(lim)
    plt.xlabel('Estimated mean')
    _ = plt.ylabel('Estimated Fano factor')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[72]:
  [[file:figure/zinb.org/overall-mean-var-rel.png]]
  :END:

  Bin the data and plot the distribution by bin.

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/overall-mean-var-rel-by-quantile.png
    n_bin = 8
    subset['bin'] = pd.cut(subset['mean'], n_bin)
    subset['fano'] = subset['var'] / subset['mean']
    plt.clf()
    plt.gcf().set_size_inches(5, 4)
    for i, (k, g) in enumerate(subset.groupby('bin')):
      if len(g) < 10:
        plt.scatter((i + .5) * np.ones(g['fano'].shape[0]), g['fano'], c='k', s=4, label=k)
      else:
        plt.boxplot(g['fano'], positions=[i + .5], sym='.', widths=.25)
    plt.xticks(np.arange(n_bin + 1), ['{:.2f}'.format(x) for x in np.linspace(0, 1, n_bin + 1)])
    plt.xlim(0, n_bin)
    plt.xlabel('Quantile of estimated mean')
    _ = plt.ylabel('Estimated Fano factor')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[122]:
  [[file:figure/zinb.org/overall-mean-var-rel-by-quantile.png]]
  :END:
