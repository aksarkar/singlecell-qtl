#+TITLE: Mean/dispersion estimation
#+SETUPFILE: setup.org

* Introduction

  We take a modular approach to call QTLs:

  1. Estimate a mean and a dispersion for each individual
  2. Treat the mean/dispersion as continuous phenotypes and perform QTL mapping

  Here, we solve (1).

  1. [[*Tensorflow implementation][We implement GPU-based ML estimation]] of a zero-inflated negative binomial
     model
  2. [[*Simulation][We show in simulation]] that the estimates are unbisaed
  3. [[*Mean expression][We compare ZINB estimates of mean expression]] against sample-based estimates
  4. [[*ZINB estimates of expression noise][We estimate Fano factors]] accounting for different depths across samples
     and different means across individuals

* Model specification

  Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
  \(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell.

  \[ r_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})\text{Poisson}(\cdot; R_{ij} \mu_{ik} u_{ijk}) \]

  \[ u_{ijk} \sim \text{Gamma}(\cdot; \phi_{ik}^{-1}, \phi_{ik}^{-1}) \]

  Here, \(\mu_{ik}\) is proportional to relative expression ([[https://arxiv.org/abs/1104.3889][Pachter 2011]]), and
  \(\phi_{ik}\) is the variance of expression noise.

  Considering just the Poisson component, marginalizing out \(u\) yields the
  log likelihood:

  \[ l(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]

  Then, marginalizing over the mixture yields the log likelihood:

  \[ \ln p(r_{ijk} \mid \cdot) = \ln(\pi_{ik} + \exp(l(\cdot)))\ \text{if}\ r_{ijk} = 0 \]

  \[ \ln p(r_{ijk} \mid \cdot) = l(\cdot)\ \text{otherwise} \]

  We have enough observations per mean/dispersion parameter that simply
  minimizing the negative log likelihood should give reasonable estimates.

  This model is equivalent to a model where we assume that the underlying rate
  is a point-Gamma mixture:

  \[ r_{ijk} \mid \lambda_{ijk} \sim \mathrm{Poisson}(\cdot; R_{ij}\lambda_{ijk}) \]

  \[ \lambda_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})
  \text{Gamma}(\lambda_{ijk}; \phi_{ik}^{-1}, \phi_{ik}^{-1}\mu_{ik}^{-1}) \]

  The Gamma component of this mixture corresponds to \(\mu_{ik}u_{ijk}\) in the
  model above. Considering just the Gamma component, marginalizing out
  \(\lambda\) yields the log likelihood:

  \[ \tilde{l}(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}} \right) + \phi_{ik}^{-1} \ln\left(\frac{\phi_{ik}^{-1}\mu_{ik}^{-1}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}}\right) + \ln\Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln\Gamma(r_{ijk} + 1) - \ln\Gamma(\phi_{ik}^{-1}) \]

  It is clear \(l = \tilde{l}\), and therefore the marginal likelihoods (over
  the mixture components) are also equal.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="16G",venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 47294314

  #+NAME: zinb-imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.optimize as so
    import scipy.stats as st
    import scipy.special as sp
    import sqlite3
  #+END_SRC

  #+RESULTS: zinb-imports
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
    import colorcet
    import matplotlib.pyplot as plt
    import functools
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[10]:
  :END:

  #+NAME: tf-imports
  #+BEGIN_SRC ipython :eval never
    import tensorflow as tf
  #+END_SRC

  #+NAME: list-local-devices
  #+BEGIN_SRC ipython :eval never
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS: list-local-devices
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 3101117888233869158, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 1
     }
     incarnation: 4298943332142850272
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:08:00.0"]
  #+END_EXAMPLE
  :END:

* Tensorflow implementation

  We optimize all of the parameters together, using one-hot encoding to map
  parameters to data points. This makes inference more amenable to running on
  the GPU.

  Use ~tensorflow~ to automatically differentiate the negative log likelihood and
  perform gradient descent.

  #+NAME: tf-zinb-impl
  #+BEGIN_SRC ipython :eval never
    def nb_llik(x, mean, inv_disp):
      """Log likelihood of x distributed as NB

      See Hilbe 2012, eq. 8.10

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)

      """
      return (x * tf.log(mean / inv_disp) -
              x * tf.log(1 + mean / inv_disp) -
              inv_disp * tf.log(1 + mean / inv_disp) +
              tf.lgamma(x + inv_disp) -
              tf.lgamma(inv_disp) -
              tf.lgamma(x + 1))

    def zinb_llik(x, mean, inv_disp, logodds):
      """Log likelihood of x distributed as ZINB

      See Hilbe 2012, eq. 11.12, 11.13

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)
      logodds - logit proportion of excess zeros

      """
      # Important identities:
      # log(x + y) = log(x) + softplus(y - x)
      # log(sigmoid(x)) = -softplus(-x)
      case_zero = -tf.nn.softplus(-logodds) + tf.nn.softplus(nb_llik(x, mean, inv_disp) + tf.nn.softplus(-logodds))
      case_non_zero = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
      return tf.where(tf.less(x, 1), case_zero, case_non_zero)

    # https://github.com/junfengwen/AMSGrad/blob/a00e3f4bcb3ba16b2fe67c75dd8643670bded0c9/optimizers.py

    from tensorflow.python.framework import ops
    from tensorflow.python.ops import control_flow_ops
    from tensorflow.python.ops import math_ops
    from tensorflow.python.ops import gen_math_ops
    from tensorflow.python.ops import variable_scope
    from tensorflow.python.training import optimizer

    class AMSGrad(optimizer.Optimizer):
      """The AMSGrad algorithm in the paper

      Reddi, Kale, Kumar, On the Convergence of Adam and Beyond, ICLR 2018

      https://openreview.net/forum?id=ryQu7f-RZ

      """
      def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999,
                   epsilon=1e-8, use_locking=False, name="AMSGrad"):
        super(AMSGrad, self).__init__(use_locking, name)
        self._lr = learning_rate
        self._beta1 = beta1
        self._beta2 = beta2
        self._epsilon = epsilon

        self._lr_t = None
        self._beta1_t = None
        self._beta2_t = None
        self._epsilon_t = None

        self._beta1_power = None
        self._beta2_power = None

      def _create_slots(self, var_list):

        first_var = min(var_list, key=lambda x: x.name)

        create_new = self._beta1_power is None

        if create_new:
          with ops.colocate_with(first_var):
            self._beta1_power = variable_scope.variable(self._beta1,
                                                        name="beta1_power",
                                                        trainable=False)
            self._beta2_power = variable_scope.variable(self._beta2,
                                                        name="beta2_power",
                                                        trainable=False)
        # Create slots for the first and second moments.
        for v in var_list:
          # first moment est
          self._zeros_slot(v, "first_mom", self._name)
          # second moment est
          self._zeros_slot(v, "second_mom", self._name)
          self._zeros_slot(v, "second_mom_max", self._name)

      def _prepare(self):
        self._lr_t = ops.convert_to_tensor(self._lr)
        self._beta1_t = ops.convert_to_tensor(self._beta1)
        self._beta2_t = ops.convert_to_tensor(self._beta2)
        self._epsilon_t = ops.convert_to_tensor(self._epsilon)
        self._one_minus_beta1 = ops.convert_to_tensor(1. - self._beta1)
        self._one_minus_beta2 = ops.convert_to_tensor(1. - self._beta2)

      def _apply_dense(self, grad, var):
        # bias-corrected learning rate
        lr = self._lr_t * math_ops.sqrt(1. - self._beta2_power) / (1. - self._beta1_power)
        first_mom = self.get_slot(var, "first_mom")
        second_mom = self.get_slot(var, "second_mom")
        second_mom_max = self.get_slot(var, "second_mom_max")
        first_update = first_mom.assign(self._beta1_t * first_mom +
                                        self._one_minus_beta1 * grad,
                                        use_locking=self._use_locking)
        second_update = second_mom.assign(self._beta2_t * second_mom +
                                          self._one_minus_beta2 * math_ops.square(grad),
                                          use_locking=self._use_locking)
        # AMSGrad compared to ADAM
        second_max_update = second_mom_max.assign(gen_math_ops.maximum(second_mom_max,
                                                                       second_update))
        var_update = var.assign_sub(lr * first_update / (math_ops.sqrt(second_max_update) +
                                                         self._epsilon_t),
                                    use_locking=self._use_locking)
        return control_flow_ops.group(*[var_update, first_update,
                                        second_update, second_max_update])

      def _apply_sparse(self, grad, var):
        # just a copy of the dense case, not properly implemented yet
        return self._apply_dense(grad, var)

      def _finish(self, update_ops, name_scope):
        # Update the power accumulators.
        with ops.control_dependencies(update_ops):
          with ops.colocate_with(self._beta1_power):
            update_beta1 = self._beta1_power.assign(
              self._beta1_power * self._beta1,
              use_locking=self._use_locking)
            update_beta2 = self._beta2_power.assign(
              self._beta2_power * self._beta2_t,
              use_locking=self._use_locking)
        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2],
                                      name=name_scope)

    def fit(umi, onehot, size_factor, design, learning_rate=1e-2, max_epochs=1000, fit_null=False, return_llik=False):
      """Return estimated log mean and log dispersion. 

      umi - count matrix (n x p; float32)
      onehot - mapping of individuals to cells (m x n; float32)
      size_factor - size factor vector (n x 1; float32)
      design - confounder matrix (n x q; float32)
      fit_null - whether to fit the null model (common dispersion)
      return_llik - whether to return the log likelihood matrix

      Returns:

      log_mean - log mean parameter (m x p)
      log_disp - log dispersion parameter (m x p)
      logodds - logit proportion of excess zeros (m x p)
      llik - if return_llik, log likelihood per gene (p)

      """
      n, p = umi.shape
      _, m = onehot.shape
      _, k = design.shape

      params = locals()
      graph = tf.Graph()
      with graph.as_default(), graph.device('/gpu:*'):
        size_factor = tf.Variable(size_factor, trainable=False)
        umi = tf.Variable(umi, trainable=False)
        onehot = tf.Variable(onehot, trainable=False)
        design = tf.Variable(design, trainable=False)

        mean = tf.exp(tf.Variable(tf.zeros([m, p])))
        if fit_null:
          inv_disp = tf.exp(tf.Variable(tf.zeros([1, p])))
        else:
          inv_disp = tf.exp(tf.Variable(tf.zeros([m, p])))
        logodds = tf.Variable(tf.zeros([m, p]))
        beta = tf.Variable(tf.zeros([k, p]))

        if fit_null:
          llik = zinb_llik(umi, size_factor * tf.matmul(onehot, mean) * tf.exp(tf.matmul(design, beta)),
                           inv_disp, tf.matmul(onehot, logodds))
        else:
          llik = zinb_llik(umi, size_factor * tf.matmul(onehot, mean) * tf.exp(tf.matmul(design, beta)),
                           tf.matmul(onehot, inv_disp), tf.matmul(onehot, logodds))

        loss = -tf.reduce_mean(llik)
        train = AMSGrad(learning_rate=learning_rate).minimize(loss)
        opt = [tf.log(mean), -tf.log(inv_disp), logodds]
        if return_llik:
          opt.append(tf.reduce_sum(llik, axis=0))
        curr = float('-inf')
        with tf.Session() as sess:
          sess.run(tf.global_variables_initializer())
          for i in range(max_epochs):
            _, update = sess.run([train, loss])
            if not np.isfinite(update):
              raise tf.train.NanLossDuringTrainingError
            if not i % 500:
              print(i, update)
          return sess.run(opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

** Simulation

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-sim.py
     <<zinb-imports>>
     <<tf-imports>>
     <<sim-impl>>
     <<tf-zinb-impl>>

     def evaluate(num_samples, num_mols, num_trials=10):
       # This will be reset inside the simulation to generate counts, but we need to
       # fix it to get one design matrix for all the simulated genes
       # def simulate(num_samples, size=None, log_mu=None, log_phi=None, logodds=None, seed=None, design=None, fold=None):
       design = np.zeros((num_samples * num_trials, 1))
       # Important: generate all of the samples for each trial in one shot, and use
       # one-hot encoding to get separate estimates
       args = [(num_samples * num_trials, num_mols, log_mu, log_phi, logodds, None, None, None)
               for log_mu in np.linspace(-12, -6, 7)
               for log_phi in np.linspace(-4, 0, 5)
               for logodds in np.linspace(-3, 3, 7)]
       umi = np.concatenate([simulate(*a)[0][:,:1] for a in args], axis=1)
       onehot = np.zeros((num_samples * num_trials, num_trials))
       onehot[np.arange(onehot.shape[0]), np.arange(onehot.shape[0]) // num_samples] = 1

       log_mu, log_phi, logodds = fit(
         umi=umi.astype(np.float32),
         onehot=onehot.astype(np.float32),
         design=design.astype(np.float32),
         size_factor=(num_mols * np.ones((num_samples * num_trials, 1))).astype(np.float32),
         learning_rate=5e-2,
         max_epochs=4000)
       result = pd.DataFrame(
         [(a[0] // num_trials, int(a[1]), int(a[2]), int(a[3]), int(a[4]), a[-1], trial)
          for a in args
          for trial in range(num_trials)],
         columns=['num_samples', 'num_mols', 'log_mu', 'log_phi', 'logodds', 'fold', 'trial'])
       # Important: the results need to be transposed before flattening
       result['log_mu_hat'] = log_mu.ravel(order='F')
       result['log_phi_hat'] = log_phi.ravel(order='F')
       result['logodds_hat'] = logodds.ravel(order='F')
       result['mean'] = result['num_mols'] * np.exp(result['log_mu_hat'])
       result['var'] = result['mean'] + np.square(result['mean']) * np.exp(result['log_phi_hat'])
       log_cpm = np.log(np.ma.masked_values(umi.reshape(num_trials, -1, umi.shape[-1]), 0)) - np.log(num_mols) + 6 * np.log(10)
       result['mean_log_cpm'] = log_cpm.mean(axis=1).ravel(order='F')
       result['var_log_cpm'] = log_cpm.var(axis=1).ravel(order='F')
       return result

     res = pd.concat([evaluate(num_samples, num_mols)
                      for num_samples in (10000, 1000, 95)
                      for num_mols in (114026, 500000, 1000000)])
     res.to_csv('simulation.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation/
     sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-sim --output=sim.out
     #!/bin/bash
     source activate scqtl
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-sim.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 47292972

   Read the results.

   #+BEGIN_SRC ipython
     result = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/simulation.txt.gz', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[3]:
   :END:

   Get the latent mean and variance.

   #+BEGIN_SRC ipython
     result['latent_mean'] = np.exp(result['log_mu'] - np.log1p(np.exp(result['logodds'])))
     result['latent_mean_hat'] = np.exp(result['log_mu_hat'] - np.log1p(np.exp(result['logodds_hat'])))
     result['latent_var'] = np.exp(2 * result['log_mu'] + result['log_phi'] - np.log1p(np.exp(result['logodds']))) + np.exp(-np.log1p(np.exp(result['logodds'])) - np.log1p(np.exp(-result['logodds'])) + 2 * result['log_mu'])
     result['latent_var_hat'] = np.exp(2 * result['log_mu_hat'] + result['log_phi_hat'] - np.log1p(np.exp(result['logodds_hat']))) + np.exp(-np.log1p(np.exp(result['logodds_hat'])) - np.log1p(np.exp(-result['logodds_hat'])) + 2 * result['log_mu_hat'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[66]:
   :END:

   Plot the accuracy of the estimated parameters and derived quantities, fixing
   the experiment size.

   #+BEGIN_SRC ipython :ipyfile figure/zinb.org/sim.png
     exp_pass = np.logical_and(result['num_samples'] == 95, result['num_mols'] == 114026)
     mu_pass = result['log_mu'] > -10
     pi_pass = result['logodds'] < 0

     plt.clf()
     fig, ax = plt.subplots(2, 3)
     fig.set_size_inches(8, 5)

     subset = result.loc[np.logical_and.reduce(np.vstack([exp_pass, pi_pass]))]
     ax[0, 0].scatter(subset['log_mu'], subset['log_mu_hat'], s=2, c='k')
     ax[0, 0].plot(ax[0, 0].get_xlim(), ax[0, 0].get_xlim(), c='r', ls=':', lw=1)
     ax[0, 0].set_xlabel('True $\ln(\mu)$')
     ax[0, 0].set_ylabel('Estimated $\ln(\mu)$')

     ax[1, 0].set_xscale('log')
     ax[1, 0].set_yscale('log')
     ax[1, 0].scatter(subset['latent_mean'], subset['latent_mean_hat'], s=2, c='k')
     ax[1, 0].plot(ax[1, 0].get_xlim(), ax[1, 0].get_xlim(), c='r', ls=':', lw=1)
     ax[1, 0].set_xlabel('True latent mean')
     ax[1, 0].set_ylabel('Estimated latent mean')

     subset = result.loc[np.logical_and.reduce(np.vstack([exp_pass, mu_pass, pi_pass]))]
     ax[0, 1].scatter(subset['log_phi'], subset['log_phi_hat'], s=2, c='k')
     ax[0, 1].plot(ax[0, 1].get_xlim(), ax[0, 1].get_xlim(), c='r', ls=':', lw=1)
     ax[0, 1].set_xlabel('True $\ln(\phi)$')
     ax[0, 1].set_ylabel('Estimated $\ln(\phi)$')

     ax[1, 1].set_xscale('log')
     ax[1, 1].set_yscale('log')
     ax[1, 1].scatter(subset['latent_var'], subset['latent_var_hat'], s=2, c='k')
     ax[1, 1].plot(ax[1, 1].get_xlim(), ax[1, 1].get_xlim(), c='r', ls=':', lw=1)
     ax[1, 1].set_xlabel('True latent variance')
     ax[1, 1].set_ylabel('Estimated latent variance')

     subset = result.loc[exp_pass]
     ax[0, 2].scatter(subset['logodds'], subset['logodds_hat'], s=2, c='k')
     ax[0, 2].plot(ax[0, 2].get_xlim(), ax[0, 2].get_xlim(), c='r', ls=':', lw=1)
     ax[0, 2].set_xlabel('True $\mathrm{logit}(\pi)$')
     ax[0, 2].set_ylabel('Estimated $\mathrm{logit}(\pi)$')

     ax[1, 2].set_axis_off()
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[69]:
   [[file:figure/zinb.org/sim.png]]
   :END:

   Plot the accuracy of estimated latent mean and variance as a function of
   number of molecules, holding the number of cells fixed at the median value
   in the real data.

   #+BEGIN_SRC ipython :ipyfile figure/zinb.org/simulation-latent-accuracy-vs-mols.png
     plt.clf()
     fig, ax = plt.subplots(3, 2)
     fig.set_size_inches(6, 9)

     for i, (k, g) in enumerate(result.loc[result['num_samples'] == 95].groupby('num_mols')):
       mu_pass = g['log_mu'] > -10
       pi_pass = g['logodds'] < 0
       subset = g.loc[pi_pass]
       ax[i, 0].semilogx()
       ax[i, 0].semilogy()
       ax[i, 0].scatter(subset['latent_mean'], subset['latent_mean_hat'], s=2, c='k')
       ax[i, 0].plot(ax[i, 0].get_xlim(), ax[i, 0].get_xlim(), c='r', ls=':', lw=1)
       ax[i, 0].set_xlabel('True expression mean')
       ax[i, 0].set_ylabel('Estimated expression mean')
       ax[i, 0].set_title('Molecules $= {}$'.format(k))

       subset = g.loc[functools.reduce(np.logical_and, [mu_pass, pi_pass])]
       ax[i, 1].semilogx()
       ax[i, 1].semilogy()
       ax[i, 1].scatter(subset['latent_var'], subset['latent_var_hat'], s=2, c='k')
       ax[i, 1].plot(ax[i, 1].get_xlim(), ax[i, 1].get_xlim(), c='r', ls=':', lw=1)
       ax[i, 1].set_xlabel('True expression variance')
       ax[i, 1].set_ylabel('Estimated expression variance')
       ax[i, 1].set_title('Molecules $= {}$'.format(k))

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[72]:
   [[file:figure/zinb.org/simulation-latent-accuracy-vs-mols.png]]
   :END:

   Plot the accuracy of estimated latent mean and variance as a function of
   number of cells, holding the number of molecules fixed at the median value
   in the real data.

   #+BEGIN_SRC ipython :ipyfile figure/zinb.org/simulation-latent-accuracy-vs-cells.png
     plt.clf()
     fig, ax = plt.subplots(3, 2)
     fig.set_size_inches(6, 9)

     for i, (k, g) in enumerate(result.loc[result['num_mols'] == 114026].groupby('num_samples')):
       mu_pass = g['log_mu'] > -10
       pi_pass = g['logodds'] < 0
       subset = g.loc[pi_pass]
       ax[i, 0].semilogx()
       ax[i, 0].semilogy()
       ax[i, 0].scatter(subset['latent_mean'], subset['latent_mean_hat'], s=2, c='k')
       ax[i, 0].plot(ax[i, 0].get_xlim(), ax[i, 0].get_xlim(), c='r', ls=':', lw=1)
       ax[i, 0].set_xlabel('True expression mean')
       ax[i, 0].set_ylabel('Estimated expression mean')
       ax[i, 0].set_title('$n = {}$'.format(k))

       subset = g.loc[functools.reduce(np.logical_and, [mu_pass, pi_pass])]
       ax[i, 1].semilogx()
       ax[i, 1].semilogy()
       ax[i, 1].scatter(subset['latent_var'], subset['latent_var_hat'], s=2, c='k')
       ax[i, 1].plot(ax[i, 1].get_xlim(), ax[i, 1].get_xlim(), c='r', ls=':', lw=1)
       ax[i, 1].set_xlabel('True expression variance')
       ax[i, 1].set_ylabel('Estimated expression variance')
       ax[i, 1].set_title('$n = {}$'.format(k))

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[75]:
   [[file:figure/zinb.org/simulation-latent-accuracy-vs-cells.png]]
   :END:

** Fit ZINB2

   Read the data.

   #+NAME: read-data-qc-impl
   #+BEGIN_SRC ipython
     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     annotations = annotations.loc[keep_samples.values.ravel()]
     header = sorted(set(annotations['chip_id']))
     umi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0).loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
     index = umi.index
   #+END_SRC

   #+RESULTS: read-data-qc-impl
   :RESULTS:
   # Out[3]:
   :END:

   Prepare the design matrix of covariates.

   #+CALL: recode-impl()

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   #+NAME: prepare-covars
   #+BEGIN_SRC ipython
     onehot = recode(annotations, 'chip_id')

     designs = []

     # Null covariate model
     designs.append(np.zeros((onehot.shape[0], 1)))

     chip = recode(annotations, 'experiment')
     chip -= chip.mean(axis=0)
     designs.append(chip)

     # These explain most PVE of circular pseudotime (Joyce Hsiao, personal
     # communication)
     cell_cycle_genes = [
       'ENSG00000094804', # CDC6
       'ENSG00000170312', # CDK1
       'ENSG00000175063', # UBE2C
       'ENSG00000131747', # TOP2A
       'ENSG00000197061', # HIST1H4C
       ]
     cell_cycle = (umi.loc[cell_cycle_genes].values / annotations['mol_hs'].values).reshape(-1, len(cell_cycle_genes))
     cell_cycle -= cell_cycle.mean(axis=0)
     cell_cycle /= cell_cycle.std(axis=0)
     designs.append(cell_cycle)

     designs.append(np.concatenate([chip, cell_cycle], axis=1))
   #+END_SRC

   #+RESULTS: prepare-covars
   :RESULTS:
   # Out[5]:
   :END:

   Estimate the parameters of the zero-inflated model assuming dropout per
   individual and gene.

   #+NAME: tf-zinb.py
   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb.py
     <<zinb-imports>>
     <<tf-imports>>
     import argparse
     <<tf-zinb-impl>>
     <<recode-impl>>

     parser = argparse.ArgumentParser()
     parser.add_argument('--design', help='Design matrix of confounders', choices=list(range(4)), type=int)
     args = parser.parse_args()
     <<read-data-qc-impl>>
     <<prepare-covars>>
     log_mu, log_phi, logodds = fit(
       umi=umi.T.astype(np.float32),
       onehot=onehot.astype(np.float32),
       design=designs[args.design].astype(np.float32),
       size_factor=annotations['mol_hs'].astype(np.float32).values.reshape(-1, 1),
       learning_rate=5e-2,
       max_epochs=4000)
     pd.DataFrame(log_mu.T, index=index, columns=header).to_csv('zi2-log-mu.txt.gz', sep=' ', compression='gzip')
     pd.DataFrame(log_phi.T, index=index, columns=header).to_csv('zi2-log-phi.txt.gz', sep=' ', compression='gzip')
     pd.DataFrame(logodds.T, index=index, columns=header).to_csv('zi2-logodds.txt.gz', sep=' ', compression='gzip')
   #+END_SRC

   #+NAME: run-tf-zinb
   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=240 --job-name=tf-zinb --output=zinb2.out
     #!/bin/bash
     source activate scqtl
     mkdir -p design{0,1,2,3}
     pushd design0
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb.py --design 0
     popd
     pushd design1
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb.py --design 1
     popd
     pushd design2
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb.py --design 2
     popd
     pushd design3
     python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb.py --design 3
     popd
   #+END_SRC

   #+RESULTS: run-tf-zinb
   : Submitted batch job 47228454

   #+NAME: rsync
   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation/
     sbatch --partition=broadwl
     #!/bin/bash
     cat >.rsync-filter <<EOF
     + */
     + zi2*.txt.gz
     - *
     EOF
     rsync -FFau --delete . /project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/
   #+END_SRC

   #+RESULTS: rsync
   : Submitted batch job 47231145

* numpy/scipy implementation                                       :noexport:

  Optimize the negative log-likelihood.

  #+NAME: np-zinb-impl2
  #+BEGIN_SRC ipython
    def log(x):
      """Numerically safe log"""
      return np.log(x + 1e-8)

    def sigmoid(x):
      """Numerically safe sigmoid"""
      lim = np.log(np.finfo(np.float64).resolution)
      return np.clip(sp.expit(x), lim, -lim)

    def nb(theta, x, size, onehot, design):
      """Return the per-data point log likelihood

      x ~ Poisson(size .* design' * theta[2 * m:k] * exp(onehot * theta[:m]) * u)
      u ~ Gamma(exp(onehot * theta[m:2 * m]), exp(onehot * theta[m:2 * m]))

      theta - (2 * m + k, 1)
      x - (n, 1)
      size - (n, 1)
      onehot - (n, m)
      design - (n, k)

      """
      n, m = onehot.shape
      assert x.shape == (n,)
      assert size.shape == (n,)
      assert design.shape[0] == n
      assert theta.shape == (2 * m + design.shape[1],)
      mean = size * np.exp(onehot.dot(theta[:m]) + design.dot(theta[2 * m:]))
      assert mean.shape == (n,)
      inv_disp = onehot.dot(np.exp(theta[m:2 * m]))
      assert inv_disp.shape == (n,)
      return (x * log(mean / inv_disp) -
              x * log(1 + mean / inv_disp) -
              inv_disp * log(1 + mean / inv_disp) +
              sp.gammaln(x + inv_disp) -
              sp.gammaln(inv_disp) -
              sp.gammaln(x + 1))

    def _nb(theta, x, size, onehot, design=None):
      """Return the mean negative log likelihood of x"""
      return -nb(theta, x, size, onehot, design).mean()

    def zinb(theta, x, size, onehot, design=None):
      """Return the mean negative log likelihood of x"""
      n, m = onehot.shape
      logodds, theta = theta[:m], theta[m:]
      case_non_zero = -np.log1p(np.exp(onehot.dot(logodds))) + nb(theta, x, size, onehot, design)
      case_zero = np.logaddexp(onehot.dot(logodds - np.log1p(np.exp(logodds))), case_non_zero)
      return -np.where(x < 1, case_zero, case_non_zero).mean()

    def _fit_gene(chunk, onehot, design=None):
      n, m = onehot.shape
      assert chunk.shape[0] == n
      # We need to take care here to initialize mu=-inf for all zero observations
      x0 = np.log((onehot * chunk[:,:1]).sum(axis=0) / onehot.sum(axis=0)) - np.log(np.ma.masked_values(onehot, 0) * chunk[:,1:]).mean(axis=0).compressed()
      x0 = np.hstack((x0, np.zeros(m)))
      if design is not None:
        assert design.shape[0] == n
        design -= design.mean(axis=0)
        x0 = np.hstack((x0, np.zeros(design.shape[1])))
      res0 = so.minimize(_nb, x0=x0, args=(chunk[:,0], chunk[:,1], onehot, design))
      res = so.minimize(zinb, x0=list(np.zeros(m)) + list(res0.x), args=(chunk[:,0], chunk[:,1], onehot, design))
      if res0.fun < res.fun:
        # This isn't a likelihood ratio test. Numerically, our implementation of
        # ZINB can't represent pi = 0, so we need to use a separate implementation
        # for it
        log_mu = res0.x[:m]
        neg_log_phi = res0.x[m:2 * m]
        logit_pi = np.zeros(m)
        logit_pi.fill(-np.inf)
      else:
        logit_pi = res.x[:m]
        log_mu = res.x[m:2 * m]
        neg_log_phi = res.x[2 * m:3 * m]
      mean_by_sample = chunk[:,1] * onehot.dot(np.exp(log_mu))
      var_by_sample = mean_by_sample + np.square(mean_by_sample) * onehot.dot(np.exp(-neg_log_phi))
      mean_by_ind = np.ma.masked_equal(onehot * mean_by_sample.reshape(-1, 1), 0).mean(axis=0).filled(0)
      var_by_ind = np.ma.masked_equal(onehot * (np.square(mean_by_sample - onehot.dot(mean_by_ind)) + var_by_sample).reshape(-1, 1), 0).mean(axis=0).filled(0)
      return [log_mu, -neg_log_phi, logit_pi, mean_by_ind, var_by_ind]

    def fit_gene(chunk, bootstraps=100):
      orig = _fit_gene(chunk)
      B = []
      for _ in range(bootstraps):
        B.append(_fit_gene(chunk[np.random.choice(chunk.shape[0], chunk.shape[0], replace=True)]))
      se = np.array(B)[:,:2].std(axis=0)
      return orig + list(se.ravel())
  #+END_SRC

  #+RESULTS: np-zinb-impl2
  :RESULTS:
  # Out[26]:
  :END:

  Computing analytic SE runs into numerical problems.

  #+BEGIN_SRC ipython
    def _pois(theta, x, size):
      mean = np.exp(theta)
      mean *= size
      return (x * log(mean) - mean - sp.gammaln(x + 1)).mean()

    def _pois_jac(theta, x, size):
      mean = np.exp(theta)
      return mean * (x / mean - size).mean()
    
    def _nb_jac(theta, x, size):
      mean, inv_disp = np.exp(theta)
      T = (1 + size * mean / inv_disp)
      return mean * (x / mean - size / inv_disp * (x + inv_disp) / T).mean()

    def check_gradients(x, f, df, args=None, num_trials=100):
      x = np.array(x)
      y = f(x, *args)
      analytic_diff = df(x, *args)
      error = []
      for i in range(num_trials):
        eps = np.random.normal(scale=1e-4, size=x.shape)
        num_diff = (f(x + eps, *args) - y) / eps
        error.append(abs(num_diff - analytic_diff))
      return np.array(error)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[92]:
  :END:

** Simulation

  Check the parameter estimation on simulated data.

  Assuming simulated confounders \(x\) are isotropic Gaussian, we can derive
  the scale of \(\beta\) to achieve a specified fold-change in relative
  abundance:

  \[ x \sim N(0, 1) \]

  Letting \(\tau\) denote precision:

  \[ \beta \sim N(0, \tau) \]

  \[ x\beta \sim N(0, 1 + \tau) \]

  \[ \mathbb{E}[x\beta] = y = \exp\left(\frac{1}{2 (1 + \tau)}\right) \]

  \[ \tau = \frac{1 - 2 \ln y}{2 \ln y} \]  

  #+NAME: sim-impl
  #+BEGIN_SRC ipython
    def simulate(num_samples, size=None, log_mu=None, log_phi=None, logodds=None, seed=None, design=None, fold=None):
      if seed is None:
        seed = 0
      np.random.seed(seed)
      if log_mu is None:
        log_mu = np.random.uniform(low=-12, high=-8)
      if log_phi is None:
        log_phi = np.random.uniform(low=-6, high=0)
      if size is None:
        size = 1e5
      if logodds is None:
        prob = np.random.uniform()
      else:
        prob = sp.expit(logodds)
      if design is None:
        design = np.random.normal(size=(num_samples, 1))
      else:
        assert design.shape[0] == num_samples
      if fold is None or np.isclose(fold, 1):
        beta = np.array([[0]])
      else:
        assert fold > 1
        beta = np.random.normal(size=(design.shape[1], 1), scale=2 * np.log(fold) / (1 - 2 * np.log(fold)))

      n = np.exp(-log_phi)
      p = 1 / (1 + size * np.exp(log_mu + design.dot(beta) + log_phi)).ravel()
      x = np.where(np.random.uniform(size=num_samples) < prob,
                   0,
                   np.random.negative_binomial(n=n, p=p, size=num_samples))
      return np.vstack((x, size * np.ones(num_samples))).T, design
  #+END_SRC

  #+RESULTS: sim-impl2
  :RESULTS:
  # Out[31]:
  :END:

  #+NAME: numpy-eval-impl
  #+BEGIN_SRC ipython
    def batch_design_matrix(num_samples, num_batches):
      """Return a matrix of binary indicators representing batch assignment"""
      design = np.zeros((num_samples, num_batches))
      design[np.arange(num_samples), np.random.choice(num_batches, size=num_samples)] = 1
      return design

    def evaluate(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial):
      x, design = simulate(num_samples=num_samples, size=num_mols,
                           log_mu=log_mu, log_phi=log_phi,
                           logodds=logodds, design=None, fold=fold, seed=trial)
      onehot = np.ones((num_samples, 1))
      keys = ['num_samples', 'num_mols', 'log_mu', 'log_phi', 'logodds', 'trial',
              'fold', 'log_mu_hat', 'log_phi_hat', 'logodds_hat', 'mean', 'var']
      result = [num_samples, num_mols, log_mu, log_phi, logodds, trial, fold] + [param[0] for param in _fit_gene(x, onehot, design)]
      result = {k: v for k, v in zip(keys, result)}
      eps = .5 / num_mols
      log_cpm = (np.log(np.ma.masked_values(x[:,0], 0) + eps) - np.log(x[:,1] + 2 * eps) + 6 * np.log(10)).compressed()
      result['mean_log_cpm'] = log_cpm.mean()
      result['var_log_cpm'] = log_cpm.var()
      return result

  #+END_SRC

  Check the implementation actually worked.

  #+BEGIN_SRC ipython
    x1, design1 = simulate(num_samples=1000, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, design=batch_design_matrix(1000, 2), fold=1.1)
    x2, design2 = simulate(num_samples=1000, size=1e5, log_mu=-9, log_phi=-6, logodds=-3, seed=0, design=batch_design_matrix(1000, 2), fold=1.1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[6]:
  :END:

  #+BEGIN_SRC ipython
    x = np.vstack((x1, x2))
    design = np.vstack((design1, design2))
    onehot = np.zeros((2000, 2))
    onehot[:1000,0] = 1
    onehot[1000:,1] = 1
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[7]:
  :END:

  #+BEGIN_SRC ipython
    so.minimize(_nb, np.zeros(6), (x[:,0], x[:,1], onehot, design - design.mean(axis=0)))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[260]:
  #+BEGIN_EXAMPLE
    fun: 3.7693708861600173
    hess_inv: array([[ 2.95492914e-01, -2.26820271e-02, -6.88886930e-02,
    3.66568258e-02, -1.35008739e-02,  1.33990791e-02],
    [-2.26820271e-02,  2.76704513e-01,  4.55879587e-03,
    6.39326308e-02, -2.03557128e-02,  2.03675939e-02],
    [-6.88886930e-02,  4.55879587e-03,  8.40491928e+00,
    3.47009715e-01,  1.14991770e-02, -1.17004131e-02],
    [ 3.66568258e-02,  6.39326308e-02,  3.47009715e-01,
    1.93344347e+01, -8.18164095e-03,  6.19982324e-03],
    [-1.35008739e-02, -2.03557128e-02,  1.14991770e-02,
    -8.18164095e-03,  6.45950264e-01,  3.54059802e-01],
    [ 1.33990791e-02,  2.03675939e-02, -1.17004131e-02,
    6.19982324e-03,  3.54059802e-01,  6.45930363e-01]])
    jac: array([ 6.82473183e-06,  8.34465027e-06,  1.07288361e-06,  8.34465027e-07,
    3.75509262e-06, -4.08291817e-06])
    message: 'Optimization terminated successfully.'
    nfev: 280
    nit: 30
    njev: 35
    status: 0
    success: True
    x: array([-7.79891405, -8.79063513,  2.05714587,  2.56621654,  0.16492975,
    -0.16494031])
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    so.minimize(zinb, np.zeros(8), (x[:,0], x[:,1], onehot, design - design.mean(axis=0)))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[261]:
  #+BEGIN_EXAMPLE
    fun: 3.1120455141161147
    hess_inv: array([[ 3.91509877e+01, -6.82255452e+00,  3.56651927e-03,
    -3.60544664e-03,  1.89769061e-03, -3.67396633e-05,
    -5.05335375e-03,  2.07017842e-03],
    [-6.82255452e+00,  3.63677578e+01, -2.89665006e-03,
    1.89766982e-03, -1.15515147e-03,  1.19995032e-04,
    4.03751660e-03, -1.28881738e-03],
    [ 3.56651927e-03, -2.89665006e-03,  1.00060484e-05,
    -4.75805864e-06, -1.52866758e-06, -9.78212511e-07,
    -7.93108063e-09, -9.33841063e-06],
    [-3.60544664e-03,  1.89766982e-03, -4.75805864e-06,
    9.43027846e-06, -1.92946942e-08, -1.76906831e-07,
    -3.04179309e-07,  5.35291550e-06],
    [ 1.89769061e-03, -1.15515147e-03, -1.52866758e-06,
    -1.92946942e-08,  2.96476652e-06,  1.16116940e-06,
    -4.91977746e-06,  2.37359794e-06],
    [-3.67396633e-05,  1.19995032e-04, -9.78212511e-07,
    -1.76906831e-07,  1.16116940e-06,  1.48628846e-06,
    -1.36236523e-06,  7.97280863e-07],
    [-5.05335375e-03,  4.03751660e-03, -7.93108063e-09,
    -3.04179309e-07, -4.91977746e-06, -1.36236523e-06,
    1.60495775e-05, -3.42044027e-06],
    [ 2.07017842e-03, -1.28881738e-03, -9.33841063e-06,
    5.35291550e-06,  2.37359794e-06,  7.97280863e-07,
    -3.42044027e-06,  1.96937195e-05]])
    jac: array([-2.98023224e-07,  7.15255737e-07,  9.99987125e-04, -3.08364630e-04,
    5.21874428e-03,  7.71874189e-03,  1.99797750e-03,  9.57250595e-05])
    message: 'Desired error not necessarily achieved due to precision loss.'
    nfev: 1942
    nit: 80
    njev: 193
    status: 2
    success: False
    x: array([-3.07858135, -3.07854889, -7.75387174, -8.74631249, 12.91019024,
    13.26839552,  0.04538458, -0.28914969])
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    _fit_gene(x, onehot)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[262]:
  #+BEGIN_EXAMPLE
    [array([-7.74001987, -8.73200364]),
    array([-3.50778747, -3.59795896]),
    array([-3.07854886, -3.07859407]),
    array([43.50629319, 16.13388663]),
    array([100.22044275,  23.26084595])]
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    _fit_gene(x, onehot, design)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[8]:
  #+BEGIN_EXAMPLE
    [array([-7.74986202, -8.7418084 ]),
    array([-6.82644387, -6.09474886]),
    array([-3.0785687 , -3.07861418]),
    array([43.08019804, 15.97647082]),
    array([45.09331255, 16.55197158])]
  #+END_EXAMPLE
  :END:

  Check what happens on all zero data.

  #+BEGIN_SRC ipython
    x = np.concatenate((np.zeros((1000, 1)), 1e5 * np.ones((1000, 1))), axis=1)
    onehot = np.ones((1000, 1))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[264]:
  :END:

  #+BEGIN_SRC ipython
    np.log((onehot * x[:,:1]).sum(axis=0) / onehot.sum(axis=0)) - np.log(np.ma.masked_values(onehot, 0) * x[:,1:]).mean(axis=0).compressed()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[265]:
  : array([-inf])
  :END:

  #+BEGIN_SRC ipython
    so.minimize(_nb, x0=(-np.inf, 0), args=(x[:,0], x[:,1], onehot))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[266]:
  #+BEGIN_EXAMPLE
    fun: 9.999999889225288e-09
    hess_inv: array([[1, 0],
    [0, 1]])
    jac: array([0.00000000e+00, 1.00000003e-08])
    message: 'Optimization terminated successfully.'
    nfev: 4
    nit: 0
    njev: 1
    status: 0
    success: True
    x: array([-inf,   0.])
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    _fit_gene(x, onehot)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[267]:
  : [array([-inf]), array([-0.]), array([0.]), array([0.]), array([0.])]
  :END:

  #+BEGIN_SRC ipython
    _fit_gene(x, onehot, design)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[163]:
  #+BEGIN_EXAMPLE
    [array([-7.75254245, -8.74529525]),
    array([-7.38635243, -6.45950176]),
    array([-3.07849171, -3.07866706]),
    array([42.9648789 , 15.92086032]),
    array([44.10874472, 16.3176927 ])]
  #+END_EXAMPLE
  :END:

  Check the end-to-end evaluation.

  #+BEGIN_SRC ipython
    evaluate(num_samples=100, num_mols=1e5, log_mu=-8, log_phi=-6, logodds=-3, fold=1.1, trial=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[32]:
  #+BEGIN_EXAMPLE
    {'fold': 1.1,
    'log_mu': -8,
    'log_mu_hat': -7.980952099176061,
    'log_phi': -6,
    'log_phi_hat': -5.681342153408702,
    'logodds': -3,
    'logodds_hat': -2.7515411255776483,
    'mean': 34.19137317124226,
    'mean_log_cpm': 5.8224834413816415,
    'num_mols': 100000.0,
    'num_samples': 100,
    'trial': 0,
    'var': 38.1766412350187,
    'var_log_cpm': 0.23075096605184894}
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :results output
    %timeit evaluate(num_samples=5000, num_mols=1e5, log_mu=-8, log_phi=-6, logodds=-3, fold=1.1, trial=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  1.41 s ± 449 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
  :END:

  Investigate what happens as the number of confounders increases.

  #+BEGIN_SRC ipython
    design = np.random.normal(size=(300, 20))
    x, _ = simulate(num_samples=300, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, design=design, fold=1.1)
    _fit_gene(x, design)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[322]:
  #+BEGIN_EXAMPLE
    [-8.00649125343069,
    -6.410804905890429,
    -2.99891159295469,
    33.32921072894424,
    35.15509338485383]
  #+END_EXAMPLE
  :END:

  Run the simulation on 28 CPUs.

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/sim.py
    <<zinb-imports>>
    import multiprocessing as mp
    import sqlite3
    <<np-zinb-impl2>>
    <<sim-impl>>
    <<np-eval-impl>>
    args = [(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial)
            for num_samples in np.linspace(100, 1000, 5).astype(int)
            for num_mols in 1e3 * np.linspace(100, 1000, 5).astype(int)
            for log_mu in np.linspace(-12, -6, 7)
            for log_phi in np.linspace(-6, 0, 7)
            for logodds in np.linspace(-3, 3, 7)
            for fold in np.linspace(1, 1.25, 6)
            for trial in range(10)]
    with mp.Pool() as pool:
      result = pd.DataFrame.from_dict(pool.starmap(evaluate, args))
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      result.to_sql(name='simulation', con=conn, index=False, if_exists='replace')
      conn.execute('create index ix_simulation on simulation(num_samples, num_mols);')
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation/
    sbatch --partition=broadwl --mem=8G --job-name sim -n1 -c28 --exclusive --out sim.out
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/sim.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 45559291

  Use this to check the parameter estimation for a particular gene/individual.

  #+BEGIN_SRC ipython
    def extract_data(ind, gene):
      with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
        umi = pd.read_sql("""select umi.value, annotation.size from umi, annotation 
        where annotation.chip_id == ? and gene == ? and 
        umi.sample == annotation.sample;""", con=conn, params=(ind, gene))
        return umi
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[25]:
  :END:

  Shard the data to parallelize over nodes. During this pass, write the data
  out to the database. 

  *Important notes:* 

  1. We need to use the actual number of molecules detected as the size factor,
     not the sum of QC'ed counts
  2. We need to estimate relative abundance for all genes (to match bulk), not
     just those which passed QC
  3. We need to recode categorical covariates as binary indicators. Although
     over the entire data set the number of indicators (in particular,
     ~experiment~) might be larger than the number of observations, for each
     subproblem it will not be.

  Write the annotations to the database.

  #+NAME: write-annotation-impl
  #+BEGIN_SRC ipython
    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    annotations['sample'] = annotations.apply(lambda x: '{chip_id}.{experiment:08d}.{well}'.format(**dict(x)), axis=1)
    annotations['size'] = annotations['mol_hs']
    annotations = annotations.loc[keep_samples.values.ravel(), ['sample', 'chip_id', 'size']]
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      annotations.to_sql(name='annotation', con=conn, if_exists='replace')
      conn.execute('create index ix_annotation on annotation(chip_id, sample);')
  #+END_SRC

  #+RESULTS: write-annotation-impl
  :RESULTS:
  # Out[51]:
  :END:

  Recode categorical variables as binary indicators.

  #+NAME: recode-impl
  #+BEGIN_SRC ipython
    def recode(annotations, key):
      n = annotations.shape[0]
      cat = sorted(set(annotations[key]))
      onehot = np.zeros((n, len(cat)))
      onehot[np.arange(n), annotations[key].apply(cat.index)] = 1
      return onehot
  #+END_SRC

  #+RESULTS: recode-impl
  :RESULTS:
  # Out[7]:
  :END:

  Check that we will have enough data points to actually estimate effects:

  #+BEGIN_SRC ipython
    annotations[keep_samples.values.ravel()].groupby('chip_id').apply(lambda x: pd.Series(recode(x, 'experiment').shape)).describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[316]:
  #+BEGIN_EXAMPLE
    0          1
    count   54.000000  54.000000
    mean    96.685185   5.500000
    std     40.056952   1.969101
    min     19.000000   2.000000
    25%     78.000000   4.000000
    50%     88.500000   5.000000
    75%    102.250000   6.750000
    max    281.000000  14.000000
  #+END_EXAMPLE
  :END:

  Show that ~batch~ is a linear combination of ~experiment~.

  #+BEGIN_SRC ipython
    design = np.concatenate([recode(annotations, k) for k in ('batch', 'experiment')], axis=1)
    u, d, v = np.linalg.svd(design)
    design.shape[1] - (d > 1e-2).sum()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[33]:
  : 6
  :END:


  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/shard.py
    <<zinb-imports>>
    <<write-annotation-impl>>
    i = 0
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      conn.execute('drop table if exists umi;')
      for chunk in pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0, chunksize=1000):
        print('Processing chunk {}'.format(i))
        chunk = (chunk
                 .loc[:,keep_samples.values.ravel()])
        if not chunk.empty:
          chunk = (chunk
                   .reset_index()
                   .melt(id_vars='gene', var_name='sample')
                   .merge(annotations, on='sample')
                   .sort_values(['gene', 'chip_id', 'sample']))
          chunk.to_csv('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(i), columns=['gene', 'chip_id', 'sample', 'value', 'size'], compression='gzip', sep='\t')
          chunk[['gene', 'sample', 'value']].to_sql(name='umi', con=conn, index=False, if_exists='append')
          i += 1
        del chunk
      conn.execute('create index ix_umi on umi(gene, sample);')
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation/
    sbatch --partition=broadwl --mem=8G --job-name shard --out shard.out
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/shard.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 45561618

  Process each chunk in parallel.

  #+NAME: process-chunk-impl
  #+BEGIN_SRC ipython
    def compute_breaks(chunk, by_ind=False):
      # Each subproblem has fixed size, so we can just split on integer indices
      # (instead of grouping)
      num_genes = len(set(chunk['gene']))
      num_samples = len(set(chunk['sample']))
      breaks = num_samples * np.arange(num_genes).reshape(-1, 1)
      if by_ind:
        num_samples_per_ind = chunk.iloc[:num_samples]['chip_id'].value_counts().sort_index().values
        # This can't be written += because of broadcasting
        breaks = breaks + np.cumsum(num_samples_per_ind).reshape(1, -1)
      else:
        # We need to get the right end point of each subproblem (exclusive)
        breaks += num_samples
      return breaks.ravel()
  #+END_SRC

  #+RESULTS: process-chunk-impl
  :RESULTS:
  # Out[40]:
  :END:

  #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-qtl/code/zinb.py
    <<zinb-imports>>
    import argparse
    import gzip
    import os
    import multiprocessing as mp
    import sqlite3
    <<np-zinb-impl2>>
    <<recode-impl>>
    <<process-chunk-impl>>

    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    annotations = annotations[keep_samples.values.ravel()]

    onehot = recode(annotations, 'chip_id')
    design = np.concatenate([recode(annotations, k) for k in ('batch', 'experiment')], axis=1)

    with mp.Pool() as pool:
      chunk = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')))
      breaks = compute_breaks(chunk)
      res = pool.starmap(
        _fit_gene,
        zip(np.split(chunk[['value', 'size']].values, breaks[:-1]),
            (onehot for b in breaks),
            (design for b in breaks)))

    with gzip.open('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')), 'wt') as f:
      for b, est in zip(breaks, res):
        gene = chunk.iloc[b - 1]['gene']
        for ind, record in zip(sorted(set(annotations['chip_id'])), zip(*est)):
          print(gene, ind, *record, sep='\t', file=f)
  #+END_SRC

  #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation
    sbatch --partition=broadwl --job-name="np-zinb" --mem=8G -a 0-20 -n1 -c28 --exclusive
    #!/bin/bash
    source activate scqtl
    python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/zinb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 45588176

  Populate the database.

  #+BEGIN_SRC ipython
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       conn.execute('drop table if exists params;')
       for i in range(21):
         for chunk in pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(i), sep=' ', header=None, chunksize=1000):
           chunk.columns = ['gene', 'ind', 'log_mu', 'log_phi', 'logodds', 'mean', 'var', 'log_mu_se', 'log_phi_se']
           chunk.to_sql(name='params', con=conn, index=False, if_exists='append')
       conn.execute('create index ix_params on params(gene, ind);')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Parameter distributions

  The simulation reveals the method has undesirable behavior when the
  proportion of zeros is too large and mean is too small.

  Read the estimated parameters.

  #+BEGIN_SRC ipython
    log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', comment='g', index_col=0, header=None, sep=' ')
    log_phi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', comment='g', index_col=0, header=None, sep=' ')
    logodds = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz', comment='g', sep=' ', header=None, index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[46]:
  :END:

  Look at the joint distribution.

  #+BEGIN_SRC ipython
    J = (log_mu.agg(np.mean, axis=1).to_frame()
         .merge(log_phi.agg(np.mean, axis=1).to_frame(), left_index=True, right_index=True)
         .rename(columns={'0_x': 'log_mu', '0_y': 'log_phi'})
         .merge(logodds.agg(np.mean, axis=1).to_frame(), left_index=True, right_index=True)
         .rename(columns={0: 'logodds'}))
    J.head()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[47]:
  #+BEGIN_EXAMPLE
    log_mu   log_phi   logodds
    0
    ENSG00000000003  -9.462848 -2.740221 -7.077608
    ENSG00000000419  -9.933007 -3.115943 -6.635574
    ENSG00000000457 -12.046886 -1.583684  0.290843
    ENSG00000000460 -11.072902 -2.532334 -3.202983
    ENSG00000001036 -11.083558 -2.455776 -2.901292
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/joint-distribution.png
    plt.clf()
    fig, ax = plt.subplots(2, 2)
    fig.set_size_inches(6, 6)

    ax[0, 0].scatter(J['log_mu'], J['log_phi'], c='k', s=2, alpha=0.25)
    ax[0, 0].set_xlabel('$\ln(\mu)$')
    ax[0, 0].set_ylabel('$\ln(\phi)$')

    ax[1, 0].scatter(J['log_mu'], J['logodds'], c='k', s=2, alpha=0.25)
    ax[1, 0].set_xlabel('$\ln(\mu)$')
    ax[1, 0].set_ylabel('$\mathrm{logit}(\pi)$')

    ax[0, 1].scatter(J['logodds'], J['log_phi'], c='k', s=2, alpha=0.25)
    ax[0, 1].set_xlabel('$\mathrm{logit}(\pi)$')
    ax[0, 1].set_ylabel('$\ln(\phi)$')

    ax[1, 1].axis('off')

    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[58]:
  [[file:figure/zinb.org/joint-distribution.png]]
  :END:

* Effect of confounding

  Estimate proportion of variance explained by confounders by estimating the
  average reduction in heterogeneity (residual variance).

  #+BEGIN_SRC ipython
    log_phi0 = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-phi.txt.gz', index_col=0, sep=' ')
    log_phi1 = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', index_col=0, sep=' ')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[128]:
  :END:

  #+BEGIN_SRC ipython
    1 - np.exp(log_phi1 - log_phi0).mean().mean()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[129]:
  : 0.1567919392443754
  :END:

  Estimate how much the mean changes due to confounding.

  #+BEGIN_SRC ipython
    log_mu0 = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-mu.txt.gz', index_col=0, sep=' ')
    log_mu1 = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[131]:
  :END:

  #+BEGIN_SRC ipython
    np.exp(log_mu1 - log_mu0).describe().loc['mean'].describe()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[132]:
  #+BEGIN_EXAMPLE
    count    54.000000
    mean      0.984503
    std       0.037119
    min       0.896817
    25%       0.963595
    50%       0.983739
    75%       1.005902
    max       1.065907
    Name: mean, dtype: float64
  #+END_EXAMPLE
  :END:

* Comparison with sample moment-based estimators
** Mean expression

  Compute pseudobulk relative abundance. *Important:* keep the \(\infty\) around.

  #+CALL: read-data-qc-impl()

  #+RESULTS:
  :RESULTS:
  # Out[13]:
  :END:

  #+BEGIN_SRC ipython
    pooled_log_mu = np.log(umi.groupby(annotations['chip_id'].values, axis=1).agg(np.sum)) - np.log(annotations.groupby('chip_id')['mol_hs'].agg(np.sum))
    pooled_log_rho = pooled_log_mu - sp.logsumexp(pooled_log_mu, axis=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[14]:
  :END:

  To first order,

  \[ E[\ln r] = \ln r \]

  #+BEGIN_SRC ipython
    # Follow edgeR
    libsize = annotations['mol_hs'].values
    eps = .5 * libsize / libsize.mean()
    log_cpm = (np.log(umi + eps) - np.log(libsize + 2 * eps) + 6 * np.log(10)) / np.log(2)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[15]:
  :END:

  CPM is proportional to relative abundance, so normalize.

  #+BEGIN_SRC ipython
    cpm_log_mu = log_cpm.groupby(annotations['chip_id'].values, axis=1).agg(np.mean)
    cpm_log_rho = cpm_log_mu - sp.logsumexp(cpm_log_mu, axis=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[16]:
  :END:

  #+BEGIN_SRC ipython
    nonzero_cpm_log_mu = log_cpm.mask(umi == 0).groupby(annotations['chip_id'].values, axis=1).agg(np.mean).dropna()
    nonzero_cpm_log_rho = nonzero_cpm_log_mu - sp.logsumexp(nonzero_cpm_log_mu, axis=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[17]:
  :END:

  #+BEGIN_SRC ipython
    zinb_log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design3/zi2-log-mu.txt.gz', sep=' ', index_col=0)
    zinb_logodds = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design3/zi2-logodds.txt.gz', sep=' ', index_col=0)
    # Important: log(sigmoid(x)) = -softplus(-x)
    zinb_log_mu -= np.log1p(np.exp(zinb_logodds))
    zinb_log_rho = zinb_log_mu - sp.logsumexp(zinb_log_mu, axis=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[18]:
  :END:

  Construct a DataFrame for convenience.

  #+BEGIN_SRC ipython
    log_rho = pd.DataFrame({'Pooled': pooled_log_rho['NA18507'],
                            'ZINB': zinb_log_rho['NA18507'],
                            'Nonzero CPM': nonzero_cpm_log_rho['NA18507'],
                            'CPM': cpm_log_rho['NA18507']})[['ZINB', 'Pooled', 'Nonzero CPM', 'CPM']]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[19]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/cpm-estimator-mu.png
    plt.clf()
    N = log_rho.shape[1]
    fig, ax = plt.subplots(N, N)
    fig.set_size_inches(8, 8)
    for y in range(N):
      ax[y, 0].set_ylabel('{}'.format(log_rho.columns[y]))
      for x in range(N):
        if y < x:
          ax[y, x].set_axis_off()
        else:
          ax[y, x].scatter(log_rho.iloc[:, x], log_rho.iloc[:, y], c='k', s=2, alpha=0.1)
          ax[y, x].plot(ax[y, x].get_xlim(), ax[y, x].get_xlim(), c='r', ls=':', lw=1)
          ax[y, x].text(.05, .95, '$r$ = {:.2g}'.format(st.mstats.spearmanr(log_rho.iloc[:, x], log_rho.iloc[:, y]).correlation), transform=ax[y, x].transAxes, verticalalignment='top')
    for x in range(N):
      ax[-1, x].set_xlabel('{}'.format(log_rho.columns[x]))
    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[20]:
  [[file:figure/zinb.org/cpm-estimator-mu.png]]
  :END:

** Expression noise

   Read the data.

   #+BEGIN_SRC ipython
     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     annotations = annotations.loc[keep_samples.values.ravel()]
     umi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0)
     zeros_pass = umi.loc[:,keep_samples.values.ravel()].agg(np.sum, axis=1) > 0
     umi = umi.loc[zeros_pass,keep_samples.values.ravel()]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[21]:
   :END:

   Look at NA18507 (individual with the most cell), and also over all individuals.

   #+BEGIN_SRC ipython
     keep_ind = (annotations['chip_id'] == 'NA18507').values.ravel()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[22]:
   :END:

   Look at Fano vs. mean, following [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3358231/][Munsky et al 2013]].

   #+BEGIN_SRC ipython
     mean = umi.agg(np.mean, axis=1)
     var = umi.agg(np.var, axis=1)
     ind_mean = umi.loc[:,keep_ind].agg(np.mean, axis=1)
     ind_var = umi.loc[:,keep_ind].agg(np.var, axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[23]:
   :END:

   #+NAME: plot-fano-vs-mean-def
   #+BEGIN_SRC ipython
     def plot_fano_vs_mean(mean, var, ax, title, method, ref=True):
       ax.semilogx()
       ax.semilogy()
       ax.scatter(mean, var / mean, c='k', s=2, zorder=0, alpha=0.25)
       lim = [.9 * mean.min(), 1.1 * mean.max()]
       grid = np.geomspace(lim[0], lim[1], 200)
       for phi in np.linspace(.2, 1, 5):
         if ref:
           ax.plot(grid, 1 + phi * np.array(grid), lw=1, c=colorcet.cm['inferno'](phi), zorder=1, label='{:.2g}'.format(phi))
       ax.set_xlim(lim)
       ax.set_ylim(1, 300)
       ax.set_xlabel('{} mean'.format(method))
       ax.set_ylabel('{} Fano factor'.format(method))
       ax.set_title(title)
   #+END_SRC

   #+RESULTS: plot-fano-vs-mean-def
   :RESULTS:
   # Out[24]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/zinb.org/count-fano-vs-count-mean.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(8, 4)
     plot_fano_vs_mean(ind_mean[ind_mean > 0], ind_var[ind_mean > 0], ax[0], 'NA18507', 'Sample')
     plot_fano_vs_mean(mean, var, ax[1], 'Over 53 individuals', 'Sample')
     ax[1].legend(title='Overdispersion', frameon=False, fancybox=False, loc='center left', bbox_to_anchor=(1, 0.5))
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[25]:
   [[file:figure/zinb.org/count-fano-vs-count-mean.png]]
   :END:

* ZINB estimates of expression noise

  Read the estimated parameters.

  #+BEGIN_SRC ipython
    log_mu = pd.read_table("/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design2/zi2-log-mu.txt.gz", index_col=0, sep=' ')
    log_phi = pd.read_table("/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design2/zi2-log-phi.txt.gz", index_col=0, sep=' ')
    logodds = pd.read_table("/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design2/zi2-logodds.txt.gz", index_col=0, sep=' ')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[26]:
  :END:

  Read the annotations.

  #+BEGIN_SRC ipython
    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    annotations = annotations.loc[keep_samples.values.ravel()]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[27]:
  :END:

  #+CALL: recode-impl()

  #+RESULTS:
  :RESULTS:
  # Out[28]:
  :END:

  #+BEGIN_SRC ipython
    onehot = recode(annotations, 'chip_id')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[29]:
  :END:

  Fixing individual \(i\), cell \(j\), gene \(k\), we have:

  \[ E[r_{ijk}] = (1 - \pi_{ik}) R_{ij} \mu_{ik} \]

  \[ V[r_{ijk}] = (1 - \pi_{ik})\left(R_{ij} \mu_{ik} + (R_{ij} \mu_{ik})^2
  \phi_{ik}\right) + \pi_{ik} (1 - \pi_{ik}) \mu_{ik}^2 \]

  #+NAME: zinb-moments
  #+BEGIN_SRC ipython
    mean_by_sample = (annotations['mol_hs'].values.reshape(-1, 1) * onehot).dot(np.exp(log_mu - np.log1p(np.exp(logodds))).T)
    # Nonzero component
    var_by_sample = mean_by_sample + np.square(mean_by_sample) * np.exp(onehot.dot(log_phi.values.T))
    var_by_sample *= onehot.dot(sp.expit(-logodds.T))
    var_by_sample += onehot.dot(sp.expit(logodds.T) * np.exp(log_mu.T)) * mean_by_sample
  #+END_SRC

  #+RESULTS: zinb-moments
  :RESULTS:
  # Out[30]:
  :END:

  The index of dispersion for observed data \(r_{ijk}\) at gene \(k\) is:

  \[ D_k = \frac{V[r_{ijk}]}{E[r_{ijk}]} \]

  where expectations (variances) are taken over individuals \(i\) and cells
  \(j\).

  Let \(g_{ijk}\) denote the zero-inflated negative binomial density as defined
  above. Then, we have:

  \[ r_{ijk} \sim \sum_{ijk} \frac{1}{N} g_{ijk}(\cdot) \]

  The mixture density has expectation:

  \[ \mu_k = \frac{1}{N} \sum E[r_{ijk}] \]

  and variance ([[http://www.springer.com/us/book/9780387329093][Frühwirth-Schnatter 2006]]):

  \[ \sigma^2_k = \frac{1}{N} \sum (E[r_{ijk}] - \mu_k)^2 + V[r_{ijk}] \]

  #+NAME: zinb-mix-moments
  #+BEGIN_SRC ipython
    mean_by_ind = onehot.T.dot(mean_by_sample) / onehot.T.sum(axis=1, keepdims=True)
    var_by_ind = onehot.T.dot(np.square(mean_by_sample - onehot.dot(mean_by_ind)) + var_by_sample) / onehot.T.sum(axis=1, keepdims=True)
    overall_mean = mean_by_sample.mean(axis=0)
    overall_var = (np.square(mean_by_sample - overall_mean.reshape(1, -1)) + var_by_sample).mean(axis=0)
  #+END_SRC

  #+RESULTS: zinb-mix-moments
  :RESULTS:
  # Out[31]:
  :END:

  #+RESULTS:
  :RESULTS:
  # Out[32]:
  :END:

  Find outliers.

  #+BEGIN_SRC ipython
    gene_info = (pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz')
                 .set_index('gene')
                 .query('source == "H. sapiens"')
                 .query('chr != "hsX"')
                 .query('chr != "hsY"')
                 .query('chr != "hsMT"'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[32]:
  :END:

  #+BEGIN_SRC ipython
    T = pd.DataFrame({'mean': overall_mean, 'var': overall_var})
    T.index = log_mu.index
    J = T.merge(gene_info, left_index=True, right_index=True).sort_values('mean', ascending=False)
    J[J['var'] / J['mean'] > 150][['mean', 'var', 'name']]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[33]:
  #+BEGIN_EXAMPLE
    mean          var       name
    gene
    ENSG00000110713  37.781379  6810.161703      NUP98
    ENSG00000163046   0.454326    71.162462  ANKRD30BL
    ENSG00000173214   0.397816    71.219849   KIAA1919
  #+END_EXAMPLE
  :END:

  Plot Fano factor vs. mean.

  #+CALL: plot-fano-vs-mean-def()

  #+RESULTS:
  :RESULTS:
  # Out[34]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/zinb-fano-vs-zinb-mean.png
    plt.clf()
    fig, ax = plt.subplots(1, 2)
    fig.set_size_inches(8, 4)
    k = sorted(set(annotations['chip_id'])).index('NA18507')
    plot_fano_vs_mean(mean_by_ind[k], var_by_ind[k], ax[0], 'NA18507', 'ZINB')
    plot_fano_vs_mean(overall_mean, overall_var, ax[1], 'Over 53 individuals', 'ZINB')
    ax[1].legend(title='Overdispersion', frameon=False, fancybox=False, loc='center left', bbox_to_anchor=(1, 0.5))

    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[35]:
  [[file:figure/zinb.org/zinb-fano-vs-zinb-mean.png]]
  :END:

  Compare estimates against each other.

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/count-fano-vs-zinb-fano.png
    S, T = pd.Series(overall_var / overall_mean, index=log_mu.index).align(var / mean, join='inner')
    plt.clf()
    plt.gcf().set_size_inches(4, 4)
    plt.semilogx()
    plt.semilogy()
    lim = [.9 * S.min(), 1.1 * S.max()]
    plt.scatter(S, T, c='k', s=2, alpha=0.25)
    plt.text(.05, .95, 'r = {:.2g}'.format(st.spearmanr(S, T).correlation), verticalalignment='top', transform=plt.gca().transAxes)
    plt.plot(lim, lim, c='r', ls=':', lw=1)
    plt.xlim(lim)
    plt.ylim(lim)
    plt.xlabel('ZINB Fano factor')
    plt.ylabel('Sample Fano factor')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[36]:
  : Text(0,0.5,'Sample Fano factor')
  [[file:figure/zinb.org/count-fano-vs-zinb-fano.png]]
  :END:
* Examples

  #+BEGIN_SRC ipython
    log_mu = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz', index_col=0, sep=' ')
    log_phi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz', index_col=0, sep=' ')
    logodds = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz', sep=' ', index_col=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[10]:
  :END:

  #+BEGIN_SRC ipython
    umi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0)
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
    umi = umi.loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
    annotations = annotations.loc[keep_samples.values.ravel()]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/zinb.org/example.svg
    %config InlineBackend.figure_formats = set(['svg'])
    plt.clf()
    fig, ax = plt.subplots(1, 2, sharey=True)
    fig.set_size_inches(4, 2)
    gene = "ENSG00000243709"
    for a, k in zip(ax, ('NA18507', 'NA19204')):
      x = umi.loc[gene, (annotations['chip_id'] == k).values]
      y = annotations[annotations['chip_id'] == k]
      grid = np.arange(x.max())
      a.hist(x, color='.75', bins=grid)
      n = np.exp(-log_phi.loc[gene, k])
      p = 1 / (1 + np.outer(y['mol_hs'], np.exp(log_mu.loc[gene, k] + log_phi.loc[gene, k])))
      G = st.nbinom(n=n.ravel(), p=p.ravel()).pmf
      pmf = np.array([G(x).mean() for x in grid])
      exp_count = x.shape[0] * pmf * sp.expit(-logodds.loc[gene, k])
      a.plot(0.5 + grid, exp_count, c='b', lw=1)
      a.arrow(0.5, 0, 0, x.shape[0] * sp.expit(logodds.loc[gene, k]), width=.01, head_width=.5, head_length=2, color='r')
    fig.text(0.5, 0, 'UMI counts', ha='center')
    fig.text(0, 0.5, 'Number of cells', va='center', rotation=90)
    fig.tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[59]:
  [[file:figure/zinb.org/example.svg]]
  :END:
