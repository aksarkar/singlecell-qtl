#+TITLE: Mean/dispersion estimation
#+SETUPFILE: setup.org

* Introduction

  We take a modular approach to call QTLs:

  1. Estimate a mean and a dispersion for each individual
  2. Treat the mean/dispersion as continuous phenotypes and perform QTL mapping

  Here, we solve (1).

  1. [[*numpy/scipy implementation][We implement CPU-based ML estimation]]
  2. [[*Genome-wide distribution of dispersion][We estimate per-gene indexes of dispersion]] accounting for the fact that
     data came from multiple individuals

* Model specification

  Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
  \(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell. As a
  first pass, define \(R_{ij} = \sum_k r_{ijk}\).

  Following Hilbe 2012, we derive the negative binomial as a Poisson-Gamma
  mixture:

  \[ r_{ijk} \sim \text{Pois}(R_{ij} \mu_{ik} u_{ijk}) \]

  \[ u_{ijk} \sim \text{Gamma}(\phi_{ik}^{-1}, \phi_{ik}^{-1}) \]

  Here, \(\mu_{ik}u_{ijk}\) denotes relative expression
  ([[https://arxiv.org/abs/1104.3889][Pachter 2011]]). Marginalizing out \(u\)
  yields the negative binomial distribution, with log likelihood:

  \[ \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}) = r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]

  We have multiple data points (30-200 cells) per mean/dispersion parameter, so
  simply minimizing the negative log likelihood should give reasonable
  estimates.

  We additionally account for zero-inflation, by letting \(\pi_{ik}\) denote
  the probability of observing an excess zero (not arising from the
  negative-binomial).

  Then, the log-likelihood of the data is:

  \[ \ln p(r_{ijk} \mid \cdot) = \ln(\pi_{ik} + (1 -  \pi_{ik}) p(r_{ijk}
    \mid R_{ij}, \mu_{ik}, \phi_{ik}))\ \text{if}\ r_{ijk} = 0 \]
  \[ \ln p(r_{ijk} \mid \cdot) = \ln(1 - \pi_{ik}) + \ln p(r_{ijk} \mid
    R_{ij}, \mu_{ik}, \phi_{ik})\ \text{otherwise} \]

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/project2/mstephens/aksarkar/projects/singlecell-qtl/analysis/dim-reduction.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="4G",venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 44724977

  #+NAME: zinb-imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.stats as st
    import scipy.special as sp
    import sqlite3
  #+END_SRC

  #+RESULTS: zinb-imports
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    import colorcet
    import matplotlib.pyplot as plt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+NAME: tf-imports
  #+BEGIN_SRC ipython :eval never
    import tensorflow as tf
  #+END_SRC

  #+NAME: list-local-devices
  #+BEGIN_SRC ipython :eval never
    from tensorflow.python.client import device_lib as dl
    dl.list_local_devices()
  #+END_SRC

  #+RESULTS: list-local-devices
  :RESULTS:
  #+BEGIN_EXAMPLE
  [name: "/cpu:0"
     device_type: "CPU"
     memory_limit: 268435456
     locality {
     }
     incarnation: 3101117888233869158, name: "/gpu:0"
     device_type: "GPU"
     memory_limit: 11324823962
     locality {
       bus_id: 1
     }
     incarnation: 4298943332142850272
     physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:08:00.0"]
  #+END_EXAMPLE
  :END:

* Tensorflow implementation                                        :noexport:

  We optimize all of the parameters together, using one-hot encoding to map
  parameters to data points. This makes inference more amenable to running on
  the GPU.

  Use ~tensorflow~ to automatically differentiate the negative log likelihood and
  perform gradient descent.

  #+NAME: zinb-impl
  #+BEGIN_SRC ipython :eval never
    def sigmoid(x):
      """Numerically safe sigmoid"""
      return tf.clip_by_value(tf.sigmoid(x), -13, 13)

    def log(x):
      """Numerically safe log"""
      return tf.log(x + 1e-8)

    def nb_llik(x, mean, inv_disp):
      """Log likelihood of x distributed as NB

      See Hilbe 2012, eq. 8.10

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)

      """
      return (x * log(mean / inv_disp) -
              x * log(1 + mean / inv_disp) -
              inv_disp * log(1 + mean / inv_disp) +
              tf.lgamma(x + inv_disp) -
              tf.lgamma(inv_disp) -
              tf.lgamma(x + 1))

    def zinb_llik(x, mean, inv_disp, logodds, eps=1e-8):
      """Log likelihood of x distributed as ZINB

      See Hilbe 2012, eq. 11.12, 11.13

      mean - mean (> 0)
      inv_disp - inverse dispersion (> 0)
      logodds - dropout log odds

      """
      case_zero = log(sigmoid(-logodds) + sigmoid(logodds) * tf.exp(nb_llik(x, mean, inv_disp)))
      case_non_zero = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
      return tf.where(tf.less(x, 1e-8), case_zero, case_non_zero)

    def fit(umi, onehot, size_factor, gene_dropout=False, ind_dropout=False, learning_rate=1e-2, max_epochs=1000):
      """Return estimated log mean and log dispersion. 

      If fitting a zero-inflated model, additionally return dropout log odds.

      umi - count matrix (n x p; float32)
      onehot - mapping of individuals to cells (m x n; float32)
      size_factor - size factor vector (n x 1; float32)
      gene_dropout - fit one dropout parameter per gene
      ind_dropout - fit one dropout parameter per individual
      init_log_mean - initial value for estimated log mean (m x p; float32)
      init_log_disp - initial value for estimated log dispersion (m x p; float32)

      If ind_dropout is True, gene_dropout must be True, otherwise raises
      ArgumentError.

      Returns:

      log_mean - log mean parameter (m x p)
      log_disp - log dispersion parameter (m x p)
      dropout - dropout log odds (1 x p if gene_dropout, n x p if ind_dropout)

      """
      n, p = umi.shape
      _, m = onehot.shape

      params = locals()
      graph = tf.Graph()
      with graph.as_default(), graph.device('/gpu:*'):
        size_factor = tf.Variable(size_factor, trainable=False)
        umi = tf.Variable(umi, trainable=False)
        onehot = tf.Variable(onehot, trainable=False)

        mean = tf.exp(tf.Variable(tf.zeros([m, p])))
        inv_disp = tf.exp(tf.Variable(tf.zeros([m, p])))

        if gene_dropout:
          if ind_dropout:
            dropout_params = tf.Variable(tf.zeros([m, p]))
            dropout = tf.matmul(onehot, dropout_params)
          else:
            dropout_params = tf.Variable(tf.zeros([1, p]))
            dropout = dropout_params
          llik = tf.reduce_mean(
            zinb_llik(umi, size_factor * tf.matmul(onehot, mean),
                      tf.matmul(onehot, inv_disp), dropout))
        elif ind_dropout:
          raise ValueError('Cannot specify individual-specific dropout without gene-specific dropout')
        else:
          llik = tf.reduce_mean(
            nb_llik(umi, size_factor * tf.matmul(onehot, mean),
                    tf.matmul(onehot, inv_disp)))

        train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-llik)
        opt = [tf.log(mean), -tf.log(inv_disp)]
        if gene_dropout:
          opt.append(dropout_params)
        curr = float('-inf')
        with tf.Session() as sess:
          sess.run(tf.global_variables_initializer())
          for i in range(max_epochs):
            _, update = sess.run([train, llik])
            if not np.isfinite(update):
              raise tf.train.NanLossDuringTrainingError
            if not i % 100:
              print(i, update)
          return sess.run(opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

** Read the data

  Read the QC'ed data.

  #+CALL: read-data-qc()

  #+RESULTS:
  :RESULTS:
  # Out[61]:
  :END:

  Onehot-encode the samples.

  #+NAME: onehot-qc
  #+BEGIN_SRC ipython
    individuals = sorted(annotations['chip_id'].unique())
    onehot = np.zeros((umi.shape[1], len(individuals)), dtype=np.float32)
    onehot[np.arange(onehot.shape[0]),annotations['chip_id'].apply(lambda x: individuals.index(x))] = 1
    onehot = pd.DataFrame(onehot, columns=individuals, index=umi.columns)
    onehot.shape
  #+END_SRC

  #+RESULTS: onehot-qc
  :RESULTS:
  # Out[5]:
  : (4995, 54)
  :END:

  Check that one-hot encoding is OK:

  #+BEGIN_SRC ipython :async t
    for _ in range(100):
      gene = np.random.choice(umi.index)
      ind = np.random.choice(individuals)
      idx = individuals.index(ind)
      assert (umi.loc[gene, (annotations['chip_id'] == ind).values] == 
              umi.loc[gene, onehot.dot(np.eye(onehot.shape[1])[idx]).astype(bool)]).all()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[5]:
  :END:

** Fit NB

  Estimate means and dispersions assuming no dropout.

  #+BEGIN_SRC ipython :eval never :tangle tensorflow-nb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/mean2.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=60 --job-name=tfnb --output=tfnb.out --error=tfnb.err
    #!/bin/bash
    source activate scqtl
    python tensorflow-nb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43884214

** Fit ZINB

  Estimate the parameters of the zero-inflated model assuming dropout per gene.

  #+BEGIN_SRC ipython :eval never :tangle zinb.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index).to_csv('/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=150 --job-name=zinb --output=zinb.out --error=zinb.err
    #!/bin/bash
    source activate scqtl
    python zinb.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43966390

** Fit ZINB2

  Estimate the parameters of the zero-inflated model assuming dropout per
  individual and gene.

  #+BEGIN_SRC ipython :eval never :tangle zinb2.py :noweb tangle
    <<zinb-imports>>
    <<tf-imports>>
    <<zinb-impl>>
    <<read-data-qc>>
    <<onehot-qc>>
    mean, dispersion, dropout = fit(
      umi=umi.values.T.astype(np.float32),
      onehot=onehot.values.astype(np.float32),
      size_factor=umi.agg(np.sum).astype(np.float32).values.reshape(-1, 1),
      gene_dropout=True,
      ind_dropout=True,
      learning_rate=1e-2,
      max_epochs=8000)
    pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-mean.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dispersion.txt.gz', sep=' ', compression='gzip')
    pd.DataFrame(dropout.T, index=umi.index, columns=onehot.columns).to_csv('/scratch/midway2/aksarkar/singlecell/zi2-dropout.txt.gz', sep=' ', compression='gzip')
  #+END_SRC

  #+BEGIN_SRC sh :eval never-export :exports none
    sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=120 --job-name=zinb2 --output=zinb2.out --error=zinb2.err
    #!/bin/bash
    source activate scqtl
    python zinb2.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 43910744

* numpy/scipy implementation

  Optimize the negative log-likelihood.

  #+NAME: np-zinb-impl
  #+BEGIN_SRC ipython
    import scipy.optimize as so

    def log(x):
      """Numerically safe log"""
      return np.log(x + 1e-8)

    def sigmoid(x):
      """Numerically safe sigmoid"""
      lim = np.log(np.finfo(np.float64).resolution)
      return np.clip(sp.expit(x), lim, -lim)

    def zinb(theta, x, size):
      theta, dropout = theta[:2], sigmoid(theta[2])
      case_zero = log(dropout + (1 - dropout) * np.exp(-nb(theta, x, size)))
      case_non_zero = log(1 - dropout) - nb(theta, x, size)
      return -np.where(x < 1e-8, case_zero, case_non_zero).mean()

    def nb(theta, x, size):
      mean, inv_disp = np.exp(theta)
      mean *= size
      assert mean.shape == x.shape
      return -(x * log(mean / inv_disp) -
               x * log(1 + mean / inv_disp) -
               inv_disp * log(1 + mean / inv_disp) +
               sp.gammaln(x + inv_disp) -
               sp.gammaln(inv_disp) -
               sp.gammaln(x + 1)).mean()
   #+END_SRC

   #+RESULTS: np-zinb-impl
   :RESULTS:
   # Out[1]:
   :END:

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Use this to check the parameter estimation for a particular gene/individual.

   #+BEGIN_SRC ipython
     def extract_data(ind, gene):
       with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
         umi = pd.read_sql("""select umi.value, annotation.size from umi, annotation 
         where annotation.chip_id == ? and gene == ? and 
         umi.sample == annotation.sample;""", con=conn, params=(ind, gene))
         return umi
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Shard the data to parallelize over nodes. During this pass, write the data
   out to the database. *Important: we need to use the actual number of
   molecules detected rather than the sum of QC'ed counts as the size factor.*

   #+BEGIN_SRC ipython
     annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
     annotations['sample'] = annotations.apply(lambda x: '{chip_id}.{experiment:08d}.{well}'.format(**dict(x)), axis=1)
     annotations['size'] = annotations['mol_hs']
     annotations = annotations[['sample', 'chip_id', 'size']]
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       annotations.to_sql(name='annotation', con=conn, if_exists='replace')
       conn.execute('create index ix_annotation on annotation(chip_id, sample);')

     keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
     keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
     keep_genes = keep_genes[keep_genes.values].index

     i = 0
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       conn.execute('drop table if exists umi;')
       for chunk in pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0, chunksize=1000):
         print('Processing chunk {}'.format(i))
         chunk = (chunk
                  .loc[:,keep_samples.values.ravel()]
                  .filter(items=keep_genes, axis='index'))
         if not chunk.empty:
           chunk = (chunk
                    .reset_index()
                    .melt(id_vars='gene', var_name='sample')
                    .merge(annotations, on='sample')
                    .sort_values(['gene', 'chip_id', 'sample']))
           chunk.to_csv('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(i), columns=['gene', 'chip_id', 'sample', 'value', 'size'], compression='gzip', sep='\t')
           chunk[['gene', 'sample', 'value']].to_sql(name='umi', con=conn, index=False, if_exists='append')
           i += 1
       conn.execute('create index ix_umi on umi(gene, sample);')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[35]:
   :END:

   Process each chunk in parallel.

   #+NAME: process-chunk-impl
   #+BEGIN_SRC ipython :eval never
     def fit_gene(chunk):
       res0 = so.minimize(nb, x0=[0, 0], args=(chunk[:,0], chunk[:,1]))
       pi0 = (chunk[:,0] == 0).sum() / chunk.shape[0]
       res = so.minimize(zinb, x0=list(res0.x) + [sp.logit(pi0 + 1e-8)], args=(chunk[:,0], chunk[:,1]))
       # Log likelihood of ZINB vs. NB. We need to flip signs again
       llr = chunk.shape[0] * (res0.fun - res.fun)
       if llr < 1:
         # This isn't a likelihood ratio test. Numerically, our implementation of
         # ZINB can't represent pi = 0, so we need to use a separate implementation
         # for it
         log_mu, neg_log_phi = res0.x
         logit_pi = -np.inf
       else:
         log_mu, neg_log_phi, logit_pi = res.x
       mean_by_sample = chunk[:,1].ravel() * np.exp(log_mu)
       var_by_sample = mean_by_sample + np.square(mean_by_sample) * np.exp(-neg_log_phi)
       mean_by_ind = mean_by_sample.mean()
       var_by_ind = (np.square(mean_by_sample - mean_by_ind) + var_by_sample).mean()
       return [log_mu, -neg_log_phi, logit_pi, mean_by_ind, var_by_ind]

     def compute_breaks(chunk, by_ind=False):
       # Each subproblem has fixed size, so we can just split on integer indices
       # (instead of grouping)
       num_genes = len(set(chunk['gene']))
       num_samples = len(set(chunk['sample']))
       breaks = num_samples * np.arange(num_genes).reshape(-1, 1)
       if by_ind:
         num_samples_per_ind = chunk.iloc[:num_samples]['chip_id'].value_counts().sort_index().values
         # This can't be written += because of broadcasting
         breaks = breaks + np.cumsum(num_samples_per_ind).reshape(1, -1)
       else:
         # We need to get the right end point of each subproblem (exclusive)
         breaks += num_samples
       return breaks.ravel()
   #+END_SRC

   #+BEGIN_SRC ipython :tangle /scratch/midway2/aksarkar/singlecell/np-zinb.py :noweb tangle
     <<zinb-imports>>
     import argparse
     import gzip
     import os
     import multiprocessing as mp
     import sqlite3
     <<np-zinb-impl>>
     <<process-chunk-impl>>

     with mp.Pool() as pool:
       chunk = pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')))
       breaks = compute_breaks(chunk, by_ind=True)
       res = pool.map(fit_gene, np.split(chunk[['value', 'size']].values, breaks[:-1]))

     with gzip.open('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(os.getenv('SLURM_ARRAY_TASK_ID')), 'wt') as f:
       for b in breaks:
         gene, ind = chunk.iloc[b - 1][['gene', 'chip_id']]
         print(gene, ind, *res.pop(0), file=f)
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/singlecell/density-estimation
     sbatch --partition=broadwl --job-name="np-zinb" --mem=4G -a 0-20 -n1 -c28 --exclusive
     #!/bin/bash
     source activate scqtl
     python /scratch/midway2/aksarkar/singlecell/np-zinb.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 44633431

   Populate the database.

   #+BEGIN_SRC ipython
     with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
       conn.execute('drop table if exists params;')
       for i in range(20):
         for chunk in pd.read_table('/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'.format(i), sep=' ', header=None, chunksize=1000):
           chunk.columns = ['gene', 'ind', 'log_mu', 'log_phi', 'logodds', 'mean', 'var']
           chunk.to_sql(name='params', con=conn, index=False, if_exists='append')
       conn.execute('create index ix_params on params(gene, ind);')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[56]:
   :END:

* Per-gene dispersion

  The index of dispersion for observed data \(r_{ijk}\) at gene \(k\) is:

  \[ D_k = \frac{V[r_{ijk}]}{E[r_{ijk}]} \]

  where expectations (variances) are taken over individuals \(i\) and cells
  \(j\).

  Let \(g_{ijk}\) denote the zero-inflated negative binomial density as defined
  above. Then, we have:

  \[ r_{ijk} \sim \sum_{ijk} \frac{1}{N} g_{ijk}(\cdot) \]

  Fixing gene \(k\), the mixture density has expectation:

  \[ \mu_k = \frac{1}{N} \sum E[r_{ijk}] \]

  and variance ([[http://www.springer.com/us/book/9780387329093][Frühwirth-Schnatter 2006]]):

  \[ \sigma^2_k = \frac{1}{N} \sum (E[r_{ijk}] - \mu)^2 + V[r_{ijk}] \]

  Fixing individual \(i\) and cell \(j\), we have:

  \[ E[r_{ijk}] = R_{ij} \mu_{ik} \]

  \[ V[r_[ijk]] = \left(R_{ij} \mu_{ik} + (R_{ij} \mu_{ik})^2 \phi_{ik}\right) \]

  Here, we ignore the factor of \((1 - \pi_{ik})\) under the assumption that
  excess zeros do not reflect biology.

  #+BEGIN_SRC ipython
    with sqlite3.connect('/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db') as conn:
      params = pd.read_sql('select * from params;', conn)
    gene_params = params.groupby('gene').apply(lambda x: pd.Series([x['mean'].mean(), (np.square(x['mean'] - x['mean'].mean()) + x['var']).mean()]))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:
