#+TITLE: NB estimation
#+SETUPFILE: setup.org

* Introduction

  Here we investigate how mean and dispersion estimates can explain away each
  other using single cell RNA-Seq data for the gene /ZSWIM7/ (the top eQTL gene
  from the iPSC bulk RNA-Seq study).

  For simplicity, we ignore genotype for now.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/qtl-mapping.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scqtl", partition="gpu2")

  #+RESULTS:
  : Submitted batch job 41382582

  #+BEGIN_SRC ipython
    %matplotlib inline

    import edward as ed
    import functools
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import pickle
    import tensorflow as tf
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Data

  We [[file:hurdle.org::*Test%20case][previously prepared the relevant data]]:

  #+BEGIN_SRC ipython
    with open('test_data.pkl', 'rb') as f:
      data = pickle.load(f)
    data.keys()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : dict_keys(['onehot', 'genotypes', 'counts', 'normalizers'])
  :END:

  Plot the distribution of the data:

  #+BEGIN_SRC ipython :ipyfile figure/nb.org/data.png
    n = data['counts'].shape[0]
    plt.clf()
    plt.gcf().set_size_inches(8, 6)
    plt.scatter(x=np.where(data['onehot'] == 1)[1] + np.random.normal(scale=0.1, size=n), y=data['counts'], s=2)
    plt.xlabel('Individual')
    plt.ylabel('UMI count')
    plt.gca().set_xticks([])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : []
  [[file:figure/nb.org/data.png]]
  :END:

* Per-individual mean, no overdispersion

  #+BEGIN_SRC ipython
    tf.reset_default_graph()
    ed.get_session().close()

    n, p = data['genotypes'].shape
    m, _ = data['onehot'].shape

    onehot = tf.placeholder(tf.float32, [m, n])
    mean_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    counts = ed.models.Poisson(rate=tf.exp(tf.matmul(onehot, mean_bias)))

    q_mean_bias = ed.models.NormalWithSoftplusScale(
      loc=tf.Variable(tf.random_normal([n, 1])),
      scale=tf.Variable(tf.random_normal([n, 1])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :results output
    inf = ed.ReparameterizationKLqp(
      latent_vars={mean_bias: q_mean_bias},
      data={onehot: data['onehot'], counts: data['counts']})
    inf.run(n_samples=10, optimizer=tf.train.AdamOptimizer(learning_rate=5e-3))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  1000/1000 [100%] ██████████████████████████████ Elapsed: 3s | Loss: 5215.281
  :END:

  Plot posterior 95% credible intervals for the latent mean of each individual.

  #+BEGIN_SRC ipython :ipyfile figure/nb.org/no-overdispersion.png
    res = pd.DataFrame(np.hstack(ed.get_session().run([q_mean_bias.mean(), q_mean_bias.variance()])))
    plt.clf()
    plt.errorbar(x=np.arange(n), y=res[0], yerr=1.96 * np.sqrt(res[1]), fmt='o')
    plt.xlabel('Individual')
    plt.ylabel('Latent mean')
    plt.gca().set_xticks([])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : []
  [[file:figure/nb.org/no-overdispersion.png]]
  :END:

* Per-individual mean, global overdispersion

  #+BEGIN_SRC ipython
    tf.reset_default_graph()
    ed.get_session().close()

    n, p = data['genotypes'].shape
    m, _ = data['onehot'].shape

    onehot = tf.placeholder(tf.float32, [m, n])
    mean_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    disp_bias = ed.models.Normal(loc=tf.zeros([1, 1]), scale=tf.ones([1, 1]))
    rate = ed.models.Gamma(
      concentration=tf.exp(tf.matmul(onehot, mean_bias)),
      rate=tf.exp(disp_bias))
    counts = ed.models.Poisson(rate=rate)

    q_mean_bias = ed.models.NormalWithSoftplusScale(
      loc=tf.Variable(tf.random_normal([n, 1])),
      scale=tf.Variable(tf.random_normal([n, 1])))
    q_disp_bias = ed.models.NormalWithSoftplusScale(
      loc=tf.Variable(tf.random_normal([1, 1])),
      scale=tf.Variable(tf.random_normal([1, 1])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :results output
    inf = ed.ReparameterizationKLqp(
      latent_vars={mean_bias: q_mean_bias, disp_bias: q_disp_bias},
      data={onehot: data['onehot'], counts: data['counts']})
    inf.run(n_samples=10, optimizer=tf.train.AdamOptimizer(learning_rate=5e-3))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  1000/1000 [100%] ██████████████████████████████ Elapsed: 10s | Loss: 15706.714
  :END:

  Plot the posterior 95% credible interval for the latent means.

  #+BEGIN_SRC ipython :ipyfile figure/nb.org/global-overdispersion.png
    res = pd.DataFrame(np.hstack(ed.get_session().run([q_mean_bias.mean(), q_mean_bias.variance()])))
    plt.clf()
    plt.errorbar(x=np.arange(n), y=res[0], yerr=1.96 * np.sqrt(res[1]), fmt='o')
    plt.xlabel('Individual')
    plt.ylabel('Latent mean')
    plt.gca().set_xticks([])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : []
  [[file:figure/nb.org/global-overdispersion.png]]
  :END:

  Recover the estimated posterior mean and variance of the global dispersion
  parameter.

  #+BEGIN_SRC ipython
    pd.DataFrame(np.hstack(ed.get_session().run([q_disp_bias.mean(), q_disp_bias.variance()])))  
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
            0         1
    0 -0.709709  0.002269
  #+END_EXAMPLE
  :END:

* Per-individual mean, per-individual overdispersion

  #+BEGIN_SRC ipython
    tf.reset_default_graph()
    ed.get_session().close()

    n, p = data['genotypes'].shape
    m, _ = data['onehot'].shape

    onehot = tf.placeholder(tf.float32, [m, n])
    genotypes = tf.placeholder(tf.float32, [n, p])
    normalizers = tf.placeholder(tf.float32, [m, 1])

    mean_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    disp_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    rate = ed.models.Gamma(
      concentration=tf.exp(tf.matmul(onehot, mean_bias)),
      rate=tf.exp(tf.matmul(onehot, disp_bias)))
    counts = ed.models.Poisson(rate=rate)

    q_mean_bias = ed.models.NormalWithSoftplusScale(
      loc=tf.Variable(tf.random_normal([n, 1])),
      scale=tf.Variable(tf.random_normal([n, 1])))
    q_disp_bias = ed.models.NormalWithSoftplusScale(
      loc=tf.Variable(tf.random_normal([n, 1])),
      scale=tf.Variable(tf.random_normal([n, 1])))

    inf = ed.ReparameterizationKLqp(
      latent_vars={mean_bias: q_mean_bias, disp_bias: q_disp_bias},
      data={globals()[k]: v for k, v in data.items()})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    inf.run(n_samples=10, optimizer=tf.train.AdamOptimizer(learning_rate=5e-3))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  Plot the posterior 95% credible intervals for the latent means.

  #+BEGIN_SRC ipython :ipyfile mean-per-individual-dispersion.png
    res = pd.DataFrame(np.hstack(ed.get_session().run([q_mean_bias.mean(), q_mean_bias.variance()])))
    plt.clf()
    plt.errorbar(x=np.arange(n), y=res[0], yerr=1.96 * np.sqrt(res[1]), fmt='o')
    plt.xlabel('Individual')
    plt.ylabel('Latent mean')
    plt.gca().set_xticks([])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : []
  [[file:mean-per-individual-dispersion.png]]
  :END:

  Plot the posterior 95% credible intervals for the latent dispersions.

  #+BEGIN_SRC ipython :ipyfile disp-per-individual-dispersion.png
    res = pd.DataFrame(np.hstack(ed.get_session().run([q_disp_bias.mean(), q_disp_bias.variance()])))
    plt.clf()
    plt.errorbar(x=np.arange(n), y=res[0], yerr=1.96 * np.sqrt(res[1]), fmt='o')
    plt.xlabel('Individual')
    plt.ylabel('Latent dispersion')
    plt.gca().set_xticks([])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : []
  [[file:disp-per-individual-dispersion.png]]
  :END:
