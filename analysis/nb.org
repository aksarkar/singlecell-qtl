#+TITLE: NB estimation
#+SETUPFILE: setup.org

* Setup

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/qtl-mapping.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scqtl", partition="gpu2")

  #+RESULTS:
  : Submitted batch job 41382582

  #+BEGIN_SRC ipython
    %matplotlib inline

    import edward as ed
    import functools
    import numpy as np
    import pandas as pd
    import pickle
    import tensorflow as tf
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Data

  #+BEGIN_SRC ipython
    with open('test_data.pkl', 'rb') as f:
      data = pickle.load(f)
    data.keys()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : dict_keys(['onehot', 'genotypes', 'counts', 'normalizers'])
  :END:

* Model specification and inference

  #+BEGIN_SRC ipython
    n, p = data['genotypes'].shape
    m, _ = data['onehot'].shape

    onehot = tf.placeholder(tf.float32, [m, n])
    genotypes = tf.placeholder(tf.float32, [n, p])
    normalizers = tf.placeholder(tf.float32, [m, 1])

    mean_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    disp_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    rate = ed.models.Gamma(
      concentration=tf.exp(tf.matmul(onehot, mean_bias)),
      rate=tf.exp(tf.matmul(onehot, disp_bias)))
    counts = ed.models.Poisson(rate=rate)

    q_mean_bias = ed.models.NormalWithSoftplusScale(
      loc=tf.Variable(tf.random_normal([n, 1])),
      scale=tf.Variable(tf.random_normal([n, 1])))
    q_disp_bias = ed.models.NormalWithSoftplusScale(
      loc=tf.Variable(tf.random_normal([n, 1])),
      scale=tf.Variable(tf.random_normal([n, 1])))

    inf = ed.ReparameterizationKLqp(
      latent_vars={mean_bias: q_mean_bias, disp_bias: q_disp_bias},
      data={globals()[k]: v for k, v in data.items()})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    inf.run(n_samples=10, optimizer=tf.train.AdamOptimizer(learning_rate=5e-3))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    pd.DataFrame(np.hstack(ed.get_session().run([q_mean_bias.mean(), q_mean_bias.variance()])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
             0         1
    0   0.007907  0.990953
    1  -0.003703  0.986969
    2   0.019831  0.967275
    3  -0.030037  0.994065
    4   0.042421  0.978162
    5   0.022240  0.972101
    6  -0.028268  1.088094
    7   0.003639  0.986326
    8   0.016368  0.953875
    9   0.022106  0.964523
    10  0.009743  0.995725
    11  0.010064  1.007149
    12 -0.016074  1.022777
    13 -0.004921  0.998337
    14  0.010286  0.998899
    15 -0.021722  1.008596
    16  0.001837  0.976684
    17 -0.000086  1.029787
    18  0.009203  0.918708
    19  0.003539  0.950592
    20  0.053963  1.018629
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    pd.DataFrame(np.hstack(ed.get_session().run([q_disp_bias.mean(), q_disp_bias.variance()])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
             0         1
    0  -0.640556  0.035325
    1  -0.312293  0.027051
    2  -0.921103  0.018066
    3  -0.615298  0.534105
    4  -0.302112  0.051284
    5  -0.306494  0.068147
    6  -0.842560  0.013169
    7  -0.538068  1.883408
    8  -0.544103  0.031011
    9  -0.627381  0.028651
    10  0.750082  2.306046
    11 -0.287370  0.100765
    12 -1.008165  0.050029
    13 -0.846644  0.013969
    14 -0.629209  0.225979
    15 -1.115731  0.022046
    16  0.161052  9.753973
    17 -0.492946  0.016056
    18 -0.417798  0.018156
    19 -0.896208  0.013385
    20 -0.417364  0.008798
  #+END_EXAMPLE
  :END:
