#+TITLE: QTL mapping
#+DATE: <2017-10-25 Wed>
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.1 (Org mode 9.1.2)
#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t

* Setup

  #+NAME: ipython3-kernel
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "singlecell") :var RESOURCES="--mem=8G --partition=gpu2"
    sbatch $RESOURCES --job-name=ipython3 --output=ipython3.out
    #!/bin/bash
    source activate scqtl
    rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
    ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS: ipython3-kernel
  : Submitted batch job 39746532

  #+NAME: imports
  #+BEGIN_SRC ipython
    %matplotlib inline

    import collections
    import glob
    import os
    import pandas as pd
    import rpy2.robjects
    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri as pandas2ri
    import tensorflow as tf

    r = rpy2.robjects.r
    biobase = rpy2.robjects.packages.importr('Biobase')
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  :END:

* Read the data

  #+BEGIN_SRC ipython
    ExpressionSet = collections.namedtuple('ExpressionSet', ['features', 'phenotypes', 'data'])

    def load_eset(filename):
      eset = r['readRDS'](filename)
      fdata = pandas2ri.ri2py(biobase.fData(eset))
      pdata = pandas2ri.ri2py(biobase.pData(eset))
      expr_matrix = pd.DataFrame(data=pandas2ri.ri2py(biobase.exprs(eset)), 
                                 index=fdata.index,
                                 columns=pdata.index)
      return ExpressionSet(fdata, pdata, expr_matrix)

    def load_project(dirname):
      files = glob.glob(os.path.join(dirname, 'data', 'eset', '*.rds'))
      return [load_eset(f) for f in files]

    esets = load_project(os.path.join(os.getenv('HOME'), 'projects', 'singlecell-qtl'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* ZINB model

  #+BEGIN_SRC ipython
    def biased_softplus(x, bias=1e-6):
      return bias + tf.nn.softplus(x)

    def sigmoid(x):
      """Sigmoid clipped to float32 resolution

      This is needed because sigmoid(x) = 0 leads to NaN downstream

      """
      min_ = np.log(np.finfo('float32').resolution)
      return tf.nn.sigmoid(tf.clip_by_value(x, min_, -min_))

    def poisson_gamma_llik(x, mean, dispersion):
      """Return log likelihood of x under Poisson-Gamma model

      \int_rate Poisson(x; rate) Gamma(rate; mean, dispersion) drate

      """
      return (mean * tf.log(dispersion) +
              tf.lgamma(x + mean) -
              tf.lgamma(mean) - 
              tf.lgamma(x + 1) -
              (x + mean) * tf.log(1 + dispersion))

    def zinb_llik(x, dropout, mean, dispersion):
      """Return log likelihood of x under ZINB model

      x ~ dropout \delta_0(x) + (1 - dropout) PoissonGamma(x; mean, dispersion)

      """
      # Every x contributes to the PG component
      zinb_comp = tf.log(1 - dropout) + poisson_gamma_llik(x, mean, dispersion)
      # tf handles log(0)
      delta_comp = tf.log(dropout * tf.cast(tf.equal(x, tf.constant(0.)), tf.float32))
      return tf.reduce_logsumexp(tf.stack([zinb_comp, delta_comp]), axis=0)

    def sample_gaussian(mean, prec, stoch_samples=1):
      return mean + tf.random_normal([stoch_samples, 1]) * tf.sqrt(tf.reciprocal(prec))

    def model0(counts, stoch_samples=10, learning_rate=1e-3):
      """Return approximate posterior mean of mean parameter and dispersion parameter

      y ~ \pi \delta_0 + (1 - \pi) PoissonGamma(\mu, \phi)

      """
      p, n = dge.shape
      with tf.Graph().as_default(), tf.Session() as sess, tf.variable_scope('q', initializer=tf.random_normal_initializer):
        counts = tf.placeholder(tf.float32)

        with tf.name_scope('dropout'):
          dropout = [tf.get_variable('mean', [p, n]),
                   biased_softplus(tf.get_variable('prec', [p, n]))]
        with tf.name_scope('mean'):
          mean = [tf.get_variable('mean', [p, n]),
                  biased_softplus(tf.get_variable('prec'), [p, np])]
        with tf.name_scope('dispersion'):
          dispersion = [tf.get_variable('mean_mean', [p, n]),
                        biased_softplus(tf.get_variable('dispersion_prec', [p, n]))]

        params = [dropout, mean, dispersion]
        kl_terms = [tf.reduce_sum(kl_normal_normal(mean_, prec_, tf.constant(0.), tf.constant(1.)))
                    for mean_, prec_ in params]

        samples = [sample_gaussian(*p) for p in params]
        llik = tf.reduce_mean(tf.reduce_sum(zinb_llik(y_ph, *samples), axis=0))

        elbo = llik - tf.add_n(kl_terms)
        train = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(-elbo)
        trace = [elbo, llik] + kl_terms
        opt = [p[0] for p in params]

        sess.run(tf.global_variables_initializer())
        for i in range(num_epochs):
          _, trace_output = sess.run([train, trace], feed_dict={x_ph: x, y_ph: y})
          if np.isnan(trace_output[0]):
            raise tf.train.NanLossDuringTrainingError
          if verbose and not i % 100:
            print(i, *trace_output)
        return sess.run(opt, feed_dict={counts: dge})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :results output
  model0()
  #+END_SRC

* Spike-and-slab version

  #+BEGIN_SRC ipython
def kl_normal_normal(mean_a, prec_a, mean_b, prec_b):
  """Rasmussen & Williams Eq. A23"""
  return tf.reduce_sum(.5 * (1 - tf.log(prec_b) + tf.log(prec_a) + prec_b * (T.sqr(mean_a - mean_b) + 1 / prec_a)), axis=-1)

def kl_bernoulli_bernoulli(p_a, p_b):
  """Rasmussen & Williams Eq. A22"""
  return tf.sum(p_a * tf.log(p_a / p_b) + (1 - p_a) * tf.log((1 - p_a) / (1 - p_b)))

def model(n, p, k):
  """Build the model

  Y_ik ~ ZINB(\sum_j X_ij \theta_j, \sum_j X_ij \phi_j, \pi_k)
  \theta_j ~ SSB(\pi_\theta, \tau_\theta^-1)
  \phi_j ~ SSB(\pi_\phi, \tau_\phi^-1)
  logit(\pi_k) ~ N(-\log(p), 1)

  Y - digital gene expression
  X - genotype (centered)
  \theta - mean effect
  \phi - inverse variance effect
  \pi - dropout
  \pi_{\theta,\phi} - sparsity
  \tau_{\theta,\phi} - effect size precision
  i - individual
  j - covariate (SNP/confounder)
  k - gene

  """
  x_ph = tf.placeholder(tf.float32)
  y_ph = tf.placeholder(tf.float32)

  with tf.variable_scope('q', initializer=tf.zeros_initializer):
    with tf.variable_scope('spikeslab'):
      q_logodds_mean = tf.get_variable('logodds_mean', initializer=tf.constant([-10.]))
      q_logodds_log_prec = tf.get_variable('logodds_log_prec', shape=[1])
      q_logodds_prec = 1e-6 + tf.nn.softplus(q_logodds_log_prec)
      # In [685]: np.log(np.finfo('float32').resolution)
      # Out[693]: -13.815511
      pi = tf.nn.sigmoid(tf.clip_by_value(q_logodds_mean, -13, 13))

      q_scale_mean = tf.get_variable('q_scale_mean', shape=[1])
      q_scale_log_prec = tf.get_variable('q_scale_log_prec', shape=[1])
      q_scale_prec = 1e-6 + tf.nn.softplus(q_scale_log_prec)
      tau = tf.nn.softplus(q_scale_mean)

      q_logit_z = tf.get_variable('q_logit_z', shape=[p, 1])
      q_z = tf.nn.sigmoid(tf.clip_by_value(q_logit_z, -13, 13))

      q_theta_mean = tf.get_variable('q_theta_mean', shape=[p, 1])
      q_theta_log_prec = tf.get_variable('q_theta_log_prec', shape=[p, 1])
      q_theta_prec = 1e-6 + tf.nn.softplus(q_theta_log_prec)

  theta_posterior_mean = q_z * q_theta_mean
  theta_posterior_var = q_z / q_theta_prec + q_z * (1 - q_z) * tf.square(q_theta_mean)
  eta_mean = tf.matmul(x_ph, theta_posterior_mean)
  eta_std = tf.sqrt(tf.matmul(tf.square(x_ph), theta_posterior_var))

  noise = tf.random_normal([50, 2])
  eta = eta_mean + noise[:,0] * eta_std
  phi = tf.nn.softplus(q_log_prec_mean + noise[:,1] * q_log_prec_std)

  llik = -.5 * tf.reduce_mean(tf.reduce_sum(-tf.log(phi) + tf.square(y_ph - eta) * phi, axis=0))
  kl_z = tf.reduce_sum(q_z * tf.log(q_z / pi) + (1 - q_z) * tf.log((1 - q_z) / (1 - pi)))
  kl_theta = tf.reduce_sum(q_z * .5 * (1 - tf.log(tau) + tf.log(q_theta_prec) + tau * (tf.square(q_theta_mean) + 1 / q_theta_prec)))
  kl_logodds = .5 * tf.reduce_sum(1 + tf.log(q_logodds_prec) + (tf.square(q_logodds_mean) + 1 / q_logodds_prec))
  kl_scale = .5 * tf.reduce_sum(1 + tf.log(q_scale_prec) + (tf.square(q_scale_mean) + 1 / q_scale_prec))
  kl_log_prec = .5 * tf.reduce_sum(1 + tf.log(q_log_prec_prec) + (tf.square(q_log_prec_mean) + 1 / q_log_prec_prec))
  elbo = llik - kl_z - kl_theta - kl_logodds - kl_scale - kl_log_prec

  optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2)
  train = optimizer.minimize(-elbo)

  # GLM coefficient of determination
  R = 1 - tf.reduce_sum(tf.square(y_ph - eta_mean)) / tf.reduce_sum(tf.square(y_ph - tf.reduce_mean(y_ph)))

  opt = [
    q_z,
    theta_posterior_mean,
    pi,
    tau,
  ]

  return train, elbo, opt

  #+END_SRC
