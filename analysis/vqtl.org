#+TITLE: ZINB estimation
#+DATE: <2017-10-25 Wed>
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.1 (Org mode 9.1.2)
#+SETUPFILE: /home/aksarkar/.local/src/org-html-themes/setup/theme-readtheorg.setup
#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t :eval never-export

* Setup

  This has to be run on the ~gpu2~ partition since it depends on ~libcuda.so~

  #+NAME: ipython3-kernel
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "singlecell") :var RESOURCES="--mem=8G --partition=gpu2"
    sbatch $RESOURCES --job-name=ipython3 --output=ipython3.out
    #!/bin/bash
    source activate scqtl
    rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
    ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS: ipython3-kernel
  : Submitted batch job 39833074

  #+NAME: imports
  #+BEGIN_SRC ipython
    %matplotlib inline

    import os
    import numpy as np
    import pandas as pd
    import tensorflow as tf
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  :END:

* Read the data

  #+BEGIN_SRC ipython
    def load_data(proj_dir):
      counts = pd.read_table(os.path.join(proj_dir), 'data', 'scqtl-counts.txt.gz')
      annotations = pd.read_table(os.path.join(proj_dir), 'data', 'scqtl-annotation.txt')

    counts, annotations = load_data(os.path.join(os.getenv('HOME'), 'projects', 'singlecell-qtl'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* ZINB model

  #+BEGIN_SRC ipython
    def biased_softplus(x, bias=1e-6):
      return bias + tf.nn.softplus(x)

    def logit(x):
      return tf.reciprocal(1 + tf.exp(-x))

    def sigmoid(x):
      """Sigmoid clipped to float32 resolution

      This is needed because sigmoid(x) = 0 leads to NaN downstream

      """
      min_ = np.log(np.finfo('float32').resolution)
      return tf.nn.sigmoid(tf.clip_by_value(x, min_, -min_))

    def nb_llik(x, mean, dispersion):
      """Return log likelihood of x under NB model

      c.f. Hilbe 2012, Eq. 8.10

      """
      theta = tf.reciprocal(dispersion)
      return (tf.lgamma(x + theta) -
              tf.lgamma(x + 1) -
              tf.lgamma(theta) +
              theta * (tf.log(theta) - tf.log(theta + mu)) +
              x * (tf.log(mu) - tf.log(theta + mu)))

    def zinb_llik(x, dropout, mean, dispersion):
      """Return log likelihood of x under ZINB model

      x ~ dropout \delta_0(x) + (1 - dropout) NB(x; mean, dispersion)

      c.f. Hilbe 2012, Eqs. 11.12, 11.13

      """
      zeros = tf.cast(tf.equal(x, 0.), tf.float32)
      case_zero = (tf.softplus(-logit(dropout) + nb_llik(x, mean, dispersion)) -
                   tf.softplus(-logit(dropout)))
      case_nonzero = (tf.log(dropout) + nb_llik(x, mean, dispersion))
      return tf.reduce_logsumexp(tf.stack([zinb_comp, delta_comp]), axis=0)

    def sample_gaussian(mean, prec, stoch_samples=1):
      """Return draws from N(mean, prec^-1) 

      Output shape is [stoch_samples] + mean.shape

      """
      return mean + tf.random_normal([stoch_samples, 1]) * tf.sqrt(tf.reciprocal(prec))

    def sgvb(llik, kl_terms, opt, learning_rate=1e-3):
      """Optimize ELBO using Stochastic Gradient Variational Bayes"""
      with tf.Session() as sess:
        elbo = llik - tf.add_n(kl_terms)
        train = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(-elbo)
        trace = [elbo, llik] + kl_terms

        sess.run(tf.global_variables_initializer())
        for i in range(num_epochs):
          _, trace_output = sess.run([train, trace], feed_dict={x_ph: x, y_ph: y})
          if np.isnan(trace_output[0]):
            raise tf.train.NanLossDuringTrainingError
          if verbose and not i % 100:
            print(i, *trace_output)
        return sess.run(opt, feed_dict={y: counts})


    def model0(counts, onehot, stoch_samples=10):
      """Return approximate posterior means of dropout, mean, dispersion

      counts ~ ZINB(dropout, mean, dispersion)

      p genes, n cells, m individuals

      counts (p x n) is digital gene expression
      onehot (n x m) is mapping from cells to individuals
      dropout (1 x n) is pooled across genes and individuals
      mean (p x m) is pooled across individuals
      dispersion (p x m) is pooled across individuals

      """
      p, n = dge.shape
      _, m = onehot.shape
      y = tf.placeholder(tf.float32)
      with tf.variable_scope('q', initializer=tf.random_normal_initializer):
        with tf.name_scope('dropout'):
          dropout = [tf.get_variable('mean', [1, n]),
                     biased_softplus(tf.get_variable('prec', [1, n]))]
        with tf.name_scope('mean'):
          mean = [tf.get_variable('mean', [p, m]),
                  biased_softplus(tf.get_variable('prec'), [p, m])]
        with tf.name_scope('dispersion'):
          dispersion = [tf.get_variable('mean', [p, m]),
                        biased_softplus(tf.get_variable('prec', [p, m]))]

      params = [dropout, mean, dispersion]
      opt = [p[0] for p in params]

      kl_terms = [tf.reduce_sum(kl_normal_normal(mean_, prec_, tf.constant(0.), tf.constant(1.)))
                  for mean_, prec_ in params]

      dropout = sigmoid(sample_gaussian(*dropout))
      mean = biased_softplus(sample_gaussian(*mean))
      dispersion = biased_softplus(sample_gaussian(*dispersion))
      llik = zinb_llik(y, dropout, mean * tf.transpose(onehot), dispersion * tf.transpose(onehot))
      llik = tf.reduce_mean(tf.reduce_sum(llik, axis=[1, 2]), axis=0)

      return sgvb(llik, kl_terms, opt, learning_rate=learning_rate)

    def fit_data(counts, annotations):
      with tf.Graph().as_default():
        model0(counts, annotations)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :results output
  model0()
  #+END_SRC

* Spike-and-slab version                                           :noexport:

  #+BEGIN_SRC ipython
def kl_normal_normal(mean_a, prec_a, mean_b, prec_b):
  """Rasmussen & Williams Eq. A23"""
  return tf.reduce_sum(.5 * (1 - tf.log(prec_b) + tf.log(prec_a) + prec_b * (T.sqr(mean_a - mean_b) + 1 / prec_a)), axis=-1)

def kl_bernoulli_bernoulli(p_a, p_b):
  """Rasmussen & Williams Eq. A22"""
  return tf.sum(p_a * tf.log(p_a / p_b) + (1 - p_a) * tf.log((1 - p_a) / (1 - p_b)))

def model(n, p, k):
  """Build the model

  Y_ik ~ ZINB(\sum_j X_ij \theta_j, \sum_j X_ij \phi_j, \pi_k)
  \theta_j ~ SSB(\pi_\theta, \tau_\theta^-1)
  \phi_j ~ SSB(\pi_\phi, \tau_\phi^-1)
  logit(\pi_k) ~ N(-\log(p), 1)

  Y - digital gene expression
  X - genotype (centered)
  \theta - mean effect
  \phi - inverse variance effect
  \pi - dropout
  \pi_{\theta,\phi} - sparsity
  \tau_{\theta,\phi} - effect size precision
  i - individual
  j - covariate (SNP/confounder)
  k - gene

  """
  x_ph = tf.placeholder(tf.float32)
  y_ph = tf.placeholder(tf.float32)

  with tf.variable_scope('q', initializer=tf.zeros_initializer):
    with tf.variable_scope('spikeslab'):
      q_logodds_mean = tf.get_variable('logodds_mean', initializer=tf.constant([-10.]))
      q_logodds_log_prec = tf.get_variable('logodds_log_prec', shape=[1])
      q_logodds_prec = 1e-6 + tf.nn.softplus(q_logodds_log_prec)
      # In [685]: np.log(np.finfo('float32').resolution)
      # Out[693]: -13.815511
      pi = tf.nn.sigmoid(tf.clip_by_value(q_logodds_mean, -13, 13))

      q_scale_mean = tf.get_variable('q_scale_mean', shape=[1])
      q_scale_log_prec = tf.get_variable('q_scale_log_prec', shape=[1])
      q_scale_prec = 1e-6 + tf.nn.softplus(q_scale_log_prec)
      tau = tf.nn.softplus(q_scale_mean)

      q_logit_z = tf.get_variable('q_logit_z', shape=[p, 1])
      q_z = tf.nn.sigmoid(tf.clip_by_value(q_logit_z, -13, 13))

      q_theta_mean = tf.get_variable('q_theta_mean', shape=[p, 1])
      q_theta_log_prec = tf.get_variable('q_theta_log_prec', shape=[p, 1])
      q_theta_prec = 1e-6 + tf.nn.softplus(q_theta_log_prec)

  theta_posterior_mean = q_z * q_theta_mean
  theta_posterior_var = q_z / q_theta_prec + q_z * (1 - q_z) * tf.square(q_theta_mean)
  eta_mean = tf.matmul(x_ph, theta_posterior_mean)
  eta_std = tf.sqrt(tf.matmul(tf.square(x_ph), theta_posterior_var))

  noise = tf.random_normal([50, 2])
  eta = eta_mean + noise[:,0] * eta_std
  phi = tf.nn.softplus(q_log_prec_mean + noise[:,1] * q_log_prec_std)

  llik = -.5 * tf.reduce_mean(tf.reduce_sum(-tf.log(phi) + tf.square(y_ph - eta) * phi, axis=0))
  kl_z = tf.reduce_sum(q_z * tf.log(q_z / pi) + (1 - q_z) * tf.log((1 - q_z) / (1 - pi)))
  kl_theta = tf.reduce_sum(q_z * .5 * (1 - tf.log(tau) + tf.log(q_theta_prec) + tau * (tf.square(q_theta_mean) + 1 / q_theta_prec)))
  kl_logodds = .5 * tf.reduce_sum(1 + tf.log(q_logodds_prec) + (tf.square(q_logodds_mean) + 1 / q_logodds_prec))
  kl_scale = .5 * tf.reduce_sum(1 + tf.log(q_scale_prec) + (tf.square(q_scale_mean) + 1 / q_scale_prec))
  kl_log_prec = .5 * tf.reduce_sum(1 + tf.log(q_log_prec_prec) + (tf.square(q_log_prec_mean) + 1 / q_log_prec_prec))
  elbo = llik - kl_z - kl_theta - kl_logodds - kl_scale - kl_log_prec

  optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2)
  train = optimizer.minimize(-elbo)

  # GLM coefficient of determination
  R = 1 - tf.reduce_sum(tf.square(y_ph - eta_mean)) / tf.reduce_sum(tf.square(y_ph - tf.reduce_mean(y_ph)))

  opt = [
    q_z,
    theta_posterior_mean,
    pi,
    tau,
  ]

  return train, elbo, opt

  #+END_SRC
