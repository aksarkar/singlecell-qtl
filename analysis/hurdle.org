#+TITLE: Hurdle model estimation
#+SETUPFILE: setup.org

* Introduction

  The key idea of ~voom~ ([[http://genomebiology.com/2014/15/2/R29][Law et al 2014]]) is that the distribution of \(Y =
  \log_2(\mathrm{CPM} + 1)\) is approximately Gaussian. For simplicity,
  consider one gene in one cell.

  \[ R = \mbox{number of reads} \]

  Assuming \(R\) follows the NB2 negative binomial model (Hilbe 2012):

  \[ E[R] = \lambda \]
  \[ V[R] = \lambda + \phi \lambda^2 \]

  By the definition of CPM:

  \[ Y = \log_2(R) + \mathrm{const} \]

  By first order Taylor expansion:

  \[ E[Y] = \mu \approx \log_2 \lambda + \mathrm{const} \]

  By the delta method:

  \[ V[Y] = \sigma^2 \approx V[R] / \lambda^2 = 1/\lambda + \phi \]

  The key idea of ~mast~ ([[https://dx.doi.org/10.1186/s13059-015-0844-5][Finak et al 2015]]) is to model non-zero \(R\) using a
  Gaussian distribution, and model zero \(R\) using logistic regression. The
  key distinction between this approach and the [[file:zinb.org][zero-inflated
  negative binomial model]] is that zeroes are assumed to arise from only one
  process.

  This naturally suggests a Bayesian model which simultaneously calls mean and
  variance QTLs based on the likelihood:

  \[ Y \mid Y = 0 \sim \mathrm{Bernoulli}(1 - \mathrm{sigmoid}(AX\theta_m + \delta)) \]

  \[ Y \mid Y > 0 \sim N(AX\theta_m + \delta, \exp(-AX\theta_m - \delta) +
  \exp(AX\theta_v) + \sigma^2) \]

  where \(\delta\) is a known constant that depends on library size, \(A\) maps
  from cells to individuals, and \(X\) is the genotype matrix.

  The key issues are:

  1. *Is it possible to develop an individual-level model?* An alternative
     model would be to estimate a variance for each individual, and then plug
     into existing QTL mapping software. This approach is naturally suggested
     by the ~voom~ approximation, but it is not obvious that the approach will
     generalize to zero-inflated data.

     Fitting the proposed model will require writing new software as opposed to
     plugging into existing software. We previously built an efficient
     algorithm and inference engine to build these kind of models ([[https://www.biorxiv.org/content/early/2017/02/10/107623][Park et al
     2017]]), but it has one major issue: it is computationally expensive to
     perform statistical tests on the fitted models because the approximate
     Bayesian inferences underestimate the uncertainty in the estimated values.

  2. *Do we need to actually fit the dropout model?* ~mast~ conditions on the
     observed \(Y = 0\), not on a latent \(Z = 0\), which means we could simply
     do the same and ignore zeros.

     If we did so, then mean/variance QTL effect size estimation will be less
     robust for genes with high dropout. This might not be a problem depending
     on the stringency of gene filtering.

  3. *For a single gene, do we need to worry about mean QTLs in LD with
     variance QTLs?* We previously [[https://github.com/YPARK/fqtl][built multivariate mean/variance QTL models]]
     which could account for LD, and could share information between the mean
     and variance models.

     The fundamental problem is that if we assume that the mean and dispersion
     both have genetic components, then the mean is no longer independent of
     the dispersion.

     This actually could be derived without using the fact that both depend on
     the same genotypes if we use second-order Taylor expansion:

     \[ \mu \approx \log_2 \lambda + \frac{V[R]}{2 \lambda^2} \]

  4. *Do we need to share parameters between genes?* ~mast~ assumes genes are
     conditionally independent. But this is no longer true when nearby genes
     can be driven by overlapping (or correlated) /cis/-genotypes.

     We previously developed multiresponse QTL models which learned the target
     genes of causal variants, allowing the true target gene to explain away
     nearby correlated genes ([[https://www.biorxiv.org/content/early/2017/11/14/219428][Park et al 2017]]).

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/sc-vs-bulk.org")
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/qtl-mapping.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 41429070

  #+BEGIN_SRC ipython
    import edward as ed
    import functools
    import gzip
    import os.path
    import numpy as np
    import pandas as pd
    import pickle
    import tabix
    import tensorflow as tf
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+CALL: concordance-def()

  #+RESULTS:
  :RESULTS:
  :END:

  #+CALL: load-data-defs()

  #+RESULTS:
  :RESULTS:
  :END:

  #+CALL: load-data()

  #+RESULTS:
  :RESULTS:
  : ((20327, 2261), (34608, 15))
  :END:

  #+CALL: umi-qc()

  #+RESULTS:
  :RESULTS:
  : (15636, 1810)
  :END:

  #+CALL: onehot-qc()

  #+RESULTS:
  :RESULTS:
  :END:

* Test case

  #+CALL: get-gene-info() :exports both

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
                    chr      start        end      name strand      source
    gene                                                                    
    ENSG00000000419  hs20   49551404   49575092      DPM1      -  H. sapiens
    ENSG00000000457   hs1  169818772  169863408     SCYL3      -  H. sapiens
    ENSG00000000460   hs1  169631245  169823221  C1orf112      +  H. sapiens
    ENSG00000000938   hs1   27938575   27961788       FGR      -  H. sapiens
    ENSG00000000971   hs1  196621008  196716634       CFH      +  H. sapiens
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    gene_info.query('name == "ZSWIM7"')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
                    chr     start       end    name strand      source
    gene                                                                
    ENSG00000214941  hs17  15879874  15903031  ZSWIM7      -  H. sapiens
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    with gzip.open('/project2/gilad/singlecell-qtl/bulk/genotypes.vcf.gz', 'rt') as f:
      header = next(f).split()
    vcf = tabix.open('/project2/gilad/singlecell-qtl/bulk/genotypes.vcf.gz')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    def _extract_genotypes(row, vcf, header, window):
      if row['strand'] == '+':
        start = row['start'] - window
        end = row['start']
      else:
        start = row['end']
        end = row['end'] + window
      result = pd.DataFrame(list(vcf.query('chr{}'.format(row['chr'][2:]), start, end)), columns=header).iloc[:,9:].astype(np.float32)
      return result
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    genotypes = _extract_genotypes(gene_info.loc['ENSG00000214941'], vcf=vcf, header=header, window=int(1e6)).rename(columns=lambda x: 'NA{}'.format(x))[sorted(annotations_qc['chip_id'].unique())]
    genotypes = genotypes.T.transform(lambda x: x - x.mean()).fillna(0)
    genotypes.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : (21, 2506)
  :END:

  #+BEGIN_SRC ipython
    data = {'onehot': onehot,
            'genotypes': genotypes.values,
            'counts': umi_qc.loc['ENSG00000214941'].values.astype(np.float32).reshape(-1, 1),
            'normalizers': -np.log(umi_qc.agg(np.sum)).values.reshape(-1, 1)}
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    with open('test_data.pkl', 'wb') as f:
      pickle.dump(data, f)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Voom component

  #+BEGIN_SRC ipython
    with open('test_data.pkl', 'rb') as f:
      data = pickle.load(f)

    data['log_cpm'] = np.log(data['counts'] + 1) + data['normalizers'] + 6 * np.log(10)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    tf.reset_default_graph()
    ed.get_session().close()

    m, n = data['onehot'].shape

    onehot = tf.placeholder(tf.float32, [m, n])
    eta0 = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    eta1 = ed.models.Normal(loc=tf.zeros([n, 1]), scale=tf.ones([n, 1]))
    log_cpm = ed.models.Normal(
      loc=tf.matmul(onehot, tf.exp(eta0)),
      scale=tf.sqrt(tf.matmul(onehot, tf.exp(-eta0) + tf.exp(eta1))))

    q_eta0 = ed.models.Normal(loc=tf.Variable(tf.zeros([n, 1])), scale=tf.Variable(tf.ones([n, 1])))
    q_eta1 = ed.models.Normal(loc=tf.Variable(tf.zeros([n, 1])), scale=tf.Variable(tf.ones([n, 1])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython 
    inf = ed.ReparameterizationKLqp(
      latent_vars={eta0: q_eta0, eta1: q_eta1},
      data={onehot: data['onehot'], log_cpm: data['log_cpm']})
    inf.run(n_samples=10, optimizer=tf.train.AdamOptimizer(learning_rate=5e-3))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : 1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 4948.953
  :END:

  #+BEGIN_SRC ipython
    pd.DataFrame(
      np.hstack(ed.get_session().run([q_eta0.mean(), q_eta0.variance(), q_eta1.mean(), q_eta1.variance()])),
      columns=['eta0_mean', 'eta0_var', 'eta1_mean', 'eta1_var'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
      eta0_mean  eta0_var  eta1_mean  eta1_var
    0    0.134668  0.587063   1.774024  0.155157
    1    0.865564  0.288437   1.891148  0.037003
    2    0.336732  0.608907   2.205442  0.021856
    3    0.023808  0.637865   1.475013  0.048397
    4    0.560011  0.409512   1.824579  0.040784
    5   -0.088260  0.612847   1.317034  0.277709
    6    0.418118  0.561308   2.237652  0.017574
    7    0.277662  0.595737   2.064718  0.025858
    8   -0.126591  0.609919   1.014545  0.251775
    9    0.602666  0.441734   2.065972  0.032605
    10   0.077610  0.677382   1.875362  0.048096
    11  -0.022883  0.659982   1.401854  0.181760
    12   0.141997  0.699906   2.073957  0.070775
    13  -0.054175  0.711434   1.291426  0.325011
    14   0.050481  0.705741   1.764009  0.138111
    15   0.369330  0.574703   2.174503  0.022635
    16   0.700163  0.458168   2.124360  0.071042
    17   0.594862  0.436958   1.999896  0.026416
    18   1.027588  0.208037   1.733976  0.119377
    19   0.095099  0.707778   1.838709  0.032868
    20  -0.008542  0.718278   1.828956  0.107599
  #+END_EXAMPLE
  :END:
