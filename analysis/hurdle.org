#+TITLE: Hurdle model estimation
#+SETUPFILE: setup.org

* Introduction

  The key idea of ~voom~ ([[http://genomebiology.com/2014/15/2/R29][Law et al 2014]]) is that the distribution of
  log-transformed counts per million is approximately Gaussian. Consider the
  expression of a single gene in individual \(i\), cell \(j\).

  \[ r_{ij} = \mbox{number of reads} \]

  Assuming \(r_{ij}\) follows the NB2 negative binomial model (Hilbe 2012):

  \[ E[r_{ij}] = \lambda_{ij} \]
  \[ V[r_{ij}] = \lambda_{ij} + \phi_{ij} \lambda_{ij}^2 \]

  Let \(L_j\) be the library size of cell \(j\). Then, by the definition of CPM:

  \[ y_{ij} = \log_2(10^6 \times \frac{r_{ij}}{L_j} + 1) \]

  \[ = \log_2\left(10^6 \times \left(r_{ij} + \frac{L_j}{10^6}\right)\right) - \log_2(L_j) \]

  Assuming that \(L_j \ll 10^6\):

  \[ y_{ij} \approx \log_2(r_{ij}) - \log_2(L_j) + 6 \log_2(10) \]

  By first order Taylor expansion:

  \[ E[y_{ij}] = \mu_{ij} \approx \log_2 \lambda_{ij} + \mathrm{const} \]

  By the delta method:

  \[ V[y_{ij}] = \sigma_{ij}^2 \approx V[r_{ij}] / \lambda_{ij}^2 = 1/\lambda_{ij} + \phi_{ij} \]

  We assume:

  \[ y_{ij} \approx N(\mu_{ij}, \sigma^2_{ij}) \]

  Now, we seek to write a hierarchical model for \(y\) in terms of the genotype
  of individual \(i\) to call mean and variance QTLs.

  Our idea is to start from generalized linear models for the underlying rate
  and dispersion:

  \[ \log \lambda_{ij} = X_i \beta_\lambda + u_i^\lambda + \epsilon^\lambda_{ij} \]

  \[ \log \phi_{ij} = X_i \beta_\phi + u_i^\phi + \epsilon^\phi_{ij} \]

  We assume errors are uncorrelated with \(X, \beta, u\) so we can write
  \(\sigma^2 = V[\epsilon_{ij}^\lambda] + V[\epsilon_{ij}^\phi]\)

  Then, the likelihood of each data point is given by:

  \[ y_{ij} \sim N(X_i \beta_\lambda + u_i^\lambda, \exp(-(X_i \beta_\lambda +
  u_i^\lambda)) + \exp(X_i \beta_\phi + u_i^\phi) + \sigma^2) \]

  In this model, between individual variance is explained by /cis/-genotype
  (\(X_i \beta_\lambda)\), unobserved factors (\(u_i^\lambda)\), and sampling
  (\(\epsilon_{ij}^\lambda\)).

  Within individual variance is explained by the rate of expression (as derived
  above), /cis/-genotype (\(X_i \beta_\phi\)), unobserved factors \(u_i^\phi\),
  and sampling (\(\epsilon_{ij}^\phi\)).

  Unobserved factors are needed to account for the fact that two individuals
  with the same genotype could still have different underlying rate and
  dispersion of expression.

  We can estimate the posterior \(p(\beta, u \mid Y, \cdot)\) this model using
  a combination of black-box variational inference and variational EM.

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/sc-vs-bulk.org")
    (org-babel-lob-ingest "/home/aksarkar/projects/singlecell-qtl/analysis/qtl-mapping.org")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 41590915

  #+BEGIN_SRC ipython
    import edward as ed
    import functools
    import gzip
    import os.path
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import pickle
    import tabix
    import tensorflow as tf
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+CALL: concordance-def()

  #+RESULTS:
  :RESULTS:
  :END:

  #+CALL: load-data-defs()

  #+RESULTS:
  :RESULTS:
  :END:

  #+CALL: load-data()

  #+RESULTS:
  :RESULTS:
  : ((20327, 2261), (34608, 15))
  :END:

  #+CALL: umi-qc()

  #+RESULTS:
  :RESULTS:
  : (15636, 1810)
  :END:

  #+CALL: onehot-qc()

  #+RESULTS:
  :RESULTS:
  :END:

  #+NAME: reset
  #+BEGIN_SRC ipython
    tf.reset_default_graph()
    ed.get_session().close()
  #+END_SRC

  #+RESULTS: reset
  :RESULTS:
  :END:

* Test case

  Look at the top eQTL SNP-gene pair from the bulk RNA-sequencing data,
  ~rs73276049~ and /ZSWIM7/.

  #+BEGIN_EXAMPLE
                    symbol        rsid  effect_size     beta_perm
    ENSG00000214941   ZSWIM7  rs73276049     1.792670  1.567000e-17
    ENSG00000145725  PPIP5K2     rs34822     1.155860  1.280260e-15
    ENSG00000164978    NUDT2   rs7848476     0.998739  6.685150e-15
    ENSG00000243317  C7orf73   rs6467603     1.371900  1.508780e-14
    ENSG00000240344    PPIL3  rs13412214    -1.343370  1.566880e-14
  #+END_EXAMPLE

  Load the genotype data for ~rs73276049~.

  #+NAME: extract-header
  #+BEGIN_SRC sh :eval never-export
    zcat /project2/gilad/singlecell-qtl/bulk/genotypes.vcf.gz | head -n1 | awk '{for (i=1; i <= NF; i++) {if (i > 9) {$i = "NA" $i}} print $0}'
  #+END_SRC

  #+NAME: extract-geno
  #+BEGIN_SRC sh :eval never-export
    zgrep -wm1 "rs73276049" /project2/gilad/singlecell-qtl/bulk/genotypes.vcf.gz
  #+END_SRC

  #+BEGIN_SRC ipython :noweb no-export
    genotypes = pd.Series("""<<extract-geno()>>"""[2:-2].split())
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :noweb no-export :exports code
    genotypes.index = """<<extract-header()>>""".split()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  Compute library sizes and CPM.

  #+BEGIN_SRC ipython
    normalizers = (6 * np.log(10) - np.log(umi_qc.agg(np.sum))) / np.log(2)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython
    data = {'onehot': onehot,
            'genotypes': genotypes.T[sorted(annotations_qc['chip_id'].unique())].values.reshape(-1, 1),
            'counts': umi_qc.loc['ENSG00000214941'].values.astype(np.float32).reshape(-1, 1),
            'log_cpm': cpm(umi_qc, log2=True).loc['ENSG00000214941'].values.astype(np.float32).reshape(-1, 1),
            'normalizers': normalizers.values.reshape(-1, 1)}
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  Write out the processed data to use as a test case.

  #+BEGIN_SRC ipython
    with open('test_data.pkl', 'wb') as f:
      pickle.dump(data, f)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  Plot the data, mean, and two standard deviations per individual (restricted
  to non-zero CPM values):

  #+BEGIN_SRC ipython
    onehot_cpm = np.ma.masked_equal(data['log_cpm'] * data['onehot'], 0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/hurdle.org/data.png
    plt.clf()
    plt.gcf().set_size_inches(8, 6)
    plt.scatter(x=np.where(data['onehot'] == 1)[1] + np.random.normal(scale=0.1, size=data['onehot'].shape[0]), y=data['log_cpm'], s=2, alpha=0.5)
    plt.errorbar(x=np.arange(data['onehot'].shape[1]), y=onehot_cpm.mean(axis=0), yerr=2 * onehot_cpm.std(axis=0), fmt='o', c='red')
    plt.xlabel('Individual')
    plt.ylabel('$\log_2(CPM + 1)$')
    plt.gca().set_xticks([])
    plt.gcf()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:figure/hurdle.org/data.png]]
  :END:

  Use this block to load the data without loading/processing the entire counts
  matrix (requires much less memory).

  #+BEGIN_SRC ipython
    with open('test_data.pkl', 'rb') as f:
      data = pickle.load(f)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Model specification and inference

  #+CALL: reset()

  #+RESULTS:
  :RESULTS:
  :END:

  We specify the model in Edward. We assume a fully factored variational
  approximation \(q(\beta_\lambda)q(\beta_\phi)q(u^\lambda)q(u^\phi)\).

  #+BEGIN_SRC ipython
    nonzero_cpm = data['log_cpm'].ravel() > 0
    q = nonzero_cpm.sum()
    m, n = data['onehot'].shape
    _, p = data['genotypes'].shape

    onehot = tf.placeholder(tf.float32, [q, n])
    genotypes = tf.placeholder(tf.float32, [n, 1])
    cell_bias = tf.placeholder(tf.float32, [q, 1])

    resid_var_scale = tf.exp(tf.Variable(tf.zeros([1])))

    ind_bias_scale = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
    ind_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=ind_bias_scale)

    rate_effect_scale = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
    rate_effect = ed.models.Normal(loc=tf.zeros([1, 1]), scale=rate_effect_scale)

    log_rate = tf.matmul(onehot, tf.matmul(genotypes, rate_effect) + ind_bias)
    mean = log_rate + cell_bias

    disp_bias_scale = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
    disp_bias = ed.models.Normal(loc=tf.zeros([n, 1]), scale=disp_bias_scale)

    disp_effect_scale = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
    disp_effect = ed.models.Normal(loc=tf.zeros([1, 1]), scale=disp_effect_scale)

    disp = tf.matmul(onehot, tf.matmul(genotypes, disp_effect) + disp_bias)
    var = tf.exp(-log_rate) + tf.exp(disp) + resid_var_scale

    log_cpm = ed.models.Normal(loc=mean, scale=tf.sqrt(var))

    q_ind_bias = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([n, 1])), scale=tf.Variable(tf.ones([n, 1])))
    q_disp_bias = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([n, 1])), scale=tf.Variable(tf.ones([n, 1])))
    q_rate_effect = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([1, 1])), scale=tf.Variable(tf.random_normal([1, 1])))
    q_disp_effect = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([1, 1])), scale=tf.Variable(tf.random_normal([1, 1])))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  We optimize the evidence lower bound with respect to the variational parameters
  and model hyperparameters (scales) simultaneously using the
  reparameterization gradient and gradient descent.

  #+BEGIN_SRC ipython
    inf = ed.ReparameterizationKLKLqp(
      latent_vars={
        rate_effect: q_rate_effect,
        ind_bias: q_ind_bias,
        disp_effect: q_disp_effect,
        disp_bias: q_disp_bias,
      },
      data={
        onehot: data['onehot'][nonzero_cpm],
        genotypes: data['genotypes'],
        cell_bias: data['normalizers'][nonzero_cpm],
        log_cpm: data['log_cpm'][nonzero_cpm],
      })
    inf.run(n_samples=10, optimizer=tf.train.AdamOptimizer(learning_rate=5e-2))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : 1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 2315.045
  :END:

  Tabulate the estimated hyperparameters:

  #+BEGIN_SRC ipython
    pd.DataFrame(ed.get_session().run(
      [resid_var_scale,
       rate_effect_scale,
       ind_bias_scale,
       disp_effect_scale,
       disp_bias_scale]))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
            0
    0  0.008109
    1  1.785125
    2  0.930144
    3  0.019294
    4  0.305007
  #+END_EXAMPLE
  :END:

  Compute posterior 95% credible intervals for the effect sizes:

  #+BEGIN_SRC ipython
    res = pd.DataFrame(np.hstack(ed.get_session().run(
      [q_rate_effect.mean(),
       1.96 * tf.sqrt(q_rate_effect.variance()),
       q_disp_effect.mean(),
       1.96 * tf.sqrt(q_disp_effect.variance())])),
      columns=['rate_effect_mean', 'rate_effect_ci', 'disp_effect_mean', 'disp_effect_ci'])
    res
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
     rate_effect_mean  rate_effect_ci  disp_effect_mean  disp_effect_ci
    0          1.811013        0.077898         -0.005509        0.038054
  #+END_EXAMPLE
  :END:

  Estimate posterior 95% credible intervals for the bias terms:

  #+BEGIN_SRC ipython :ipyfile figure/hurdle.org/bias.png
    res = pd.DataFrame(np.hstack(ed.get_session().run(
      [q_ind_bias.mean(),
       1.96 * tf.sqrt(q_ind_bias.variance()),
       q_disp_bias.mean(),
       1.96 * tf.sqrt(q_ind_bias.variance())]
    )))
    plt.clf()
    fig, ax = plt.subplots(2, 1)
    ax[0].errorbar(x=res.index, y=res[0], yerr=res[1], fmt='o')
    ax[0].set_xticks([])
    ax[0].set_xlabel('')
    ax[0].set_ylabel('Rate bias')
    ax[1].errorbar(x=res.index, y=res[2], yerr=res[3], fmt='o')
    ax[1].set_xticks([])
    ax[1].set_xlabel('Individual')
    ax[1].set_ylabel('Dispersion bias')
    plt.gcf()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:figure/hurdle.org/bias.png]]
  :END:

  Plot a posterior predictive draw, and the real data means and twice standard
  deviations.

  #+BEGIN_SRC ipython :ipyfile figure/hurdle.org/post-pred.png
    post_pred = ed.get_session().run(
      ed.copy(log_cpm, inf.latent_vars),
      {
        onehot: data['onehot'][nonzero_cpm],
        genotypes: data['genotypes'][:,0:1],
        cell_bias: data['normalizers'][nonzero_cpm],
        log_cpm: data['log_cpm'][nonzero_cpm],
      })

    plt.clf()
    plt.gcf().set_size_inches(8, 6)
    plt.scatter(x=np.where(data['onehot'][nonzero_cpm] == 1)[1] + np.random.normal(scale=0.1, size=q), y=post_pred, s=2)
    plt.errorbar(x=np.arange(n), y=onehot_cpm.mean(axis=0), yerr=2 * onehot_cpm.std(axis=0), fmt='o', c='red')
    plt.xlabel('Individual')
    plt.ylabel('$\log_2(CPM + 1)$')
    plt.gca().set_xticks([])
    plt.gcf()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:figure/hurdle.org/post-pred.png]]
  :END:

  Investigate the terms of the variance model to understand why the posterior
  predictive distribution has larger variance than the original data. Compare
  the sample variance of non-zero CPM between cells within each individual to
  the estimated variance (plugging in the estimated posterior means into the
  model).

  #+BEGIN_SRC ipython
    pd.DataFrame(np.hstack([
      onehot_cpm.var(axis=0).filled().reshape(-1, 1),
      ed.get_session().run(tf.exp(-tf.matmul(genotypes, q_rate_effect.mean()) - q_ind_bias.mean()), {onehot: data['onehot'][nonzero_cpm], genotypes: data['genotypes'], cell_bias: data['normalizers'][nonzero_cpm]}),
      ed.get_session().run(tf.exp(tf.matmul(genotypes, q_disp_effect.mean()) + q_disp_bias.mean()), {onehot: data['onehot'][nonzero_cpm], genotypes: data['genotypes'], cell_bias: data['normalizers'][nonzero_cpm]})
    ]), columns=['sample_var', 'mean_component', 'disp_component'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
      sample_var  mean_component  disp_component
    0     0.871253        0.302830        1.118143
    1     0.685687        0.288532        0.619445
    2     0.832976        0.156608        0.971450
    3     0.700838        0.201736        0.719656
    4     0.814616        0.274285        0.666530
    5     0.691467        0.333145        0.607604
    6     0.783659        0.192529        1.115879
    7     0.802240        0.149890        1.041115
    8     0.834025        0.244234        1.002957
    9     0.772049        0.227092        0.910875
    10    0.473087        0.141089        0.815421
    11    0.849796        0.278309        0.724199
    12    0.866909        0.139993        0.909418
    13    1.008157        0.169650        1.079417
    14    0.534548        0.137537        0.923383
    15    0.577289        0.136400        1.124875
    16    0.592843        0.183967        0.855929
    17    0.868248        0.262581        0.881569
    18    0.643367        0.183501        0.850260
    19    0.610603        0.150725        0.781642
    20    0.561710        0.275817        0.672184
  #+END_EXAMPLE
  :END:

  The dispersion effect is close to zero, which suggests that the problem is
  with the estimation of \(u_i^\phi\):

  #+BEGIN_SRC ipython
    ed.get_session().run(tf.exp(q_disp_bias.mean()), {onehot: data['onehot'][nonzero_cpm], genotypes: data['genotypes'], cell_bias: data['normalizers'][nonzero_cpm]})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  array([[ 1.08771348],
           [ 0.67666239],
           [ 1.02014589],
           [ 0.7174871 ],
           [ 0.73400629],
           [ 0.60470819],
           [ 1.13071108],
           [ 1.08213329],
           [ 1.0003413 ],
           [ 0.86023176],
           [ 0.82803261],
           [ 0.70752406],
           [ 0.91508389],
           [ 1.10532486],
           [ 0.89808285],
           [ 1.13777578],
           [ 0.83974165],
           [ 0.81459731],
           [ 0.88679075],
           [ 0.82396138],
           [ 0.67643213]], dtype=float32)
  #+END_EXAMPLE
  :END:
  
* Next steps

  1. *Do we need to fit a multivariate model?* Doing inference one SNP at a
     time will be slow (6s per SNP). We should be able to fit a multivariate
     regression with the spike and slab prior in roughly the same amount of
     time. However, inference of the corresponding variational approximation is
     only guaranteed to find a SNP in LD with the causal variant (Carbonetto
     and Stephens 2012), and is known to overstate the confidence in its
     posterior inclusion probability (Park et al. 2016).

  2. *Do we need to actually fit a dropout model?* The key idea of ~mast~
     ([[https://dx.doi.org/10.1186/s13059-015-0844-5][Finak et al 2015]]) is to model non-zero \(R\) using a Gaussian
     distribution, and model zero \(R\) using logistic regression.

     This can be easily be incorporated in the BBVI algorithm (although likely
     not in the Edward probabilistic programming language).

     Incorporating the mean expression model as well as other known covariates
     into the dropout model might allow us to reliably estimate the mean
     parameters (i.e., reduce their posterior variance) even for genes with
     moderate levels of zero-inflation.

     We might choose not to model zeros because ~mast~ conditions on the
     observed \(Y = 0\), not on a latent \(Z = 0\). This means we could simply
     do the same and ignore zeros.

     If we did so, then mean/variance QTL effect size estimation will be less
     robust for genes with high dropout. This might not be a problem depending
     on the stringency of gene filtering.

  3. *For a single gene, do we need to worry about mean QTLs in LD with
     variance QTLs?* We previously [[https://github.com/YPARK/fqtl][built multivariate mean/variance QTL models]]
     which could account for LD, and could share information between the mean
     and variance models.

     The fundamental problem is that if we assume that the mean and dispersion
     both have genetic components, then the mean is no longer independent of
     the dispersion.

     This actually could be derived without using the fact that both depend on
     the same genotypes if we use second-order Taylor expansion:

     \[ \mu \approx \log_2 \lambda + \frac{V[R]}{2 \lambda^2} \]

  4. *Do we need to share parameters between genes?* ~mast~ assumes genes are
     conditionally independent. But this is no longer true when nearby genes
     can be driven by overlapping (or correlated) /cis/-genotypes.

     We previously developed multiresponse QTL models which learned the target
     genes of causal variants, allowing the true target gene to explain away
     nearby correlated genes ([[https://www.biorxiv.org/content/early/2017/11/14/219428][Park et al 2017]]).

