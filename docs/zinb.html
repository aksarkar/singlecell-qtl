<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2018-12-17 Mon 11:34 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Mean/dispersion estimation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="site_libs/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Mean/dispersion estimation</h1>

<div id="outline-container-org49d326f" class="outline-2">
<h2 id="org49d326f">Introduction</h2>
<div class="outline-text-2" id="text-org49d326f">
<p>
We take a modular approach to call QTLs:
</p>

<ol class="org-ol">
<li>Estimate a mean and a dispersion for each individual</li>
<li>Treat the mean/dispersion as continuous phenotypes and perform QTL mapping</li>
</ol>

<p>
Here, we solve (1).
</p>

<ol class="org-ol">
<li><a href="#orgd9442d4">We implement GPU-based ML estimation</a> of a zero-inflated negative binomial
model</li>
<li><a href="#orgca44e7f">We show in simulation</a> that the estimates are unbiased</li>
<li><a href="#org7ae35b9">We compare ZINB estimates of mean expression</a> against sample-based estimates</li>
<li><a href="#org17af587">We implement a test for goodness of fit</a> for the ZINB model</li>
</ol>
</div>
</div>

<div id="outline-container-orgc376b56" class="outline-2">
<h2 id="orgc376b56">Model specification</h2>
<div class="outline-text-2" id="text-orgc376b56">
<p>
Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
\(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell.
</p>

<p>
\[ r_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})\text{Poisson}(\cdot; R_{ij} \mu_{ik} u_{ijk}) \]
</p>

<p>
\[ u_{ijk} \sim \text{Gamma}(\cdot; \phi_{ik}^{-1}, \phi_{ik}^{-1}) \]
</p>

<p>
Here, \(\mu_{ik}\) is proportional to relative expression (<a href="https://arxiv.org/abs/1104.3889">Pachter 2011</a>), and
\(\phi_{ik}\) is the variance of expression noise.
</p>

<p>
Considering just the Poisson component, marginalizing out \(u\) yields the
log likelihood:
</p>

<p>
\[ l(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]
</p>

<p>
Then, marginalizing over the mixture yields the log likelihood:
</p>

<p>
\[ \ln p(r_{ijk} \mid \cdot) = \ln(\pi_{ik} + \exp(l(\cdot)))\ \text{if}\ r_{ijk} = 0 \]
</p>

<p>
\[ \ln p(r_{ijk} \mid \cdot) = l(\cdot)\ \text{otherwise} \]
</p>

<p>
We have enough observations per mean/dispersion parameter that simply
minimizing the negative log likelihood should give reasonable estimates.
</p>

<p>
This model is equivalent to a model where we assume that the underlying rate
is a point-Gamma mixture:
</p>

<p>
\[ r_{ijk} \mid \lambda_{ijk} \sim \mathrm{Poisson}(\cdot; R_{ij}\lambda_{ijk}) \]
</p>

<p>
\[ \lambda_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})
  \text{Gamma}(\lambda_{ijk}; \phi_{ik}^{-1}, \phi_{ik}^{-1}\mu_{ik}^{-1}) \]
</p>

<p>
The Gamma component of this mixture corresponds to \(\mu_{ik}u_{ijk}\) in the
model above. Considering just the Gamma component, marginalizing out
\(\lambda\) yields the log likelihood:
</p>

<p>
\[ \tilde{l}(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}} \right) + \phi_{ik}^{-1} \ln\left(\frac{\phi_{ik}^{-1}\mu_{ik}^{-1}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}}\right) + \ln\Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln\Gamma(r_{ijk} + 1) - \ln\Gamma(\phi_{ik}^{-1}) \]
</p>

<p>
It is clear \(l = \tilde{l}\), and therefore the marginal likelihoods (over
the mixture components) are also equal.
</p>
</div>
</div>

<div id="outline-container-orgd9442d4" class="outline-2">
<h2 id="orgd9442d4">Tensorflow implementation</h2>
<div class="outline-text-2" id="text-orgd9442d4">
<p>
Use <code>tensorflow</code> to optimize all of the parameters together using one-hot
encoding to map parameters to data points. This makes inference more amenable
to running on the GPU.
</p>

<p>
The method is implemented in the Python package <code>scqtl</code>.
</p>
</div>

<div id="outline-container-orgca44e7f" class="outline-3">
<h3 id="orgca44e7f">Simulation</h3>
<div class="outline-text-3" id="text-orgca44e7f">
<div class="org-src-container">
<pre class="src src-ipython" id="org6a724dd">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;

<span class="org-keyword">def</span> <span class="org-function-name">evaluate</span>(num_samples, num_mols, num_trials=10):
  <span class="org-comment-delimiter"># </span><span class="org-comment">This will be reset inside the simulation to generate counts, but we need to</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">fix it to get one design matrix for all the simulated genes</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">def simulate(num_samples, size=None, log_mu=None, log_phi=None, logodds=None, seed=None, design=None, fold=None):</span>
  <span class="org-variable-name">design</span> = np.zeros((num_samples * num_trials, 1))
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: generate all of the samples for each trial in one shot, and use</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">one-hot encoding to get separate estimates</span>
  <span class="org-variable-name">args</span> = [(num_samples * num_trials, num_mols, log_mu, log_phi, logodds, <span class="org-constant">None</span>, <span class="org-constant">None</span>, <span class="org-constant">None</span>)
          <span class="org-keyword">for</span> log_mu <span class="org-keyword">in</span> np.linspace(-12, -6, 7)
          <span class="org-keyword">for</span> log_phi <span class="org-keyword">in</span> np.linspace(-4, 0, 5)
          <span class="org-keyword">for</span> logodds <span class="org-keyword">in</span> np.linspace(-3, 3, 7)]
  <span class="org-variable-name">umi</span> = np.concatenate([scqtl.simulation.simulate(*a)[0][:,:1] <span class="org-keyword">for</span> a <span class="org-keyword">in</span> args], axis=1)
  <span class="org-variable-name">onehot</span> = np.zeros((num_samples * num_trials, num_trials))
  onehot[np.arange(onehot.shape[0]), np.arange(onehot.shape[0]) // num_samples] = 1

  <span class="org-variable-name">init</span> = scqtl.tf.fit(
    umi=umi.astype(np.float32),
    onehot=onehot.astype(np.float32),
    design=design.astype(np.float32),
    size_factor=(num_mols * np.ones((num_samples * num_trials, 1))).astype(np.float32),
    learning_rate=1e-3,
    max_epochs=20000,
    verbose=<span class="org-constant">True</span>,
  )
  <span class="org-variable-name">log_mu</span>, <span class="org-variable-name">log_phi</span>, <span class="org-variable-name">logodds</span>, <span class="org-variable-name">nb_llik</span>, <span class="org-variable-name">zinb_llik</span> = scqtl.tf.fit(
    umi=umi.astype(np.float32),
    onehot=onehot.astype(np.float32),
    design=design.astype(np.float32),
    size_factor=(num_mols * np.ones((num_samples * num_trials, 1))).astype(np.float32),
    learning_rate=1e-3,
    max_epochs=20000,
    warm_start=init[:3],
    verbose=<span class="org-constant">True</span>)
  <span class="org-variable-name">result</span> = pd.DataFrame(
    [(<span class="org-string">'rmsprop'</span>, a[0] // num_trials, <span class="org-builtin">int</span>(a[1]), <span class="org-builtin">int</span>(a[2]), <span class="org-builtin">int</span>(a[3]), <span class="org-builtin">int</span>(a[4]), a[-1], trial)
     <span class="org-keyword">for</span> a <span class="org-keyword">in</span> args
     <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials)],
    columns=[<span class="org-string">'method'</span>, <span class="org-string">'num_samples'</span>, <span class="org-string">'num_mols'</span>, <span class="org-string">'log_mu'</span>, <span class="org-string">'log_phi'</span>, <span class="org-string">'logodds'</span>, <span class="org-string">'fold'</span>, <span class="org-string">'trial'</span>])
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: the results need to be transposed before flattening</span>
  <span class="org-variable-name">result</span>[<span class="org-string">'log_mu_hat'</span>] = log_mu.ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'log_phi_hat'</span>] = log_phi.ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'logodds_hat'</span>] = logodds.ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'mean'</span>] = result[<span class="org-string">'num_mols'</span>] * np.exp(result[<span class="org-string">'log_mu_hat'</span>])
  <span class="org-variable-name">result</span>[<span class="org-string">'var'</span>] = result[<span class="org-string">'mean'</span>] + np.square(result[<span class="org-string">'mean'</span>]) * np.exp(result[<span class="org-string">'log_phi_hat'</span>])
  <span class="org-variable-name">log_cpm</span> = np.log(np.ma.masked_values(umi.reshape(num_trials, -1, umi.shape[-1]), 0)) - np.log(num_mols) + 6 * np.log(10)
  <span class="org-variable-name">result</span>[<span class="org-string">'mean_log_cpm'</span>] = log_cpm.mean(axis=1).ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'var_log_cpm'</span>] = log_cpm.var(axis=1).ravel(order=<span class="org-string">'F'</span>)

  <span class="org-variable-name">diagnostic</span> = []
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(umi.shape[1]):
    <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(onehot.shape[1]):
      <span class="org-variable-name">idx</span> = onehot[:,j].astype(<span class="org-builtin">bool</span>)
      diagnostic.append(scqtl.diagnostic.diagnostic_test(
        umi[idx,i].reshape(-1, 1),
        log_mu[j,i],
        log_phi[j,i],
        -logodds[j,i],
        num_mols,
        np.ones((num_samples, 1))))
  <span class="org-variable-name">diagnostic</span> = np.array(diagnostic)
  <span class="org-variable-name">result</span>[<span class="org-string">'ks_d'</span>] = diagnostic[:,0]
  <span class="org-variable-name">result</span>[<span class="org-string">'ks_p'</span>] = diagnostic[:,1]

  <span class="org-keyword">return</span> result

<span class="org-variable-name">res</span> = evaluate(num_samples=95, num_mols=114026, num_trials=10)
res.to_csv(<span class="org-string">'tf-simulation.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=1:00:00 --job-name=tf-sim --output=tf-sim.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-sim.py
</pre>
</div>

<pre class="example">
Submitted batch job 55767347

</pre>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">result</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/density-estimation/tf-simulation.txt.gz'</span>, index_col=0)
</pre>
</div>

<p>
Get the latent mean and variance.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">result</span>[<span class="org-string">'latent_mean'</span>] = np.exp(result[<span class="org-string">'log_mu'</span>] - np.log1p(np.exp(result[<span class="org-string">'logodds'</span>])))
<span class="org-variable-name">result</span>[<span class="org-string">'latent_mean_hat'</span>] = np.exp(result[<span class="org-string">'log_mu_hat'</span>] - np.log1p(np.exp(result[<span class="org-string">'logodds_hat'</span>])))
<span class="org-variable-name">result</span>[<span class="org-string">'latent_var'</span>] = np.exp(2 * result[<span class="org-string">'log_mu'</span>] + result[<span class="org-string">'log_phi'</span>] - np.log1p(np.exp(result[<span class="org-string">'logodds'</span>]))) + np.exp(-np.log1p(np.exp(result[<span class="org-string">'logodds'</span>])) - np.log1p(np.exp(-result[<span class="org-string">'logodds'</span>])) + 2 * result[<span class="org-string">'log_mu'</span>])
<span class="org-variable-name">result</span>[<span class="org-string">'latent_var_hat'</span>] = np.exp(2 * result[<span class="org-string">'log_mu_hat'</span>] + result[<span class="org-string">'log_phi_hat'</span>] - np.log1p(np.exp(result[<span class="org-string">'logodds_hat'</span>]))) + np.exp(-np.log1p(np.exp(result[<span class="org-string">'logodds_hat'</span>])) - np.log1p(np.exp(-result[<span class="org-string">'logodds_hat'</span>])) + 2 * result[<span class="org-string">'log_mu_hat'</span>])
</pre>
</div>

<p>
Plot the accuracy of the estimated parameters and derived quantities, fixing
the experiment size.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">exp_pass</span> = np.logical_and(result[<span class="org-string">'num_samples'</span>] == 95, result[<span class="org-string">'num_mols'</span>] == 114026)
<span class="org-variable-name">mu_pass</span> = result[<span class="org-string">'log_mu'</span>] &gt; -10
<span class="org-variable-name">pi_pass</span> = result[<span class="org-string">'logodds'</span>] &lt; 0

plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 3)
fig.set_size_inches(8, 5)

<span class="org-variable-name">subset</span> = result.loc[np.logical_and.<span class="org-builtin">reduce</span>(np.vstack([exp_pass, pi_pass]))]
ax[0, 0].scatter(subset[<span class="org-string">'log_mu'</span>], subset[<span class="org-string">'log_mu_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
ax[0, 0].plot(ax[0, 0].get_xlim(), ax[0, 0].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
ax[0, 0].set_xlabel(<span class="org-string">'True $\ln(\mu)$'</span>)
ax[0, 0].set_ylabel(<span class="org-string">'Estimated $\ln(\mu)$'</span>)

ax[1, 0].set_xscale(<span class="org-string">'log'</span>)
ax[1, 0].set_yscale(<span class="org-string">'log'</span>)
ax[1, 0].scatter(subset[<span class="org-string">'latent_mean'</span>], subset[<span class="org-string">'latent_mean_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
ax[1, 0].plot(ax[1, 0].get_xlim(), ax[1, 0].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
ax[1, 0].set_xlabel(<span class="org-string">'True latent mean'</span>)
ax[1, 0].set_ylabel(<span class="org-string">'Estimated latent mean'</span>)

<span class="org-variable-name">subset</span> = result.loc[np.logical_and.<span class="org-builtin">reduce</span>(np.vstack([exp_pass, mu_pass, pi_pass]))]
ax[0, 1].scatter(subset[<span class="org-string">'log_phi'</span>], subset[<span class="org-string">'log_phi_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
ax[0, 1].plot(ax[0, 1].get_xlim(), ax[0, 1].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
ax[0, 1].set_xlabel(<span class="org-string">'True $\ln(\phi)$'</span>)
ax[0, 1].set_ylabel(<span class="org-string">'Estimated $\ln(\phi)$'</span>)

ax[1, 1].set_xscale(<span class="org-string">'log'</span>)
ax[1, 1].set_yscale(<span class="org-string">'log'</span>)
ax[1, 1].scatter(subset[<span class="org-string">'latent_var'</span>], subset[<span class="org-string">'latent_var_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
ax[1, 1].plot(ax[1, 1].get_xlim(), ax[1, 1].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
ax[1, 1].set_xlabel(<span class="org-string">'True latent variance'</span>)
ax[1, 1].set_ylabel(<span class="org-string">'Estimated latent variance'</span>)

<span class="org-variable-name">subset</span> = result.loc[exp_pass]
ax[0, 2].scatter(subset[<span class="org-string">'logodds'</span>], subset[<span class="org-string">'logodds_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
ax[0, 2].plot(ax[0, 2].get_xlim(), ax[0, 2].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
ax[0, 2].set_xlabel(<span class="org-string">'True $\mathrm{logit}(\pi)$'</span>)
ax[0, 2].set_ylabel(<span class="org-string">'Estimated $\mathrm{logit}(\pi)$'</span>)

ax[1, 2].set_axis_off()
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/sim.png" alt="sim.png">
</p>
</div>

<p>
Plot the accuracy of estimated latent mean and variance as a function of
number of molecules, holding the number of cells fixed at the median value
in the real data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(3, 2)
fig.set_size_inches(6, 9)

<span class="org-keyword">for</span> i, (k, g) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(result.loc[result[<span class="org-string">'num_samples'</span>] == 95].groupby(<span class="org-string">'num_mols'</span>)):
  <span class="org-variable-name">mu_pass</span> = g[<span class="org-string">'log_mu'</span>] &gt; -10
  <span class="org-variable-name">pi_pass</span> = g[<span class="org-string">'logodds'</span>] &lt; 0
  <span class="org-variable-name">subset</span> = g.loc[pi_pass]
  ax[i, 0].semilogx()
  ax[i, 0].semilogy()
  ax[i, 0].scatter(subset[<span class="org-string">'latent_mean'</span>], subset[<span class="org-string">'latent_mean_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
  ax[i, 0].plot(ax[i, 0].get_xlim(), ax[i, 0].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
  ax[i, 0].set_xlabel(<span class="org-string">'True expression mean'</span>)
  ax[i, 0].set_ylabel(<span class="org-string">'Estimated expression mean'</span>)
  ax[i, 0].set_title(<span class="org-string">'Molecules $= {}$'</span>.<span class="org-builtin">format</span>(k))

  <span class="org-variable-name">subset</span> = g.loc[functools.<span class="org-builtin">reduce</span>(np.logical_and, [mu_pass, pi_pass])]
  ax[i, 1].semilogx()
  ax[i, 1].semilogy()
  ax[i, 1].scatter(subset[<span class="org-string">'latent_var'</span>], subset[<span class="org-string">'latent_var_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
  ax[i, 1].plot(ax[i, 1].get_xlim(), ax[i, 1].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
  ax[i, 1].set_xlabel(<span class="org-string">'True expression variance'</span>)
  ax[i, 1].set_ylabel(<span class="org-string">'Estimated expression variance'</span>)
  ax[i, 1].set_title(<span class="org-string">'Molecules $= {}$'</span>.<span class="org-builtin">format</span>(k))

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/simulation-latent-accuracy-vs-mols.png" alt="simulation-latent-accuracy-vs-mols.png">
</p>
</div>

<p>
Plot the accuracy of estimated latent mean and variance as a function of
number of cells, holding the number of molecules fixed at the median value
in the real data.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(3, 2)
fig.set_size_inches(6, 9)

<span class="org-keyword">for</span> i, (k, g) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(result.loc[result[<span class="org-string">'num_mols'</span>] == 114026].groupby(<span class="org-string">'num_samples'</span>)):
  <span class="org-variable-name">mu_pass</span> = g[<span class="org-string">'log_mu'</span>] &gt; -10
  <span class="org-variable-name">pi_pass</span> = g[<span class="org-string">'logodds'</span>] &lt; 0
  <span class="org-variable-name">subset</span> = g.loc[pi_pass]
  ax[i, 0].semilogx()
  ax[i, 0].semilogy()
  ax[i, 0].scatter(subset[<span class="org-string">'latent_mean'</span>], subset[<span class="org-string">'latent_mean_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
  ax[i, 0].plot(ax[i, 0].get_xlim(), ax[i, 0].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
  ax[i, 0].set_xlabel(<span class="org-string">'True expression mean'</span>)
  ax[i, 0].set_ylabel(<span class="org-string">'Estimated expression mean'</span>)
  ax[i, 0].set_title(<span class="org-string">'$n = {}$'</span>.<span class="org-builtin">format</span>(k))

  <span class="org-variable-name">subset</span> = g.loc[functools.<span class="org-builtin">reduce</span>(np.logical_and, [mu_pass, pi_pass])]
  ax[i, 1].semilogx()
  ax[i, 1].semilogy()
  ax[i, 1].scatter(subset[<span class="org-string">'latent_var'</span>], subset[<span class="org-string">'latent_var_hat'</span>], s=2, c=<span class="org-string">'k'</span>)
  ax[i, 1].plot(ax[i, 1].get_xlim(), ax[i, 1].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
  ax[i, 1].set_xlabel(<span class="org-string">'True expression variance'</span>)
  ax[i, 1].set_ylabel(<span class="org-string">'Estimated expression variance'</span>)
  ax[i, 1].set_title(<span class="org-string">'$n = {}$'</span>.<span class="org-builtin">format</span>(k))

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/simulation-latent-accuracy-vs-cells.png" alt="simulation-latent-accuracy-vs-cells.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgb4319a4" class="outline-3">
<h3 id="orgb4319a4">Compare against CPU implementation</h3>
<div class="outline-text-3" id="text-orgb4319a4">
<p>
We also implemented second-order optimization of the ZINB log
likelihood. Compare the simulation results of the GPU implementation to the
CPU implementation to make verify.
</p>

<p>
Write the results to the database backing the interactive browser.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">tf_res</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/tf-simulation.txt.gz'</span>, index_col=0)
<span class="org-variable-name">np_res</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/np-simulation.txt.gz'</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">np_res</span>[<span class="org-string">'method'</span>] = <span class="org-string">'lbfgs'</span>
<span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  pd.concat([tf_res, np_res], axis=0, join=<span class="org-string">'inner'</span>).to_sql(name=<span class="org-string">'simulation'</span>, con=conn, index=<span class="org-constant">False</span>, if_exists=<span class="org-string">'replace'</span>)
  conn.execute(<span class="org-string">'create index ix_simulation on simulation(log_mu, log_phi, logodds);'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgb7df54b" class="outline-3">
<h3 id="orgb7df54b">Fit ZINB2</h3>
<div class="outline-text-3" id="text-orgb7df54b">
<p>
Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org7a5399c"><span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">keep_genes</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
<span class="org-variable-name">header</span> = <span class="org-builtin">sorted</span>(<span class="org-builtin">set</span>(annotations[<span class="org-string">'chip_id'</span>]))
<span class="org-variable-name">umi</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, index_col=0).loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
<span class="org-variable-name">index</span> = umi.index
</pre>
</div>

<p>
Prepare the design matrix of covariates.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgd5e1fe2"><span class="org-variable-name">onehot</span> = recode(annotations, <span class="org-string">'chip_id'</span>)

<span class="org-variable-name">designs</span> = []

<span class="org-comment-delimiter"># </span><span class="org-comment">Null covariate model</span>
designs.append(np.zeros((onehot.shape[0], 1)))

<span class="org-variable-name">chip</span> = recode(annotations, <span class="org-string">'experiment'</span>)
<span class="org-variable-name">chip</span> -= chip.mean(axis=0)
designs.append(chip)

<span class="org-comment-delimiter"># </span><span class="org-comment">These explain most PVE of circular pseudotime (Joyce Hsiao, personal</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">communication)</span>
<span class="org-variable-name">cell_cycle_genes</span> = [
  <span class="org-string">'ENSG00000094804'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">CDC6</span>
  <span class="org-string">'ENSG00000170312'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">CDK1</span>
  <span class="org-string">'ENSG00000175063'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">UBE2C</span>
  <span class="org-string">'ENSG00000131747'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">TOP2A</span>
  <span class="org-string">'ENSG00000197061'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">HIST1H4C</span>
  ]
<span class="org-variable-name">cell_cycle</span> = (umi.loc[cell_cycle_genes].values / annotations[<span class="org-string">'mol_hs'</span>].values).reshape(-1, <span class="org-builtin">len</span>(cell_cycle_genes))
<span class="org-variable-name">cell_cycle</span> -= cell_cycle.mean(axis=0)
<span class="org-variable-name">cell_cycle</span> /= cell_cycle.std(axis=0)
designs.append(cell_cycle)

designs.append(np.concatenate([chip, cell_cycle], axis=1))
</pre>
</div>

<p>
Estimate the parameters of the zero-inflated model assuming dropout per
individual and gene.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgc8dfe3f">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;
<span class="org-keyword">import</span> argparse
&lt;&lt;recode-impl&gt;&gt;

<span class="org-variable-name">parser</span> = argparse.ArgumentParser()
parser.add_argument(<span class="org-string">'--design'</span>, <span class="org-builtin">help</span>=<span class="org-string">'Design matrix of confounders'</span>, choices=<span class="org-builtin">list</span>(<span class="org-builtin">range</span>(2)), <span class="org-builtin">type</span>=<span class="org-builtin">int</span>)
<span class="org-variable-name">args</span> = parser.parse_args()
&lt;&lt;read-data-qc-impl&gt;&gt;
&lt;&lt;prepare-covars&gt;&gt;

<span class="org-variable-name">umi</span> = umi.T.astype(np.float32)
<span class="org-variable-name">onehot</span> = onehot.astype(np.float32)
<span class="org-variable-name">design</span> = designs[args.design].astype(np.float32)
<span class="org-variable-name">size_factor</span> = annotations[<span class="org-string">'mol_hs'</span>].astype(np.float32).values.reshape(-1, 1)
<span class="org-variable-name">init</span> = scqtl.tf.fit(
  umi=umi,
  onehot=onehot,
  design=design,
  size_factor=size_factor,
  learning_rate=1e-3,
  max_epochs=30000,
  verbose=<span class="org-constant">True</span>,
)
<span class="org-variable-name">log_mu</span>, <span class="org-variable-name">log_phi</span>, <span class="org-variable-name">logodds</span>, <span class="org-variable-name">nb_nll</span>, <span class="org-variable-name">zinb_nll</span>, <span class="org-variable-name">beta</span> = scqtl.tf.fit(
  umi=umi,
  onehot=onehot,
  design=design,
  size_factor=size_factor,
  learning_rate=1e-3,
  warm_start=init[:3],
  max_epochs=30000,
  return_beta=<span class="org-constant">True</span>,
  verbose=<span class="org-constant">True</span>,
)
pd.DataFrame(log_mu.T, index=index, columns=header).to_csv(<span class="org-string">'zi2-log-mu.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(log_phi.T, index=index, columns=header).to_csv(<span class="org-string">'zi2-log-phi.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(logodds.T, index=index, columns=header).to_csv(<span class="org-string">'zi2-logodds.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(beta.T, index=index).to_csv(<span class="org-string">'beta.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh" id="orgab32e4c">sbatch --partition=gpu --gres=gpu:1 --mem=16G --time=7:00:00 -a 1 --job-name=tf-zinb --output=zinb2.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
mkdir -p design$<span class="org-variable-name">SLURM_ARRAY_TASK_ID</span>
<span class="org-builtin">pushd</span> design$<span class="org-variable-name">SLURM_ARRAY_TASK_ID</span>
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb.py --design $<span class="org-variable-name">SLURM_ARRAY_TASK_ID</span>
</pre>
</div>

<pre class="example">
Submitted batch job 55727034

</pre>

<div class="org-src-container">
<pre class="src src-sh" id="org245007b">sbatch --partition=broadwl --time=10
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
cat &gt;.rsync-filter &lt;&lt;EOF
<span class="org-sh-heredoc">+ */</span>
<span class="org-sh-heredoc">+ *.txt.gz</span>
<span class="org-sh-heredoc">- *</span>
<span class="org-sh-heredoc">EOF</span>
rsync -FFau --delete . /project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/
</pre>
</div>

<pre class="example">
Submitted batch job 55767365

</pre>
</div>
</div>
</div>

<div id="outline-container-org0bb6277" class="outline-2">
<h2 id="org0bb6277">numpy/scipy implementation</h2>
<div class="outline-text-2" id="text-org0bb6277">
<p>
Optimize the negative log-likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgff4d856"><span class="org-keyword">def</span> <span class="org-function-name">log</span>(x):
  <span class="org-doc">"""Numerically safe log"""</span>
  <span class="org-keyword">return</span> np.log(x + 1e-8)

<span class="org-keyword">def</span> <span class="org-function-name">sigmoid</span>(x):
  <span class="org-doc">"""Numerically safe sigmoid"""</span>
  <span class="org-variable-name">lim</span> = np.log(np.finfo(np.float64).resolution)
  <span class="org-keyword">return</span> np.clip(sp.expit(x), lim, -lim)

<span class="org-keyword">def</span> <span class="org-function-name">nb</span>(theta, x, size, onehot, design):
  <span class="org-doc">"""Return the per-data point log likelihood</span>

<span class="org-doc">  x ~ Poisson(size .* design' * theta[2 * m:k] * exp(onehot * theta[:m]) * u)</span>
<span class="org-doc">  u ~ Gamma(exp(onehot * theta[m:2 * m]), exp(onehot * theta[m:2 * m]))</span>

<span class="org-doc">  theta - (2 * m + k, 1)</span>
<span class="org-doc">  x - (n, 1)</span>
<span class="org-doc">  size - (n, 1)</span>
<span class="org-doc">  onehot - (n, m)</span>
<span class="org-doc">  design - (n, k)</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">n</span>, <span class="org-variable-name">m</span> = onehot.shape
  <span class="org-keyword">assert</span> x.shape == (n,)
  <span class="org-keyword">assert</span> size.shape == (n,)
  <span class="org-keyword">assert</span> design.shape[0] == n
  <span class="org-keyword">assert</span> theta.shape == (2 * m + design.shape[1],)
  <span class="org-variable-name">mean</span> = size * np.exp(onehot.dot(theta[:m]) + design.dot(theta[2 * m:]))
  <span class="org-keyword">assert</span> mean.shape == (n,)
  <span class="org-variable-name">inv_disp</span> = onehot.dot(np.exp(theta[m:2 * m]))
  <span class="org-keyword">assert</span> inv_disp.shape == (n,)
  <span class="org-keyword">return</span> (x * log(mean / inv_disp) -
          x * log(1 + mean / inv_disp) -
          inv_disp * log(1 + mean / inv_disp) +
          sp.gammaln(x + inv_disp) -
          sp.gammaln(inv_disp) -
          sp.gammaln(x + 1))

<span class="org-keyword">def</span> <span class="org-function-name">_nb</span>(theta, x, size, onehot, design=<span class="org-constant">None</span>):
  <span class="org-doc">"""Return the mean negative log likelihood of x"""</span>
  <span class="org-keyword">return</span> -nb(theta, x, size, onehot, design).mean()

<span class="org-keyword">def</span> <span class="org-function-name">zinb</span>(theta, x, size, onehot, design=<span class="org-constant">None</span>):
  <span class="org-doc">"""Return the mean negative log likelihood of x"""</span>
  <span class="org-variable-name">n</span>, <span class="org-variable-name">m</span> = onehot.shape
  <span class="org-variable-name">logodds</span>, <span class="org-variable-name">theta</span> = theta[:m], theta[m:]
  <span class="org-variable-name">case_non_zero</span> = -np.log1p(np.exp(onehot.dot(logodds))) + nb(theta, x, size, onehot, design)
  <span class="org-variable-name">case_zero</span> = np.logaddexp(onehot.dot(logodds - np.log1p(np.exp(logodds))), case_non_zero)
  <span class="org-keyword">return</span> -np.where(x &lt; 1, case_zero, case_non_zero).mean()

<span class="org-keyword">def</span> <span class="org-function-name">_fit_gene</span>(chunk, onehot, design=<span class="org-constant">None</span>):
  <span class="org-variable-name">n</span>, <span class="org-variable-name">m</span> = onehot.shape
  <span class="org-keyword">assert</span> chunk.shape[0] == n
  <span class="org-comment-delimiter"># </span><span class="org-comment">We need to take care here to initialize mu=-inf for all zero observations</span>
  <span class="org-variable-name">x0</span> = np.log((onehot * chunk[:,:1]).<span class="org-builtin">sum</span>(axis=0) / onehot.<span class="org-builtin">sum</span>(axis=0)) - np.log(np.ma.masked_values(onehot, 0) * chunk[:,1:]).mean(axis=0).compressed()
  <span class="org-variable-name">x0</span> = np.hstack((x0, np.zeros(m)))
  <span class="org-keyword">if</span> design <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-keyword">assert</span> design.shape[0] == n
    <span class="org-variable-name">design</span> -= design.mean(axis=0)
    <span class="org-variable-name">x0</span> = np.hstack((x0, np.zeros(design.shape[1])))
  <span class="org-variable-name">res0</span> = so.minimize(_nb, x0=x0, args=(chunk[:,0], chunk[:,1], onehot, design))
  <span class="org-variable-name">res</span> = so.minimize(zinb, x0=<span class="org-builtin">list</span>(np.zeros(m)) + <span class="org-builtin">list</span>(res0.x), args=(chunk[:,0], chunk[:,1], onehot, design))
  <span class="org-keyword">if</span> res0.fun &lt; res.fun:
    <span class="org-comment-delimiter"># </span><span class="org-comment">This isn't a likelihood ratio test. Numerically, our implementation of</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">ZINB can't represent pi = 0, so we need to use a separate implementation</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">for it</span>
    <span class="org-variable-name">log_mu</span> = res0.x[:m]
    <span class="org-variable-name">neg_log_phi</span> = res0.x[m:2 * m]
    <span class="org-variable-name">logit_pi</span> = np.zeros(m)
    logit_pi.fill(-np.inf)
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">logit_pi</span> = res.x[:m]
    <span class="org-variable-name">log_mu</span> = res.x[m:2 * m]
    <span class="org-variable-name">neg_log_phi</span> = res.x[2 * m:3 * m]
  <span class="org-variable-name">mean_by_sample</span> = chunk[:,1] * onehot.dot(np.exp(log_mu))
  <span class="org-variable-name">var_by_sample</span> = mean_by_sample + np.square(mean_by_sample) * onehot.dot(np.exp(-neg_log_phi))
  <span class="org-variable-name">mean_by_ind</span> = np.ma.masked_equal(onehot * mean_by_sample.reshape(-1, 1), 0).mean(axis=0).filled(0)
  <span class="org-variable-name">var_by_ind</span> = np.ma.masked_equal(onehot * (np.square(mean_by_sample - onehot.dot(mean_by_ind)) + var_by_sample).reshape(-1, 1), 0).mean(axis=0).filled(0)
  <span class="org-keyword">return</span> [log_mu, -neg_log_phi, logit_pi, mean_by_ind, var_by_ind]

<span class="org-keyword">def</span> <span class="org-function-name">fit_gene</span>(chunk, bootstraps=100):
  <span class="org-variable-name">orig</span> = _fit_gene(chunk)
  <span class="org-variable-name">B</span> = []
  <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(bootstraps):
    B.append(_fit_gene(chunk[np.random.choice(chunk.shape[0], chunk.shape[0], replace=<span class="org-constant">True</span>)]))
  <span class="org-variable-name">se</span> = np.array(B)[:,:2].std(axis=0)
  <span class="org-keyword">return</span> orig + <span class="org-builtin">list</span>(se.ravel())
</pre>
</div>

<p>
Computing analytic SE runs into numerical problems.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_pois</span>(theta, x, size):
  <span class="org-variable-name">mean</span> = np.exp(theta)
  <span class="org-variable-name">mean</span> *= size
  <span class="org-keyword">return</span> (x * log(mean) - mean - sp.gammaln(x + 1)).mean()

<span class="org-keyword">def</span> <span class="org-function-name">_pois_jac</span>(theta, x, size):
  <span class="org-variable-name">mean</span> = np.exp(theta)
  <span class="org-keyword">return</span> mean * (x / mean - size).mean()

<span class="org-keyword">def</span> <span class="org-function-name">_nb_jac</span>(theta, x, size):
  <span class="org-variable-name">mean</span>, <span class="org-variable-name">inv_disp</span> = np.exp(theta)
  <span class="org-variable-name">T</span> = (1 + size * mean / inv_disp)
  <span class="org-keyword">return</span> mean * (x / mean - size / inv_disp * (x + inv_disp) / T).mean()

<span class="org-keyword">def</span> <span class="org-function-name">check_gradients</span>(x, f, df, args=<span class="org-constant">None</span>, num_trials=100):
  <span class="org-variable-name">x</span> = np.array(x)
  <span class="org-variable-name">y</span> = f(x, *args)
  <span class="org-variable-name">analytic_diff</span> = df(x, *args)
  <span class="org-variable-name">error</span> = []
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
    <span class="org-variable-name">eps</span> = np.random.normal(scale=1e-4, size=x.shape)
    <span class="org-variable-name">num_diff</span> = (f(x + eps, *args) - y) / eps
    error.append(<span class="org-builtin">abs</span>(num_diff - analytic_diff))
  <span class="org-keyword">return</span> np.array(error)
</pre>
</div>
</div>

<div id="outline-container-org23377ab" class="outline-3">
<h3 id="org23377ab">Simulation</h3>
<div class="outline-text-3" id="text-org23377ab">
<p>
Check the parameter estimation on simulated data.
</p>

<p>
Assuming simulated confounders \(x\) are isotropic Gaussian, we can derive
the scale of \(\beta\) to achieve a specified fold-change in relative
abundance:
</p>

<p>
\[ x \sim N(0, 1) \]
</p>

<p>
Letting \(\tau\) denote precision:
</p>

<p>
\[ \beta \sim N(0, \tau) \]
</p>

<p>
\[ x\beta \sim N(0, 1 + \tau) \]
</p>

<p>
\[ \mathbb{E}[x\beta] = y = \exp\left(\frac{1}{2 (1 + \tau)}\right) \]
</p>

<p>
\[ \tau = \frac{1 - 2 \ln y}{2 \ln y} \]  
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org21575d5"><span class="org-keyword">def</span> <span class="org-function-name">simulate</span>(num_samples, size=<span class="org-constant">None</span>, log_mu=<span class="org-constant">None</span>, log_phi=<span class="org-constant">None</span>, logodds=<span class="org-constant">None</span>, seed=<span class="org-constant">None</span>, design=<span class="org-constant">None</span>, fold=<span class="org-constant">None</span>):
  <span class="org-keyword">if</span> seed <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">seed</span> = 0
  np.random.seed(seed)
  <span class="org-keyword">if</span> log_mu <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">log_mu</span> = np.random.uniform(low=-12, high=-8)
  <span class="org-keyword">if</span> log_phi <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">log_phi</span> = np.random.uniform(low=-6, high=0)
  <span class="org-keyword">if</span> size <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">size</span> = 1e5
  <span class="org-keyword">if</span> logodds <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">prob</span> = np.random.uniform()
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">prob</span> = sp.expit(logodds)
  <span class="org-keyword">if</span> design <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">design</span> = np.random.normal(size=(num_samples, 1))
  <span class="org-keyword">else</span>:
    <span class="org-keyword">assert</span> design.shape[0] == num_samples
  <span class="org-keyword">if</span> fold <span class="org-keyword">is</span> <span class="org-constant">None</span> <span class="org-keyword">or</span> np.isclose(fold, 1):
    <span class="org-variable-name">beta</span> = np.array([[0]])
  <span class="org-keyword">else</span>:
    <span class="org-keyword">assert</span> fold &gt; 1
    <span class="org-variable-name">beta</span> = np.random.normal(size=(design.shape[1], 1), scale=2 * np.log(fold) / (1 - 2 * np.log(fold)))

  <span class="org-variable-name">n</span> = np.exp(-log_phi)
  <span class="org-variable-name">p</span> = 1 / (1 + size * np.exp(log_mu + design.dot(beta) + log_phi)).ravel()
  <span class="org-variable-name">x</span> = np.where(np.random.uniform(size=num_samples) &lt; prob,
               0,
               np.random.negative_binomial(n=n, p=p, size=num_samples))
  <span class="org-keyword">return</span> np.vstack((x, size * np.ones(num_samples))).T, design
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython" id="orgd061d7a"><span class="org-keyword">def</span> <span class="org-function-name">batch_design_matrix</span>(num_samples, num_batches):
  <span class="org-doc">"""Return a matrix of binary indicators representing batch assignment"""</span>
  <span class="org-variable-name">design</span> = np.zeros((num_samples, num_batches))
  <span class="org-variable-name">design</span>[np.arange(num_samples), np.random.choice(num_batches, size=num_samples)] = 1
  <span class="org-keyword">return</span> design

<span class="org-keyword">def</span> <span class="org-function-name">evaluate</span>(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial):
  <span class="org-variable-name">x</span>, <span class="org-variable-name">design</span> = simulate(num_samples=num_samples, size=num_mols,
                       log_mu=log_mu, log_phi=log_phi,
                       logodds=logodds, design=<span class="org-constant">None</span>, fold=fold, seed=trial)
  <span class="org-variable-name">onehot</span> = np.ones((num_samples, 1))
  <span class="org-variable-name">keys</span> = [<span class="org-string">'num_samples'</span>, <span class="org-string">'num_mols'</span>, <span class="org-string">'log_mu'</span>, <span class="org-string">'log_phi'</span>, <span class="org-string">'logodds'</span>, <span class="org-string">'trial'</span>,
          <span class="org-string">'fold'</span>, <span class="org-string">'log_mu_hat'</span>, <span class="org-string">'log_phi_hat'</span>, <span class="org-string">'logodds_hat'</span>, <span class="org-string">'mean'</span>, <span class="org-string">'var'</span>]
  <span class="org-variable-name">result</span> = [num_samples, num_mols, log_mu, log_phi, logodds, trial, fold] + [param[0] <span class="org-keyword">for</span> param <span class="org-keyword">in</span> _fit_gene(x, onehot, design)]
  <span class="org-variable-name">result</span> = {k: v <span class="org-keyword">for</span> k, v <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(keys, result)}
  <span class="org-variable-name">eps</span> = .5 / num_mols
  <span class="org-variable-name">log_cpm</span> = (np.log(np.ma.masked_values(x[:,0], 0) + eps) -
             np.log(x[:,1] + 2 * eps) +
             6 * np.log(10)).compressed()
  <span class="org-variable-name">result</span>[<span class="org-string">'mean_log_cpm'</span>] = log_cpm.mean()
  <span class="org-variable-name">result</span>[<span class="org-string">'var_log_cpm'</span>] = log_cpm.var()
  <span class="org-variable-name">d</span>, <span class="org-variable-name">p</span> = diagnostic_test(x[:,0],
                         np.atleast_1d(result[<span class="org-string">'log_mu_hat'</span>]),
                         np.atleast_1d(result[<span class="org-string">'log_phi_hat'</span>]),
                         np.atleast_1d(-result[<span class="org-string">'logodds_hat'</span>]),
                         num_mols,
                         onehot)
  <span class="org-variable-name">result</span>[<span class="org-string">'ks_d'</span>] = d
  <span class="org-variable-name">result</span>[<span class="org-string">'ks_p'</span>] = p
  <span class="org-keyword">return</span> result
</pre>
</div>

<p>
Check the implementation actually worked.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x1</span>, <span class="org-variable-name">design1</span> = simulate(num_samples=1000, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, design=batch_design_matrix(1000, 2), fold=1.1)
<span class="org-variable-name">x2</span>, <span class="org-variable-name">design2</span> = simulate(num_samples=1000, size=1e5, log_mu=-9, log_phi=-6, logodds=-3, seed=0, design=batch_design_matrix(1000, 2), fold=1.1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = np.vstack((x1, x2))
<span class="org-variable-name">design</span> = np.vstack((design1, design2))
<span class="org-variable-name">onehot</span> = np.zeros((2000, 2))
<span class="org-variable-name">onehot</span>[:1000,0] = 1
<span class="org-variable-name">onehot</span>[1000:,1] = 1
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">so.minimize(_nb, np.zeros(6), (x[:,0], x[:,1], onehot, design - design.mean(axis=0)))
</pre>
</div>

<pre class="example">
fun: 3.7693708861600173
hess_inv: array([[ 2.95492914e-01, -2.26820271e-02, -6.88886930e-02,
3.66568258e-02, -1.35008739e-02,  1.33990791e-02],
[-2.26820271e-02,  2.76704513e-01,  4.55879587e-03,
6.39326308e-02, -2.03557128e-02,  2.03675939e-02],
[-6.88886930e-02,  4.55879587e-03,  8.40491928e+00,
3.47009715e-01,  1.14991770e-02, -1.17004131e-02],
[ 3.66568258e-02,  6.39326308e-02,  3.47009715e-01,
1.93344347e+01, -8.18164095e-03,  6.19982324e-03],
[-1.35008739e-02, -2.03557128e-02,  1.14991770e-02,
-8.18164095e-03,  6.45950264e-01,  3.54059802e-01],
[ 1.33990791e-02,  2.03675939e-02, -1.17004131e-02,
6.19982324e-03,  3.54059802e-01,  6.45930363e-01]])
jac: array([ 6.82473183e-06,  8.34465027e-06,  1.07288361e-06,  8.34465027e-07,
3.75509262e-06, -4.08291817e-06])
message: 'Optimization terminated successfully.'
nfev: 280
nit: 30
njev: 35
status: 0
success: True
x: array([-7.79891405, -8.79063513,  2.05714587,  2.56621654,  0.16492975,
-0.16494031])
</pre>

<div class="org-src-container">
<pre class="src src-ipython">so.minimize(zinb, np.zeros(8), (x[:,0], x[:,1], onehot, design - design.mean(axis=0)))
</pre>
</div>

<pre class="example">
fun: 3.1120455141161147
hess_inv: array([[ 3.91509877e+01, -6.82255452e+00,  3.56651927e-03,
-3.60544664e-03,  1.89769061e-03, -3.67396633e-05,
-5.05335375e-03,  2.07017842e-03],
[-6.82255452e+00,  3.63677578e+01, -2.89665006e-03,
1.89766982e-03, -1.15515147e-03,  1.19995032e-04,
4.03751660e-03, -1.28881738e-03],
[ 3.56651927e-03, -2.89665006e-03,  1.00060484e-05,
-4.75805864e-06, -1.52866758e-06, -9.78212511e-07,
-7.93108063e-09, -9.33841063e-06],
[-3.60544664e-03,  1.89766982e-03, -4.75805864e-06,
9.43027846e-06, -1.92946942e-08, -1.76906831e-07,
-3.04179309e-07,  5.35291550e-06],
[ 1.89769061e-03, -1.15515147e-03, -1.52866758e-06,
-1.92946942e-08,  2.96476652e-06,  1.16116940e-06,
-4.91977746e-06,  2.37359794e-06],
[-3.67396633e-05,  1.19995032e-04, -9.78212511e-07,
-1.76906831e-07,  1.16116940e-06,  1.48628846e-06,
-1.36236523e-06,  7.97280863e-07],
[-5.05335375e-03,  4.03751660e-03, -7.93108063e-09,
-3.04179309e-07, -4.91977746e-06, -1.36236523e-06,
1.60495775e-05, -3.42044027e-06],
[ 2.07017842e-03, -1.28881738e-03, -9.33841063e-06,
5.35291550e-06,  2.37359794e-06,  7.97280863e-07,
-3.42044027e-06,  1.96937195e-05]])
jac: array([-2.98023224e-07,  7.15255737e-07,  9.99987125e-04, -3.08364630e-04,
5.21874428e-03,  7.71874189e-03,  1.99797750e-03,  9.57250595e-05])
message: 'Desired error not necessarily achieved due to precision loss.'
nfev: 1942
nit: 80
njev: 193
status: 2
success: False
x: array([-3.07858135, -3.07854889, -7.75387174, -8.74631249, 12.91019024,
13.26839552,  0.04538458, -0.28914969])
</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot)
</pre>
</div>

<pre class="example">
[array([-7.74001987, -8.73200364]),
array([-3.50778747, -3.59795896]),
array([-3.07854886, -3.07859407]),
array([43.50629319, 16.13388663]),
array([100.22044275,  23.26084595])]
</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot, design)
</pre>
</div>

<pre class="example">
[array([-7.74986202, -8.7418084 ]),
array([-6.82644387, -6.09474886]),
array([-3.0785687 , -3.07861418]),
array([43.08019804, 15.97647082]),
array([45.09331255, 16.55197158])]
</pre>

<p>
Check what happens on all zero data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = np.concatenate((np.zeros((1000, 1)), 1e5 * np.ones((1000, 1))), axis=1)
<span class="org-variable-name">onehot</span> = np.ones((1000, 1))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">np.log((onehot * x[:,:1]).<span class="org-builtin">sum</span>(axis=0) / onehot.<span class="org-builtin">sum</span>(axis=0)) - np.log(np.ma.masked_values(onehot, 0) * x[:,1:]).mean(axis=0).compressed()
</pre>
</div>

<pre class="example">
array([-inf])

</pre>

<div class="org-src-container">
<pre class="src src-ipython">so.minimize(_nb, x0=(-np.inf, 0), args=(x[:,0], x[:,1], onehot))
</pre>
</div>

<pre class="example">
fun: 9.999999889225288e-09
hess_inv: array([[1, 0],
[0, 1]])
jac: array([0.00000000e+00, 1.00000003e-08])
message: 'Optimization terminated successfully.'
nfev: 4
nit: 0
njev: 1
status: 0
success: True
x: array([-inf,   0.])
</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot)
</pre>
</div>

<pre class="example">
[array([-inf]), array([-0.]), array([0.]), array([0.]), array([0.])]

</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot, design)
</pre>
</div>

<pre class="example">
[array([-7.75254245, -8.74529525]),
array([-7.38635243, -6.45950176]),
array([-3.07849171, -3.07866706]),
array([42.9648789 , 15.92086032]),
array([44.10874472, 16.3176927 ])]
</pre>

<p>
Check the end-to-end evaluation.
</p>

<div class="org-src-container">
<pre class="src src-ipython">evaluate(num_samples=500, num_mols=1e5, log_mu=-8, log_phi=-6, logodds=-3, fold=<span class="org-constant">None</span>, trial=1)
</pre>
</div>

<pre class="example">
{'fold': None,
'ks_d': 0.02216464461580947,
'ks_p': 9.23381354739375e-22,
'log_mu': -8,
'log_mu_hat': -7.995410361780751,
'log_phi': -6,
'log_phi_hat': -6.032810431842001,
'logodds': -3,
'logodds_hat': -2.9873654919335273,
'mean': 33.700581863533316,
'mean_log_cpm': 5.803550886989237,
'num_mols': 100000.0,
'num_samples': 500,
'trial': 1,
'var': 36.42490436676706,
'var_log_cpm': 0.03387162521847725}
</pre>

<div class="org-src-container">
<pre class="src src-ipython">%timeit evaluate(num_samples=5000, num_mols=1e5, log_mu=-8, log_phi=-6, logodds=-3, fold=1.1, trial=0)
</pre>
</div>

<p>
1.41 s ± 449 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
</p>

<p>
Investigate what happens as the number of confounders increases.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">design</span> = np.random.normal(size=(300, 20))
<span class="org-variable-name">x</span>, <span class="org-variable-name">_</span> = simulate(num_samples=300, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, design=design, fold=1.1)
_fit_gene(x, design)
</pre>
</div>

<pre class="example">
[-8.00649125343069,
-6.410804905890429,
-2.99891159295469,
33.32921072894424,
35.15509338485383]
</pre>

<p>
Run the simulation on 28 CPUs.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org11ef0dc">&lt;&lt;zinb-imports&gt;&gt;
<span class="org-keyword">import</span> multiprocessing <span class="org-keyword">as</span> mp
<span class="org-keyword">import</span> sqlite3
&lt;&lt;np-zinb-impl&gt;&gt;
&lt;&lt;sim-impl&gt;&gt;
&lt;&lt;np-<span class="org-builtin">eval</span>-impl&gt;&gt;
&lt;&lt;zinb-diagnostic&gt;&gt;
<span class="org-variable-name">args</span> = [(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial)
        <span class="org-keyword">for</span> num_samples <span class="org-keyword">in</span> (95,)
        <span class="org-keyword">for</span> num_mols <span class="org-keyword">in</span> (114026,)
        <span class="org-keyword">for</span> log_mu <span class="org-keyword">in</span> np.linspace(-12, -6, 7)
        <span class="org-keyword">for</span> log_phi <span class="org-keyword">in</span> np.linspace(-6, 0, 7)
        <span class="org-keyword">for</span> logodds <span class="org-keyword">in</span> np.linspace(-3, 3, 7)
        <span class="org-keyword">for</span> fold <span class="org-keyword">in</span> np.linspace(1, 1.25, 6)
        <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10)]
<span class="org-keyword">with</span> mp.Pool() <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">result</span> = pd.DataFrame.from_dict(pool.starmap(evaluate, args))
result.to_csv(<span class="org-string">'np-simulation.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=mstephens --mem=8G --job-name sim -n1 -c16 --time=1:00:00 --out np-sim.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/sim.py
</pre>
</div>

<pre class="example">
Submitted batch job 55019435

</pre>
</div>
</div>
</div>

<div id="outline-container-org6a48a5a" class="outline-2">
<h2 id="org6a48a5a">Parameter distributions</h2>
<div class="outline-text-2" id="text-org6a48a5a">
<p>
The simulation reveals the method has undesirable behavior when the
proportion of zeros is too large and mean is too small.
</p>

<p>
Read the estimated parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz'</span>, comment=<span class="org-string">'g'</span>, index_col=0, header=<span class="org-constant">None</span>, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_phi</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz'</span>, comment=<span class="org-string">'g'</span>, index_col=0, header=<span class="org-constant">None</span>, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">logodds</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz'</span>, comment=<span class="org-string">'g'</span>, sep=<span class="org-string">' '</span>, header=<span class="org-constant">None</span>, index_col=0)
</pre>
</div>

<p>
Look at the joint distribution.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">J</span> = (log_mu.agg(np.mean, axis=1).to_frame()
     .merge(log_phi.agg(np.mean, axis=1).to_frame(), left_index=<span class="org-constant">True</span>, right_index=<span class="org-constant">True</span>)
     .rename(columns={<span class="org-string">'0_x'</span>: <span class="org-string">'log_mu'</span>, <span class="org-string">'0_y'</span>: <span class="org-string">'log_phi'</span>})
     .merge(logodds.agg(np.mean, axis=1).to_frame(), left_index=<span class="org-constant">True</span>, right_index=<span class="org-constant">True</span>)
     .rename(columns={0: <span class="org-string">'logodds'</span>}))
J.head()
</pre>
</div>

<pre class="example">
log_mu   log_phi    logodds
0
ENSG00000000003  -9.548541 -2.684409 -16.055537
ENSG00000000419  -9.940811 -3.439683 -14.967152
ENSG00000000457 -12.600461 -2.165913  -7.902089
ENSG00000000460 -11.135371 -3.090537 -10.376486
ENSG00000001036 -11.127939 -2.941718  -9.684571
</pre>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 2)
fig.set_size_inches(6, 6)

ax[0, 0].scatter(J[<span class="org-string">'log_mu'</span>], J[<span class="org-string">'log_phi'</span>], c=<span class="org-string">'k'</span>, s=2, alpha=0.25)
ax[0, 0].set_xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
ax[0, 0].set_ylabel(<span class="org-string">'$\ln(\phi)$'</span>)

ax[1, 0].scatter(J[<span class="org-string">'log_mu'</span>], J[<span class="org-string">'logodds'</span>], c=<span class="org-string">'k'</span>, s=2, alpha=0.25)
ax[1, 0].set_xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
ax[1, 0].set_ylabel(<span class="org-string">'$\mathrm{logit}(\pi)$'</span>)

ax[0, 1].scatter(J[<span class="org-string">'logodds'</span>], J[<span class="org-string">'log_phi'</span>], c=<span class="org-string">'k'</span>, s=2, alpha=0.25)
ax[0, 1].set_xlabel(<span class="org-string">'$\mathrm{logit}(\pi)$'</span>)
ax[0, 1].set_ylabel(<span class="org-string">'$\ln(\phi)$'</span>)

ax[1, 1].axis(<span class="org-string">'off'</span>)

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/joint-distribution.png" alt="joint-distribution.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org71cecf6" class="outline-2">
<h2 id="org71cecf6">Effect of confounding</h2>
<div class="outline-text-2" id="text-org71cecf6">
<p>
Estimate proportion of variance explained by confounders by estimating the
average reduction in heterogeneity (residual variance).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_phi0</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-phi.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_phi1</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">1 - np.exp(log_phi1 - log_phi0).mean().mean()
</pre>
</div>

<pre class="example">
-1.90349049788247

</pre>

<p>
Estimate how much the mean changes due to confounding.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu0</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-mu.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_mu1</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">np.exp(log_mu1 - log_mu0).describe().loc[<span class="org-string">'mean'</span>].describe()
</pre>
</div>

<pre class="example">
count    54.000000
mean      1.008531
std       0.171844
min       0.894419
25%       0.964850
50%       0.987312
75%       1.008456
max       2.213495
Name: mean, dtype: float64
</pre>
</div>
</div>

<div id="outline-container-orgf0b1cab" class="outline-2">
<h2 id="orgf0b1cab">Comparison with sample moment-based estimators</h2>
<div class="outline-text-2" id="text-orgf0b1cab">
</div>
<div id="outline-container-org7ae35b9" class="outline-3">
<h3 id="org7ae35b9">Mean expression</h3>
<div class="outline-text-3" id="text-org7ae35b9">
<p>
Compute pseudobulk relative abundance. <b>Important:</b> keep the \(\infty\) around.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">pooled_log_mu</span> = np.log(umi.groupby(annotations[<span class="org-string">'chip_id'</span>].values, axis=1).agg(np.<span class="org-builtin">sum</span>)) - np.log(annotations.groupby(<span class="org-string">'chip_id'</span>)[<span class="org-string">'mol_hs'</span>].agg(np.<span class="org-builtin">sum</span>))
<span class="org-variable-name">pooled_log_rho</span> = pooled_log_mu - sp.logsumexp(pooled_log_mu, axis=0)
</pre>
</div>

<p>
To first order,
</p>

<p>
\[ E[\ln r] = \ln r \]
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-comment-delimiter"># </span><span class="org-comment">Follow edgeR</span>
<span class="org-variable-name">libsize</span> = annotations[<span class="org-string">'mol_hs'</span>].values
<span class="org-variable-name">eps</span> = .5 * libsize / libsize.mean()
<span class="org-variable-name">log_cpm</span> = (np.log(umi + eps) - np.log(libsize + 2 * eps) + 6 * np.log(10)) / np.log(2)
</pre>
</div>

<p>
CPM is proportional to relative abundance, so normalize.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cpm_log_mu</span> = log_cpm.groupby(annotations[<span class="org-string">'chip_id'</span>].values, axis=1).agg(np.mean)
<span class="org-variable-name">cpm_log_rho</span> = cpm_log_mu - sp.logsumexp(cpm_log_mu, axis=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">nonzero_cpm_log_mu</span> = log_cpm.mask(umi == 0).groupby(annotations[<span class="org-string">'chip_id'</span>].values, axis=1).agg(np.mean).dropna()
<span class="org-variable-name">nonzero_cpm_log_rho</span> = nonzero_cpm_log_mu - sp.logsumexp(nonzero_cpm_log_mu, axis=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zinb_log_mu</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">zinb_logodds</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-comment-delimiter"># </span><span class="org-comment">Important: log(sigmoid(x)) = -softplus(-x)</span>
<span class="org-variable-name">zinb_log_mu</span> -= np.log1p(np.exp(zinb_logodds))
<span class="org-variable-name">zinb_log_rho</span> = zinb_log_mu - sp.logsumexp(zinb_log_mu, axis=0)
</pre>
</div>

<p>
Construct a DataFrame for convenience.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_rho</span> = pd.DataFrame({<span class="org-string">'Pooled'</span>: pooled_log_rho[<span class="org-string">'NA18507'</span>],
                        <span class="org-string">'ZINB'</span>: zinb_log_rho[<span class="org-string">'NA18507'</span>],
                        <span class="org-string">'Nonzero CPM'</span>: nonzero_cpm_log_rho[<span class="org-string">'NA18507'</span>],
                        <span class="org-string">'CPM'</span>: cpm_log_rho[<span class="org-string">'NA18507'</span>]})[[<span class="org-string">'ZINB'</span>, <span class="org-string">'Pooled'</span>, <span class="org-string">'Nonzero CPM'</span>, <span class="org-string">'CPM'</span>]]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">N</span> = log_rho.shape[1]
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(N, N)
fig.set_size_inches(8, 8)
<span class="org-keyword">for</span> y <span class="org-keyword">in</span> <span class="org-builtin">range</span>(N):
  ax[y, 0].set_ylabel(<span class="org-string">'{}'</span>.<span class="org-builtin">format</span>(log_rho.columns[y]))
  <span class="org-keyword">for</span> x <span class="org-keyword">in</span> <span class="org-builtin">range</span>(N):
    <span class="org-keyword">if</span> y &lt; x:
      ax[y, x].set_axis_off()
    <span class="org-keyword">else</span>:
      ax[y, x].scatter(log_rho.iloc[:, x], log_rho.iloc[:, y], c=<span class="org-string">'k'</span>, s=2, alpha=0.1)
      ax[y, x].plot(ax[y, x].get_xlim(), ax[y, x].get_xlim(), c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
      ax[y, x].text(.05, .95, <span class="org-string">'$r$ = {:.2g}'</span>.<span class="org-builtin">format</span>(st.mstats.spearmanr(log_rho.iloc[:, x], log_rho.iloc[:, y]).correlation), transform=ax[y, x].transAxes, verticalalignment=<span class="org-string">'top'</span>)
<span class="org-keyword">for</span> x <span class="org-keyword">in</span> <span class="org-builtin">range</span>(N):
  ax[-1, x].set_xlabel(<span class="org-string">'{}'</span>.<span class="org-builtin">format</span>(log_rho.columns[x]))
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/cpm-estimator-mu.png" alt="cpm-estimator-mu.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orga360af8" class="outline-3">
<h3 id="orga360af8">Expression noise</h3>
<div class="outline-text-3" id="text-orga360af8">
<p>
Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
<span class="org-variable-name">umi</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, index_col=0)
<span class="org-variable-name">zeros_pass</span> = umi.loc[:,keep_samples.values.ravel()].agg(np.<span class="org-builtin">sum</span>, axis=1) &gt; 0
<span class="org-variable-name">umi</span> = umi.loc[zeros_pass,keep_samples.values.ravel()]
</pre>
</div>

<p>
Look at NA18507 (individual with the most cell), and also over all individuals.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">keep_ind</span> = (annotations[<span class="org-string">'chip_id'</span>] == <span class="org-string">'NA18507'</span>).values.ravel()
</pre>
</div>

<p>
Look at Fano vs. mean, following <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3358231/">Munsky et al 2013</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mean</span> = umi.agg(np.mean, axis=1)
<span class="org-variable-name">var</span> = umi.agg(np.var, axis=1)
<span class="org-variable-name">ind_mean</span> = umi.loc[:,keep_ind].agg(np.mean, axis=1)
<span class="org-variable-name">ind_var</span> = umi.loc[:,keep_ind].agg(np.var, axis=1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython" id="orgd9fb17f"><span class="org-keyword">def</span> <span class="org-function-name">plot_fano_vs_mean</span>(mean, var, ax, title, method, ref=<span class="org-constant">True</span>):
  ax.semilogx()
  ax.semilogy()
  ax.scatter(mean, var / mean, c=<span class="org-string">'k'</span>, s=2, zorder=0, alpha=0.25)
  <span class="org-variable-name">lim</span> = [.9 * mean.<span class="org-builtin">min</span>(), 1.1 * mean.<span class="org-builtin">max</span>()]
  <span class="org-variable-name">grid</span> = np.geomspace(lim[0], lim[1], 200)
  <span class="org-keyword">for</span> phi <span class="org-keyword">in</span> np.linspace(.2, 1, 5):
    <span class="org-keyword">if</span> ref:
      ax.plot(grid, 1 + phi * np.array(grid), lw=1, c=colorcet.cm[<span class="org-string">'bmy'</span>](phi), zorder=1, label=<span class="org-string">'{:.2g}'</span>.<span class="org-builtin">format</span>(phi))
  ax.set_xlim(lim)
  ax.set_ylim(1, 300)
  ax.set_xlabel(<span class="org-string">'{} mean'</span>.<span class="org-builtin">format</span>(method))
  ax.set_ylabel(<span class="org-string">'{} Fano factor'</span>.<span class="org-builtin">format</span>(method))
  ax.set_title(title)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(8, 4)
plot_fano_vs_mean(ind_mean[ind_mean &gt; 0], ind_var[ind_mean &gt; 0], ax[0], <span class="org-string">'NA18507'</span>, <span class="org-string">'Sample'</span>)
plot_fano_vs_mean(mean, var, ax[1], <span class="org-string">'Over 53 individuals'</span>, <span class="org-string">'Sample'</span>)
ax[1].legend(title=<span class="org-string">'Overdispersion'</span>, frameon=<span class="org-constant">False</span>, fancybox=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, 0.5))
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/count-fano-vs-count-mean.png" alt="count-fano-vs-count-mean.png">
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org20777e4" class="outline-2">
<h2 id="org20777e4">ZINB estimates of expression noise</h2>
<div class="outline-text-2" id="text-org20777e4">
<p>
Read the estimated parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span> = pd.read_table(<span class="org-string">"/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz"</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_phi</span> = pd.read_table(<span class="org-string">"/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz"</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">logodds</span> = pd.read_table(<span class="org-string">"/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz"</span>, index_col=0, sep=<span class="org-string">' '</span>)
</pre>
</div>

<p>
Read the annotations.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">onehot</span> = recode(annotations, <span class="org-string">'chip_id'</span>)
</pre>
</div>

<p>
Fixing individual \(i\), cell \(j\), gene \(k\), we have:
</p>

<p>
\[ E[r_{ijk}] = (1 - \pi_{ik}) R_{ij} \mu_{ik} \]
</p>

<p>
\[ V[r_{ijk}] = (1 - \pi_{ik})\left(R_{ij} \mu_{ik} + (R_{ij} \mu_{ik})^2
  \phi_{ik}\right) + \pi_{ik} (1 - \pi_{ik}) \mu_{ik}^2 \]
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgb416e2f"><span class="org-variable-name">mean_by_sample</span> = (annotations[<span class="org-string">'mol_hs'</span>].values.reshape(-1, 1) * onehot).dot(np.exp(log_mu - np.log1p(np.exp(logodds))).T)
<span class="org-comment-delimiter"># </span><span class="org-comment">Nonzero component</span>
<span class="org-variable-name">var_by_sample</span> = mean_by_sample + np.square(mean_by_sample) * np.exp(onehot.dot(log_phi.values.T))
<span class="org-variable-name">var_by_sample</span> *= onehot.dot(sp.expit(-logodds.T))
<span class="org-variable-name">var_by_sample</span> += onehot.dot(sp.expit(logodds.T) * np.exp(log_mu.T)) * mean_by_sample
</pre>
</div>

<p>
The index of dispersion for observed data \(r_{ijk}\) at gene \(k\) is:
</p>

<p>
\[ D_k = \frac{V[r_{ijk}]}{E[r_{ijk}]} \]
</p>

<p>
where expectations (variances) are taken over individuals \(i\) and cells
\(j\).
</p>

<p>
Let \(g_{ijk}\) denote the zero-inflated negative binomial density as defined
above. Then, we have:
</p>

<p>
\[ r_{ijk} \sim \sum_{ijk} \frac{1}{N} g_{ijk}(\cdot) \]
</p>

<p>
The mixture density has expectation:
</p>

<p>
\[ \mu_k = \frac{1}{N} \sum E[r_{ijk}] \]
</p>

<p>
and variance (<a href="http://www.springer.com/us/book/9780387329093">Frühwirth-Schnatter 2006</a>):
</p>

<p>
\[ \sigma^2_k = \frac{1}{N} \sum (E[r_{ijk}] - \mu_k)^2 + V[r_{ijk}] \]
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgdc5f3b3"><span class="org-variable-name">mean_by_ind</span> = onehot.T.dot(mean_by_sample) / onehot.T.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
<span class="org-variable-name">var_by_ind</span> = onehot.T.dot(np.square(mean_by_sample - onehot.dot(mean_by_ind)) + var_by_sample) / onehot.T.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
<span class="org-variable-name">overall_mean</span> = mean_by_sample.mean(axis=0)
<span class="org-variable-name">overall_var</span> = (np.square(mean_by_sample - overall_mean.reshape(1, -1)) + var_by_sample).mean(axis=0)
</pre>
</div>

<p>
Find outliers.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">gene_info</span> = (pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz'</span>)
             .set_index(<span class="org-string">'gene'</span>)
             .query(<span class="org-string">'source == "H. sapiens"'</span>)
             .query(<span class="org-string">'chr != "hsX"'</span>)
             .query(<span class="org-string">'chr != "hsY"'</span>)
             .query(<span class="org-string">'chr != "hsMT"'</span>))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">T</span> = pd.DataFrame({<span class="org-string">'mean'</span>: overall_mean, <span class="org-string">'var'</span>: overall_var})
<span class="org-variable-name">T.index</span> = log_mu.index
<span class="org-variable-name">J</span> = T.merge(gene_info, left_index=<span class="org-constant">True</span>, right_index=<span class="org-constant">True</span>).sort_values(<span class="org-string">'mean'</span>, ascending=<span class="org-constant">False</span>)
J[J[<span class="org-string">'var'</span>] / J[<span class="org-string">'mean'</span>] &gt; 150][[<span class="org-string">'mean'</span>, <span class="org-string">'var'</span>, <span class="org-string">'name'</span>]]
</pre>
</div>

<pre class="example">
mean          var      name
gene
ENSG00000110713  37.779102  9160.566673     NUP98
ENSG00000053438   2.454245  1119.916885      NNAT
ENSG00000173401   0.692929   327.169561  GLIPR1L1
ENSG00000213380   0.630846   323.477680      COG8
ENSG00000120690   0.559411   324.580801      ELF1
ENSG00000137502   0.500265   760.557951     RAB30
ENSG00000127080   0.415653   467.467892      IPPK
ENSG00000000457   0.403754   777.369613     SCYL3
ENSG00000185818   0.394969   465.957390     NAT8L
ENSG00000145016   0.391726   772.221129  KIAA0226
ENSG00000099617   0.388581   751.477815     EFNA2
ENSG00000112357   0.353050   746.942494      PEX7
</pre>

<p>
Plot Fano factor vs. mean.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(8, 4)
<span class="org-variable-name">k</span> = <span class="org-builtin">sorted</span>(<span class="org-builtin">set</span>(annotations[<span class="org-string">'chip_id'</span>])).index(<span class="org-string">'NA18507'</span>)
plot_fano_vs_mean(mean_by_ind[k], var_by_ind[k], ax[0], <span class="org-string">'NA18507'</span>, <span class="org-string">'ZINB'</span>)
plot_fano_vs_mean(overall_mean, overall_var, ax[1], <span class="org-string">'Over 53 individuals'</span>, <span class="org-string">'ZINB'</span>)
ax[1].legend(title=<span class="org-string">'Overdispersion'</span>, frameon=<span class="org-constant">False</span>, fancybox=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, 0.5))

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/zinb-fano-vs-zinb-mean.png" alt="zinb-fano-vs-zinb-mean.png">
</p>
</div>

<p>
Compare estimates against each other.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">S</span>, <span class="org-variable-name">T</span> = pd.Series(overall_var / overall_mean, index=log_mu.index).align(var / mean, join=<span class="org-string">'inner'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 4)
plt.semilogx()
plt.semilogy()
<span class="org-variable-name">lim</span> = [.9 * S.<span class="org-builtin">min</span>(), 1.1 * S.<span class="org-builtin">max</span>()]
plt.scatter(S, T, c=<span class="org-string">'k'</span>, s=2, alpha=0.25)
plt.text(.05, .95, <span class="org-string">'r = {:.2g}'</span>.<span class="org-builtin">format</span>(st.spearmanr(S, T).correlation), verticalalignment=<span class="org-string">'top'</span>, transform=plt.gca().transAxes)
plt.plot(lim, lim, c=<span class="org-string">'r'</span>, ls=<span class="org-string">':'</span>, lw=1)
plt.xlim(lim)
plt.ylim(lim)
plt.xlabel(<span class="org-string">'ZINB Fano factor'</span>)
plt.ylabel(<span class="org-string">'Sample Fano factor'</span>)
</pre>
</div>

<pre class="example">
Text(0,0.5,'Sample Fano factor')

</pre>

<div class="figure">
<p><img src="figure/zinb.org/count-fano-vs-zinb-fano.png" alt="count-fano-vs-zinb-fano.png">
</p>
</div>
</div>
</div>
<div id="outline-container-org761b474" class="outline-2">
<h2 id="org761b474">Examples</h2>
<div class="outline-text-2" id="text-org761b474">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-mu.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_phi</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-log-phi.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">logodds</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design1/zi2-logodds.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">umi</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, index_col=0)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">keep_genes</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">umi</span> = umi.loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'svg'</span>])
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(4, 2)
<span class="org-variable-name">gene</span> = <span class="org-string">"ENSG00000243709"</span>
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, (<span class="org-string">'NA18507'</span>, <span class="org-string">'NA19204'</span>)):
  <span class="org-variable-name">x</span> = umi.loc[gene, (annotations[<span class="org-string">'chip_id'</span>] == k).values]
  <span class="org-variable-name">y</span> = annotations[annotations[<span class="org-string">'chip_id'</span>] == k]
  <span class="org-variable-name">grid</span> = np.arange(x.<span class="org-builtin">max</span>())
  a.hist(x, color=<span class="org-string">'.75'</span>, bins=grid)
  <span class="org-variable-name">n</span> = np.exp(-log_phi.loc[gene, k])
  <span class="org-variable-name">p</span> = 1 / (1 + np.outer(y[<span class="org-string">'mol_hs'</span>], np.exp(log_mu.loc[gene, k] + log_phi.loc[gene, k])))
  <span class="org-variable-name">G</span> = st.nbinom(n=n.ravel(), p=p.ravel()).pmf
  <span class="org-variable-name">pmf</span> = np.array([G(x).mean() <span class="org-keyword">for</span> x <span class="org-keyword">in</span> grid])
  <span class="org-variable-name">exp_count</span> = x.shape[0] * pmf * sp.expit(-logodds.loc[gene, k])
  a.plot(0.5 + grid, exp_count, c=<span class="org-string">'b'</span>, lw=1)
  a.arrow(0.5, 0, 0, x.shape[0] * sp.expit(logodds.loc[gene, k]), width=.01, head_width=.5, head_length=2, color=<span class="org-string">'r'</span>)
fig.text(0.5, 0, <span class="org-string">'UMI counts'</span>, ha=<span class="org-string">'center'</span>)
fig.text(0, 0.5, <span class="org-string">'Number of cells'</span>, va=<span class="org-string">'center'</span>, rotation=90)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><object type="image/svg+xml" data="figure/zinb.org/example.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>
</div>
</div>

<div id="outline-container-org17af587" class="outline-2">
<h2 id="org17af587">Evaluating goodness of fit</h2>
<div class="outline-text-2" id="text-org17af587">
<p>
To test whether the model actually fit the data, we can use a simple fact: if
</p>

<p>
\[ r_{ijk} \sim f_{ijk}(\cdot) \]
</p>

<p>
then
</p>

<p>
\[ F_{ijk}(r_{ijk}) \sim \mathrm{Uniform}(0, 1) \]
</p>

<p>
and we can use e.g. the Kolmogorov-Smirnov test to test for departures from
uniformity.
</p>

<p>
We need to account for the fact that the support of \(f\) is discrete, and
therefore \(F\) is discontinuous. This can be done by randomizing (<a href="https://www.jstor.org/stable/1390802">Dunn and
Smyth 1996</a>, <a href="https://arxiv.org/abs/1708.08527">Feng et al. 2017</a>):
</p>

<p>
\[ a_{ijk} = \lim_{x\rightarrow r_{ijk}^-} F_{ijk}(x) \]
</p>

<p>
\[ b_{ijk} = F_{ijk}(r_{ijk}) \]
</p>

<p>
\[ u_{ijk} \sim \mathrm{Uniform}(a_{ijk}, b_{ijk}) \]
</p>

<p>
Implement the diagnostic test.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgf9fa352"><span class="org-keyword">def</span> <span class="org-function-name">rpp</span>(x, log_mu, log_phi, logodds, size, onehot, n_samples=100):
  <span class="org-variable-name">n</span> = onehot.dot(np.exp(-log_phi))
  <span class="org-variable-name">pi0</span> = onehot.dot(sp.expit(-logodds))
  <span class="org-variable-name">p</span> = 1 / (1 + (size * onehot.dot(np.exp(log_mu + log_phi))))

  <span class="org-variable-name">cdf</span> = st.nbinom(n=n, p=p).cdf(x - 1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: this excludes the right endpoint, so we need to special case x =</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">0</span>
  <span class="org-variable-name">cdf</span> = np.where(x &gt; 0, pi0 + (1 - pi0) * cdf, cdf)
  <span class="org-variable-name">pmf</span> = st.nbinom(n=n, p=p).pmf(x)
  <span class="org-variable-name">pmf</span> *= (1 - pi0)
  <span class="org-variable-name">pmf</span>[x == 0] += pi0[x == 0]
  <span class="org-variable-name">rpp</span> = cdf + np.random.uniform(size=(n_samples, x.shape[0])) * pmf
  <span class="org-keyword">return</span> rpp

<span class="org-keyword">def</span> <span class="org-function-name">diagnostic_test</span>(x, log_mu, log_phi, logodds, size, onehot, n_samples=100):
  <span class="org-variable-name">vals</span> = rpp(x, log_mu, log_phi, logodds, size, onehot, n_samples)
  <span class="org-keyword">return</span> st.kstest(vals.ravel(), <span class="org-string">'uniform'</span>)
</pre>
</div>
</div>

<div id="outline-container-orgc5779a4" class="outline-3">
<h3 id="diagnostic-sim"><a id="orgc5779a4"></a>Simulation</h3>
<div class="outline-text-3" id="text-diagnostic-sim">
<p>
Look at randomized residuals for some simulated data. Use the CPU
implementation of ZINB for convenience.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">X</span>, <span class="org-variable-name">design</span> = simulate(num_samples=100, size=1e5, log_mu=-12, log_phi=-6, logodds=-3, seed=1, design=<span class="org-constant">None</span>, fold=<span class="org-constant">None</span>)
<span class="org-variable-name">res</span> = _fit_gene(X, np.ones((X.shape[0], 1)), design)
</pre>
</div>

<p>
Plot the simulated data and estimated density.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n</span> = np.exp(-res[1])
<span class="org-variable-name">pi0</span> = sp.expit(res[2])
<span class="org-variable-name">p</span> = 1 / (1 + (1e5 * np.exp(res[0] + res[1])))

<span class="org-variable-name">grid</span> = np.arange(X[:,0].<span class="org-builtin">max</span>())
<span class="org-variable-name">pmf</span> = st.nbinom(n=n, p=p).pmf(grid)
<span class="org-variable-name">pmf</span> *= (1 - pi0)
<span class="org-variable-name">pmf</span>[0] += pi0

plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.hist(X[:,0], color=<span class="org-string">'k'</span>, bins=grid)
plt.plot(.5 + grid, X.shape[0] * pmf, c=<span class="org-string">'r'</span>, lw=1)
plt.xlabel(<span class="org-string">'UMI count'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'Number of cells'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/simulated-zinb.png" alt="simulated-zinb.png">
</p>
</div>

<p>
Plot the QQ plot using randomized quantiles.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">Z</span> = rpp(X[:,0], res[0], res[1], -res[2], 1e5, np.ones((X.shape[0], 1)), n_samples=50)
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.plot(<span class="org-builtin">sorted</span>(Z.ravel()), np.linspace(0, 1, np.prod(Z.shape)), c=<span class="org-string">'k'</span>)
plt.text(.05, .92, <span class="org-string">'ks = {:.2g}'</span>.<span class="org-builtin">format</span>(st.kstest(Z.ravel(), <span class="org-string">'uniform'</span>)[1]), transform=plt.gca().transAxes)
plt.text(.05, .82, r<span class="org-string">'$\ln\phi = -3$'</span>, transform=plt.gca().transAxes)
plt.text(.05, .72, r<span class="org-string">'$\ln\hat\phi = {:.2g}$'</span>.<span class="org-builtin">format</span>(res[1][0]), transform=plt.gca().transAxes)
plt.xlabel(<span class="org-string">'Theoretical quantiles'</span>)
plt.ylabel(<span class="org-string">'Observed quantiles'</span>)
plt.plot([0, 1], [0, 1], c=<span class="org-string">'r'</span>, ls=<span class="org-string">'--'</span>)
</pre>
</div>

<pre class="example">
[&lt;matplotlib.lines.Line2D at 0x7f8b117b7710&gt;]

</pre>

<div class="figure">
<p><img src="figure/zinb.org/simulated-rqrs.png" alt="simulated-rqrs.png">
</p>
</div>

<p>
Plot the histogram of randomized quantiles.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">Z</span> = rpp(X[:,0], res[0], res[1], -res[2], 1e5, np.ones((X.shape[0], 1)), n_samples=50)
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.hist(Z.ravel(), density=<span class="org-constant">True</span>, bins=25, color=<span class="org-string">'.7'</span>)
plt.axhline(y=1, c=<span class="org-string">'k'</span>, ls=<span class="org-string">':'</span>)
plt.xlabel(<span class="org-string">'Randomized quantile'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
</pre>
</div>

<pre class="example">
Text(0,0.5,'Density')

</pre>

<div class="figure">
<p><img src="figure/zinb.org/simulated-rqrs-hist.png" alt="simulated-rqrs-hist.png">
</p>
</div>

<p>
See how the randomized quantiles work as \(\pi_0\) changes.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">simulate_nb_diagnostic</span>(pi0=<span class="org-constant">None</span>, seed=0):
  np.random.seed(seed)
  <span class="org-variable-name">N</span> = 1000
  <span class="org-variable-name">x</span> = st.nbinom(n=4, p=.5).rvs(N)
  <span class="org-keyword">if</span> pi0 <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">y</span> = np.random.uniform(size=N) &lt; pi0
    <span class="org-variable-name">x</span>[y] = 0
  <span class="org-variable-name">cdf</span> = st.nbinom(n=4, p=.5).cdf(x - 1)
  <span class="org-variable-name">pmf</span> = st.nbinom(n=4, p=.5).pmf(x)
  <span class="org-keyword">if</span> pi0 <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">cdf</span> = np.where(x &gt; 0, pi0 + (1 - pi0) * cdf, cdf)
    <span class="org-variable-name">pmf</span> *= (1 - pi0)
    <span class="org-variable-name">pmf</span>[x == 0] += pi0
  <span class="org-variable-name">rpp</span> = cdf + np.random.uniform(size=N) * pmf
  <span class="org-variable-name">_</span>, <span class="org-variable-name">p1</span> = st.kstest(rpp, <span class="org-string">'uniform'</span>)
  <span class="org-variable-name">nrpp</span> = st.norm().ppf(rpp)
  <span class="org-variable-name">_</span>, <span class="org-variable-name">p2</span> = st.shapiro(nrpp)
  <span class="org-keyword">return</span> pi0, seed, p1, p2

pd.DataFrame([simulate_nb_diagnostic(pi0=pi0, seed=trial) <span class="org-keyword">for</span> pi0 <span class="org-keyword">in</span> (<span class="org-constant">None</span>, 0.01, 0.1) <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10)], columns=[<span class="org-string">'pi0'</span>, <span class="org-string">'trial'</span>, <span class="org-string">'ks'</span>, <span class="org-string">'sw'</span>])
</pre>
</div>

<pre class="example">
pi0  trial        ks        sw
0    NaN      0  0.030844  0.145921
1    NaN      1  0.922857  0.579844
2    NaN      2  0.096322  0.995171
3    NaN      3  0.425145  0.748691
4    NaN      4  0.839172  0.158713
5    NaN      5  0.193963  0.305555
6    NaN      6  0.645417  0.229020
7    NaN      7  0.442194  0.935033
8    NaN      8  0.462132  0.280841
9    NaN      9  0.883011  0.262984
10  0.01      0  0.065004  0.665126
11  0.01      1  0.627508  0.902265
12  0.01      2  0.061377  0.783419
13  0.01      3  0.492274  0.544046
14  0.01      4  0.322213  0.120402
15  0.01      5  0.170058  0.108539
16  0.01      6  0.595968  0.637531
17  0.01      7  0.430416  0.812700
18  0.01      8  0.599287  0.258203
19  0.01      9  0.932962  0.191678
20  0.10      0  0.009072  0.263614
21  0.10      1  0.905894  0.899727
22  0.10      2  0.013516  0.702477
23  0.10      3  0.884857  0.659797
24  0.10      4  0.858585  0.516515
25  0.10      5  0.377873  0.372610
26  0.10      6  0.785309  0.800714
27  0.10      7  0.109332  0.307531
28  0.10      8  0.661486  0.359344
29  0.10      9  0.984754  0.085754
</pre>

<p>
Look at the histogram of p-values for the complete range of simulation
parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">tf_res</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/tf-simulation.txt.gz'</span>, index_col=0)
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.hist(tf_res[<span class="org-string">'ks_p'</span>], color=<span class="org-string">'.7'</span>, bins=10, density=<span class="org-constant">True</span>)
plt.axhline(y=1, c=<span class="org-string">'k'</span>, ls=<span class="org-string">'--'</span>)
plt.xlabel(<span class="org-string">'KS test $p$-value'</span>)
plt.ylabel(<span class="org-string">'Density'</span>)
</pre>
</div>

<pre class="example">
Text(0,0.5,'Density')

</pre>

<div class="figure">
<p><img src="figure/zinb.org/sim-ks.png" alt="sim-ks.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgac166c0" class="outline-3">
<h3 id="double-use"><a id="orgac166c0"></a>Using the data twice</h3>
<div class="outline-text-3" id="text-double-use">
<p>
Investigate what the KS test does when the reference distribution is
estimated from the same data which is going to be tested for departure from
the reference.
</p>

<p>
First simulate the simplest case: 
</p>

<p>
\[ x_i \sim N(0, 1) \]
</p>

<p>
Estimate the MLE \(\hat\mu, \hat{\sigma^2}\), then test for departure from
the maximum likelihood solution.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">trial</span>(seed, N=1000):
  np.random.seed(seed)
  <span class="org-variable-name">x</span> = np.random.normal(size=N)
  <span class="org-variable-name">mu</span> = x.mean()
  <span class="org-variable-name">sigma</span> = x.std()
  <span class="org-variable-name">q</span> = st.norm(loc=mu, scale=sigma).cdf(x)
  <span class="org-keyword">return</span> st.kstest(q, <span class="org-string">'uniform'</span>)
</pre>
</div>

<p>
Plot the histogram of \(p\)-values, varying the sample size \(n\).
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(8, 3)
<span class="org-keyword">for</span> n, a <span class="org-keyword">in</span> <span class="org-builtin">zip</span>([100, 1000, 5000], ax):
  <span class="org-variable-name">S</span> = pd.DataFrame([trial(seed, N=n) <span class="org-keyword">for</span> seed <span class="org-keyword">in</span> <span class="org-builtin">range</span>(100)])
  a.hist(S[<span class="org-string">'pvalue'</span>], color=<span class="org-string">'.7'</span>, density=<span class="org-constant">True</span>, bins=10)
  a.axhline(y=1, c=<span class="org-string">'k'</span>, ls=<span class="org-string">'--'</span>)
  a.set_xlabel(<span class="org-string">'KS test $p$-value'</span>)
  a.set_title(<span class="org-string">'n={}'</span>.<span class="org-builtin">format</span>(n))
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/ks-test-double-use.png" alt="ks-test-double-use.png">
</p>
</div>

<p>
In the continuous case, the diagnostic reflects the fact that the MLE is the
best description of the observed data, and therefore \(p\)-values are
concentrated around 1.
</p>

<p>
Next, simulate a simple discrete case:
</p>

<p>
\[ x_i \sim \mathrm{Poisson}(10) \]
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">pois_trial</span>(seed, N=1000, n_samples=1):
  np.random.seed(seed)
  <span class="org-variable-name">x</span> = np.random.poisson(lam=10, size=N)
  <span class="org-variable-name">lamhat</span> = x.mean()
  <span class="org-variable-name">fhat</span> = st.poisson(mu=lamhat)
  <span class="org-variable-name">u</span> = np.random.uniform(size=(n_samples, N))
  <span class="org-variable-name">q</span> = fhat.cdf(x - 1) + u * fhat.pmf(x)
  <span class="org-keyword">return</span> st.kstest(q.ravel(), <span class="org-string">'uniform'</span>)
</pre>
</div>

<p>
Plot the analogous histogram just varying the sample size \(n\).
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(8, 3)
<span class="org-keyword">for</span> n, a <span class="org-keyword">in</span> <span class="org-builtin">zip</span>([100, 1000, 5000], ax):
  <span class="org-variable-name">S</span> = pd.DataFrame([pois_trial(seed, N=n) <span class="org-keyword">for</span> seed <span class="org-keyword">in</span> <span class="org-builtin">range</span>(100)])
  a.hist(S[<span class="org-string">'pvalue'</span>], color=<span class="org-string">'.7'</span>, density=<span class="org-constant">True</span>, bins=10)
  a.axhline(y=1, c=<span class="org-string">'k'</span>, ls=<span class="org-string">'--'</span>)
  a.set_xlabel(<span class="org-string">'KS test $p$-value'</span>)
  a.set_title(<span class="org-string">'n={}'</span>.<span class="org-builtin">format</span>(n))
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/ks-test-double-use-pois.png" alt="ks-test-double-use-pois.png">
</p>
</div>

<p>
Now fix the sample size \(n=100\) and vary the number of randomized
quantiles drawn per observation \(m\).
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(8, 3)
<span class="org-keyword">for</span> m, a <span class="org-keyword">in</span> <span class="org-builtin">zip</span>([1, 2, 5], ax):
  <span class="org-variable-name">S</span> = pd.DataFrame([pois_trial(seed, N=100, n_samples=m) <span class="org-keyword">for</span> seed <span class="org-keyword">in</span> <span class="org-builtin">range</span>(100)])
  a.hist(S[<span class="org-string">'pvalue'</span>], color=<span class="org-string">'.7'</span>, density=<span class="org-constant">True</span>, bins=10)
  a.axhline(y=1, c=<span class="org-string">'k'</span>, ls=<span class="org-string">'--'</span>)
  a.set_xlabel(<span class="org-string">'KS test $p$-value'</span>)
  a.set_title(<span class="org-string">'m={}'</span>.<span class="org-builtin">format</span>(m))
ax[0].set_ylabel(<span class="org-string">'Density'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/ks-test-double-use-pois-m.png" alt="ks-test-double-use-pois-m.png">
</p>
</div>

<p>
The results suggest that the diagnostic is only valid for \(m = 1\), because
for \(m > 1\) the randomized quantiles are no longer iid. For the case where
\(m = 1\), the skew in the \(p\)-values suggests the test is conservative.
</p>
</div>
</div>

<div id="outline-container-orgdd93ccf" class="outline-3">
<h3 id="orgdd93ccf">Real data</h3>
<div class="outline-text-3" id="text-orgdd93ccf">
<p>
Run the diagnostic on the estimated parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
<span class="org-keyword">import</span> multiprocessing <span class="org-keyword">as</span> mp
<span class="org-keyword">import</span> scqtl
<span class="org-keyword">import</span> functools
&lt;&lt;recode-impl&gt;&gt;

<span class="org-keyword">def</span> <span class="org-function-name">shard</span>(counts, keep_samples, log_mu):
  <span class="org-keyword">for</span> chunk <span class="org-keyword">in</span> pd.read_table(counts, index_col=0, chunksize=100):
    <span class="org-variable-name">chunk</span> = chunk.loc[:,keep_samples.values.ravel()].<span class="org-builtin">filter</span>(items=log_mu.index, axis=<span class="org-string">'index'</span>)
    <span class="org-keyword">if</span> <span class="org-keyword">not</span> chunk.empty:
      <span class="org-keyword">yield</span> chunk

<span class="org-keyword">def</span> <span class="org-function-name">process_chunk</span>(chunk, log_mu, log_phi, logodds, size, onehot, n_samples):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> k, x <span class="org-keyword">in</span> chunk.iterrows():
    <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(onehot.shape[1]):
      <span class="org-variable-name">idx</span> = onehot[:,j].astype(<span class="org-builtin">bool</span>)
      <span class="org-variable-name">col</span> = log_mu.columns[j]
      <span class="org-variable-name">d</span>, <span class="org-variable-name">p</span> = scqtl.diagnostic.diagnostic_test(
        x.loc[idx].values.reshape(-1, 1),
        log_mu.loc[k, col],
        log_phi.loc[k,col],
        -logodds.loc[k,col],
        size[idx].values.reshape(-1, 1),
        np.ones((<span class="org-builtin">int</span>(idx.<span class="org-builtin">sum</span>()), 1)),
        n_samples=n_samples)
      result.append((k, col, d, p))
  <span class="org-keyword">return</span> pd.DataFrame(result)

<span class="org-keyword">with</span> mp.Pool() <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">log_mu</span> = pd.read_table(<span class="org-string">"/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-mu.txt.gz"</span>, index_col=0, sep=<span class="org-string">' '</span>)
  <span class="org-variable-name">log_phi</span> = pd.read_table(<span class="org-string">"/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-log-phi.txt.gz"</span>, index_col=0, sep=<span class="org-string">' '</span>)
  <span class="org-variable-name">logodds</span> = pd.read_table(<span class="org-string">"/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/zi2-logodds.txt.gz"</span>, index_col=0, sep=<span class="org-string">' '</span>)

  <span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
  <span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
  <span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]

  <span class="org-variable-name">onehot</span> = recode(annotations, <span class="org-string">'chip_id'</span>)

  <span class="org-variable-name">process_chunk_</span> = functools.partial(
    process_chunk,
    log_mu=log_mu,
    log_phi=log_phi,
    logodds=logodds,
    size=annotations[<span class="org-string">'mol_hs'</span>],
    onehot=onehot,
    n_samples=1)

  <span class="org-variable-name">result</span> = pd.concat(pool.<span class="org-builtin">map</span>(
    process_chunk_,
    shard(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, keep_samples, log_mu),
  ))
  result.set_index(0).to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/diagnostic.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --job-name=<span class="org-string">"tf-diagnostic"</span> --mem=8G -n1 -c28 --exclusive --time=10:00
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-diagnostic.py
</pre>
</div>

<pre class="example">
Submitted batch job 55767376

</pre>

<p>
Look at the histogram of KS-test p-values across all genes.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">diagnostic_results</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/design0/diagnostic.txt.gz'</span>, index_col=0)
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.hist(diagnostic_results[<span class="org-string">'3'</span>], density=<span class="org-constant">True</span>, color=<span class="org-string">'.7'</span>, bins=10)
plt.axhline(y=1, c=<span class="org-string">'k'</span>, ls=<span class="org-string">'--'</span>)
plt.xlabel(<span class="org-string">'KS test $p$-value'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'Density'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/design-0-ks.png" alt="design-0-ks.png">
</p>
</div>

<p>
Find genes for which we can reject the null (after Bonferroni correction).
</p>

<div class="org-src-container">
<pre class="src src-ipython">diagnostic_results[diagnostic_results[<span class="org-string">'3'</span>] &lt; .05 / diagnostic_results.shape[0]].shape[0]
</pre>
</div>

<pre class="example">
60

</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2018-12-17 Mon 11:34</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
