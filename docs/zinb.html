<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2018-01-22 Mon 16:07 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Mean/dispersion estimation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Mean/dispersion estimation</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org6af1f01">Introduction</a></li>
<li><a href="#orgad32fb3">Model specification</a></li>
<li><a href="#orgc967b6d">Quality control</a></li>
<li><a href="#orgde19fe3">Read the data</a></li>
<li><a href="#org978d4fc">Serial algorithm</a></li>
<li><a href="#org21f6e33">Parallel algorithm</a></li>
<li><a href="#org7363c08">Visualize the fitted models</a></li>
<li><a href="#org9ee0e04">Compare NB to ZINB</a></li>
</ul>
</div>
</div>

<div id="outline-container-org6af1f01" class="outline-2">
<h2 id="org6af1f01">Introduction</h2>
<div class="outline-text-2" id="text-org6af1f01">
<p>
The simplest approach to call mean/variance QTLs is to estimate a mean and a
dispersion for each individual, treat them as continuous phenotypes, and plug
into standard QTL mapping software.
</p>

<p>
As a first pass, perform stringent QC to avoid the sparsest genes and simply
find maximum likelihood estimates of a negative binomial model for the mean,
and dispersion per individual per gene.
</p>
</div>
</div>

<div id="outline-container-orgad32fb3" class="outline-2">
<h2 id="orgad32fb3">Model specification</h2>
<div class="outline-text-2" id="text-orgad32fb3">
<p>
Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
\(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell. As a
first pass, define \(R_{ij} = \sum_k r_{ijk}\).
</p>

<p>
Following Hilbe 2012, we derive the negative binomial as a Poisson-Gamma
mixture:
</p>

<p>
\[ r_{ijk} \sim \text{Pois}(R_{ij} \mu_{ik} u_{ijk}) \]
</p>

<p>
\[ u_{ijk} \sim \text{Gamma}(\phi_{ik}^{-1}, \phi_{ik}^{-1}) \]
</p>

<p>
Here, \(\mu_{ik}u_{ijk}\) denotes relative expression
(<a href="https://arxiv.org/abs/1104.3889">Pachter 2011</a>). Marginalizing out \(u\)
yields the negative binomial distribution, with log likelihood:
</p>

<p>
\[ \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}) = r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]
</p>

<p>
We have multiple data points (30-200 cells) per mean/dispersion parameter, so
simply minimizing the negative log likelihood should give reasonable
estimates.
</p>

<p>
We can additionally account for zero-inflation, by letting \(\pi_{ijk}\)
denote the probability of a "technical zero" (i.e., not arising from the
negative-binomial). Following prior work, we introduce a dropout parameter
for each cell (indexed by \(i\) and \(j\)) and each gene (<a href="https://arxiv.org/abs/1709.02082">Lopez et al 2017</a>,
<a href="https://www.nature.com/articles/s41467-017-02554-5">Risso et al 2017</a>).
</p>

<p>
Then, the log-likelihood of the data is:
</p>

<p>
\[ \ln p(r_{ijk} \mid r_{ijk} = 0, \cdot) = -\ln(\pi_{ijk} + (1 - \pi_{ijk})
  \exp(\ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}))) \]
</p>

<p>
\[ \ln p(r_{ijk} \mid r_{ijk} > 0, \cdot) = \ln(1 - \pi_{ijk}) + \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik})) \]
</p>
</div>
</div>

<div id="outline-container-orgc967b6d" class="outline-2">
<h2 id="orgc967b6d">Quality control</h2>
<div class="outline-text-2" id="text-orgc967b6d">
<p>
Filter out cells on percent spike-in and gene detection rate, and filter out
genes on individual detection rate. For now, use conservative filters:
</p>

<ul class="org-ul">
<li>keep cells with % spike-in &lt; 50%</li>
<li>keep cells with detected genes &gt; 4000</li>
<li>keep genes detected in &gt;70% of cells</li>
</ul>

<div class="org-src-container">
<pre class="src src-ipython" id="orgdfffb52"><span class="org-variable-name">keep_cells</span> = functools.<span class="org-builtin">reduce</span>(
  np.logical_and,
  [
    annotations[<span class="org-string">'reads_ercc'</span>] / annotations.<span class="org-builtin">filter</span>(like=<span class="org-string">'reads_'</span>, axis=1).agg(np.<span class="org-builtin">sum</span>, axis=1) &lt; 0.5,
    annotations[<span class="org-string">'detect_hs'</span>] &gt; 4000,
    annotations[<span class="org-string">'chip_id'</span>] != <span class="org-string">'NA19092'</span>,
  ]).values
<span class="org-variable-name">keep_genes</span> = functools.<span class="org-builtin">reduce</span>(
  np.logical_and,
  [
    umi.<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: x &gt; 0).agg(np.mean, axis=1).<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: x &gt; .7),
  ]).values
<span class="org-variable-name">umi_qc</span> = umi.loc[keep_genes, keep_cells]
<span class="org-variable-name">annotations_qc</span> = annotations.loc[keep_cells]
umi_qc.shape
</pre>
</div>

<pre class="example">
(4148, 3100)

</pre>

<div class="org-src-container">
<pre class="src src-ipython" id="org5400cb3"><span class="org-variable-name">individuals</span> = <span class="org-builtin">sorted</span>(annotations_qc[<span class="org-string">'chip_id'</span>].unique())
<span class="org-variable-name">onehot</span> = np.zeros((umi_qc.shape[1], <span class="org-builtin">len</span>(individuals)), dtype=np.float32)
onehot[np.arange(onehot.shape[0]),annotations_qc[<span class="org-string">'chip_id'</span>].<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: individuals.index(x))] = 1
<span class="org-variable-name">onehot</span> = pd.DataFrame(onehot, columns=individuals, index=umi_qc.columns)
onehot.shape
</pre>
</div>

<pre class="example">
(3100, 32)

</pre>

<p>
Check that one-hot encoding is OK:
</p>

<div class="org-src-container">
<pre class="src src-ipython">(umi_qc.loc[<span class="org-string">'ENSG00000000003'</span>, (annotations_qc[<span class="org-string">'chip_id'</span>] == <span class="org-string">'NA18489'</span>).values] == 
 umi_qc.loc[<span class="org-string">'ENSG00000000003'</span>, onehot.dot(np.eye(onehot.shape[1])[0]).astype(<span class="org-builtin">bool</span>)]).<span class="org-builtin">all</span>()
</pre>
</div>

<pre class="example">
True

</pre>

<div class="org-src-container">
<pre class="src src-ipython">umi_qc.to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/umi-qc.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">onehot.to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/onehot-qc.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">annotations_qc.to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/annotations-qc.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgde19fe3" class="outline-2">
<h2 id="orgde19fe3">Read the data</h2>
<div class="outline-text-2" id="text-orgde19fe3">
<div class="org-src-container">
<pre class="src src-ipython" id="org8f219ad"><span class="org-variable-name">umi</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/umi-qc.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">onehot</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/onehot-qc.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/annotations-qc.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>
</div>
</div>

<div id="outline-container-org978d4fc" class="outline-2">
<h2 id="org978d4fc">Serial algorithm</h2>
<div class="outline-text-2" id="text-org978d4fc">
<p>
Optimize each pair of mean/dispersion parameters by sequentially considering
each subset of the data (set of cells per gene per individual). Use
<code>multiprocessing</code> to parallelize this over chunks of genes.
</p>

<p>
Optimize the negative log-likelihood using BFGS.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">nll</span>(theta, x, size):
  <span class="org-variable-name">mean</span>, <span class="org-variable-name">inv_disp</span> = np.exp(theta)
  <span class="org-variable-name">mean</span> *= size
  <span class="org-keyword">assert</span> mean.shape == x.shape
  <span class="org-keyword">return</span> -(x * np.log(mean / inv_disp + 1e-8) -
           x * np.log(1 + mean / inv_disp + 1e-8) -
           inv_disp * np.log(1 + mean / inv_disp + 1e-8) +
           scipy.special.gammaln(x + inv_disp) -
           scipy.special.gammaln(inv_disp) -
           scipy.special.gammaln(x + 1)).<span class="org-builtin">sum</span>()

<span class="org-keyword">def</span> <span class="org-function-name">fit_scipy</span>(x, size):
  <span class="org-variable-name">res</span> = scipy.optimize.minimize(fun=nll, x0=[0, 0], args=(x, size), tol=1e-4)
  <span class="org-keyword">if</span> res.success:
    <span class="org-keyword">return</span> <span class="org-builtin">tuple</span>(res.x)
  <span class="org-keyword">else</span>:
    <span class="org-keyword">return</span> (<span class="org-builtin">float</span>(<span class="org-string">'nan'</span>), <span class="org-builtin">float</span>(<span class="org-string">'nan'</span>))

<span class="org-keyword">def</span> <span class="org-function-name">process_chunk</span>(x, size):
  <span class="org-variable-name">res</span> = pd.DataFrame([(gene, k) + fit_scipy(row.loc[g].values, size.loc[g].values)
                       <span class="org-keyword">for</span> gene, row <span class="org-keyword">in</span> x.iterrows()
                       <span class="org-keyword">for</span> k, g <span class="org-keyword">in</span> row.groupby(annotations[<span class="org-string">'chip_id'</span>].values).groups.items()])
  <span class="org-variable-name">res.columns</span> = [<span class="org-string">'gene'</span>, <span class="org-string">'individual'</span>, <span class="org-string">'mean'</span>, <span class="org-string">'dispersion'</span>]
  res.set_index(<span class="org-string">'gene'</span>, inplace=<span class="org-constant">True</span>)
  <span class="org-keyword">return</span> res
</pre>
</div>

<p>
Output record-like data for each parameter to make reassembling the result
easier.
</p>

<div class="org-src-container">
<pre class="src src-ipython">process_chunk(umi.iloc[:5], size).head()
</pre>
</div>

<pre class="example">
individual      mean  dispersion
gene
ENSG00000000003    NA18519 -8.223728    2.564775
ENSG00000000003    NA18862 -8.406216    2.263670
ENSG00000000003    NA19093 -8.339645    2.393590
ENSG00000000003    NA19128 -8.479335    2.293822
ENSG00000000003    NA18852 -8.609513    2.454818
</pre>

<div class="org-src-container">
<pre class="src src-ipython">Out[90].pivot(columns=<span class="org-string">'individual'</span>, values=<span class="org-string">'mean'</span>) 
</pre>
</div>

<pre class="example">
individual        NA18519   NA18852   NA18862   NA19093   NA19128
gene
ENSG00000000003 -8.223728 -8.609513 -8.406216 -8.339645 -8.479335
</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">f</span> = functools.partial(process_chunk, size=umi.agg(np.<span class="org-builtin">sum</span>))
<span class="org-keyword">with</span> mp.Pool() <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">result</span> = pd.concat(pool.<span class="org-builtin">map</span>(f, np.array_split(umi, 100)))

<span class="org-variable-name">log_mean</span> = result.pivot(columns=<span class="org-string">'individual'</span>, values=<span class="org-string">'mean'</span>)
<span class="org-variable-name">log_disp</span> = result.pivot(columns=<span class="org-string">'individual'</span>, values=<span class="org-string">'dispersion'</span>) 

log_mean.to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
log_disp.to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/disp.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org21f6e33" class="outline-2">
<h2 id="org21f6e33">Parallel algorithm</h2>
<div class="outline-text-2" id="text-org21f6e33">
<p>
We optimize all of the parameters together, using one-hot encoding to map
parameters to data points. This makes inference more amenable to running on
the GPU. Although this is slower for the NB estimation problem, it will be
faster for the ZINB estimation problem when parameters are shared between
genes and we need to operate on the entire count matrix.
</p>

<p>
Use tensorflow to automatically differentiate the negative log likelihood and
perform gradient descent.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org8a5aab3"><span class="org-keyword">def</span> <span class="org-function-name">sigmoid</span>(x):
  <span class="org-doc">"""Numerically safe sigmoid"""</span>
  <span class="org-keyword">return</span> tf.clip_by_value(tf.sigmoid(x), -13, 13)

<span class="org-keyword">def</span> <span class="org-function-name">log</span>(x):
  <span class="org-doc">"""Numerically safe log"""</span>
  <span class="org-keyword">return</span> tf.log(x + 1e-8)

<span class="org-keyword">def</span> <span class="org-function-name">nb_llik</span>(x, mean, inv_disp):
  <span class="org-doc">"""Log likelihood of x distributed as NB</span>

<span class="org-doc">  See Hilbe 2012, eq. 8.10</span>

<span class="org-doc">  mean - mean (&gt; 0)</span>
<span class="org-doc">  inv_disp - inverse dispersion (&gt; 0)</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">return</span> (x * log(mean / inv_disp) -
          x * log(1 + mean / inv_disp) -
          inv_disp * log(1 + mean / inv_disp) +
          tf.lgamma(x + inv_disp) -
          tf.lgamma(inv_disp) -
          tf.lgamma(x + 1))

<span class="org-keyword">def</span> <span class="org-function-name">zinb_llik</span>(x, mean, inv_disp, logodds, eps=1e-8):
  <span class="org-doc">"""Log likelihood of x distributed as ZINB</span>

<span class="org-doc">  See Hilbe 2012, eq. 11.12, 11.13</span>

<span class="org-doc">  mean - mean (&gt; 0)</span>
<span class="org-doc">  inv_disp - inverse dispersion (&gt; 0)</span>
<span class="org-doc">  logodds - dropout log odds</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">case_zero</span> = -log(sigmoid(-logodds) + sigmoid(logodds) * tf.exp(nb_llik(x, mean, inv_disp)))
  <span class="org-variable-name">case_non_zero</span> = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
  <span class="org-keyword">return</span> tf.where(tf.less(x, 1e-8), case_zero, case_non_zero)

<span class="org-keyword">def</span> <span class="org-function-name">fit</span>(umi, onehot, size_factor, zero_inflation=<span class="org-constant">False</span>, learning_rate=1e-2, max_epochs=1000):
  <span class="org-doc">"""Return estimated log mean and log dispersion. </span>

<span class="org-doc">  If fitting a zero-inflated model, additionally return dropout log odds.</span>

<span class="org-doc">  umi - count matrix (n x p; float32)</span>
<span class="org-doc">  onehot - mapping of individuals to cells (m x n; float32)</span>
<span class="org-doc">  size_factor - size factor vector (n x 1; float32)</span>

<span class="org-doc">  Returns:</span>

<span class="org-doc">  log_mean - log mean parameter (m x p)</span>
<span class="org-doc">  log_disp - log dispersion parameter (m x p)</span>
<span class="org-doc">  dropout - if zero_inflation, dropout log odds (n x p)</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">n</span>, <span class="org-variable-name">p</span> = umi.shape
  <span class="org-variable-name">_</span>, <span class="org-variable-name">m</span> = onehot.shape

  <span class="org-variable-name">graph</span> = tf.Graph()
  <span class="org-keyword">with</span> graph.as_default(), graph.device(<span class="org-string">'/gpu:*'</span>):
    <span class="org-variable-name">size_factor</span> = tf.Variable(size_factor, trainable=<span class="org-constant">False</span>)
    <span class="org-variable-name">umi</span> = tf.Variable(umi, trainable=<span class="org-constant">False</span>)
    <span class="org-variable-name">onehot</span> = tf.Variable(onehot, trainable=<span class="org-constant">False</span>)

    <span class="org-variable-name">mean</span> = tf.exp(tf.Variable(tf.zeros([m, p])))
    <span class="org-variable-name">inv_disp</span> = tf.exp(tf.Variable(tf.zeros([m, p])))

    <span class="org-keyword">if</span> zero_inflation:
      <span class="org-variable-name">dropout</span> = tf.Variable(tf.zeros([n, p]))
      <span class="org-variable-name">llik</span> = tf.reduce_mean(
        zinb_llik(umi, size_factor * tf.matmul(onehot, mean),
                  tf.matmul(onehot, inv_disp), dropout))
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">llik</span> = tf.reduce_sum(
        nb_llik(umi, size_factor * tf.matmul(onehot, mean),
                tf.matmul(onehot, inv_disp)))

    <span class="org-keyword">with</span> graph.device(<span class="org-string">'/cpu:0'</span>):
      <span class="org-variable-name">check_op</span> = tf.assert_non_positive(llik)

    <span class="org-keyword">with</span> tf.control_dependencies([check_op]):
      <span class="org-variable-name">train</span> = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-llik)

    <span class="org-variable-name">opt</span> = [tf.log(mean), -tf.log(inv_disp)]
    <span class="org-keyword">if</span> zero_inflation:
      opt.append(dropout)
    <span class="org-variable-name">curr</span> = <span class="org-builtin">float</span>(<span class="org-string">'-inf'</span>)
    <span class="org-keyword">with</span> tf.Session() <span class="org-keyword">as</span> sess:
      sess.run(tf.global_variables_initializer())
      <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_epochs):
        <span class="org-variable-name">_</span>, <span class="org-variable-name">update</span> = sess.run([train, llik])
        <span class="org-keyword">if</span> <span class="org-keyword">not</span> np.isfinite(update):
          <span class="org-keyword">raise</span> tf.train.NanLossDuringTrainingError
        <span class="org-keyword">if</span> <span class="org-keyword">not</span> i % 100:
          <span class="org-keyword">print</span>(i, update)
      <span class="org-keyword">return</span> sess.run(opt)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mean</span>, <span class="org-variable-name">dispersion</span> = fit(
  umi=umi.values.T.astype(np.float32),
  onehot=onehot.values.astype(np.float32),
  size_factor=umi.agg(np.<span class="org-builtin">sum</span>).astype(np.float32).values.reshape(-1, 1),
  learning_rate=1e-2,
  max_epochs=8000)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mean2.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mean</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">mean2</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mean2.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<p>
Check how close the estimated parameters are.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(np.isclose(mean.values, mean2.values)).<span class="org-builtin">all</span>()
</pre>
</div>

<pre class="example">
False

</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mask</span> = np.where(np.logical_and(np.isfinite(mean.values), ~np.isclose(mean.values, mean2.values)))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series(<span class="org-builtin">abs</span>(mean.values[mask] - mean2.values[mask])).describe()
</pre>
</div>

<pre class="example">
count    5685.000000
mean        0.000546
std         0.000889
min         0.000054
25%         0.000165
50%         0.000302
75%         0.000623
max         0.030600
dtype: float64
</pre>
</div>
</div>

<div id="outline-container-org7363c08" class="outline-2">
<h2 id="org7363c08">Visualize the fitted models</h2>
<div class="outline-text-2" id="text-org7363c08">
<p>
The challenge in visualizing the fitted distributions is that the
observations \(r_{ijk}\) are not drawn iid. from a distribution
\(g_{ik}(\cdot)\). 
</p>

<p>
Instead, we have \(r_{ijk} \sim g_{ijk}(\cdot)\), and we have used maximum
likelihood to estimate distributions \(\hat{g}_{ijk}\).
</p>

<p>
We can use an idea from <code>ashr</code>: Let \(\hat{G}_{ijk}\) denote the CDF of
\(\hat{g}_{ijk}\). Then, the distribution of values
\(\hat{G}_{ijk}(r_{ijk})\) should be uniform.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">estimated_cdf</span>(x, mean, disp, size, onehot):
  <span class="org-variable-name">n</span> = onehot.dot(np.exp(-disp.values.T) + 1e-8)
  <span class="org-variable-name">p</span> = 1 / (1 + size.to_frame().values * onehot.dot(np.exp(mean.values + disp.values).T))
  <span class="org-keyword">assert</span> (n.values &gt; 0).<span class="org-builtin">all</span>()
  <span class="org-keyword">assert</span> (p.values &gt;= 0).<span class="org-builtin">all</span>()
  <span class="org-keyword">assert</span> (p.values &lt;= 1).<span class="org-builtin">all</span>()
  <span class="org-keyword">return</span> st.nbinom(n=n, p=p).cdf(x)

<span class="org-keyword">def</span> <span class="org-function-name">diagnostic</span>(umi, mean, disp, size, onehot):
  <span class="org-variable-name">q</span> = estimated_cdf(umi.values.T, mean, disp, size, onehot)
  plt.clf()
  plt.scatter(x=np.linspace(0, 1, q.shape[0]),
              y=<span class="org-builtin">sorted</span>(q.ravel()),
              s=0.5)
  plt.plot([[0, 0], [1, 1]], c=<span class="org-string">'black'</span>)
  plt.title(umi.index[0])
  plt.xlabel(<span class="org-string">'Expected quantile'</span>)
  plt.ylabel(<span class="org-string">'Estimated quantile'</span>)
</pre>
</div>

<p>
Look at a particular gene:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mean</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mean2.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">disp</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">size</span> = umi.agg(np.<span class="org-builtin">sum</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">diagnostic(umi.iloc[:1], mean.iloc[:1], disp.iloc[:1], size, onehot)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/nb-quantiles.png" alt="nb-quantiles.png">
</p>
</div>

<p>
The estimated quantiles are higher than the expected quantiles, suggesting
that the estimated distributions have too much density at lower values. One
explanation is that even for this stringent subset of genes, means are biased
downwards and dispersions are biased upwards due to zero-inflation. If this
were the case, then we should expect to find some genes which depart even
more from uniform quantiles.
</p>

<p>
Look at the quantiles of the QQ plots over all genes:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">diagnostic_quantiles</span>(umi, mean, disp, size, onehot, quantiles=<span class="org-constant">None</span>):
  <span class="org-keyword">if</span> quantiles <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">quantiles</span> = np.array([.1, .25, .5, .75, .99])
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">quantiles</span> = np.array(quantiles)
    <span class="org-keyword">assert</span> (0 &lt;= quantiles &lt;= 1).<span class="org-builtin">all</span>()
  <span class="org-variable-name">cdf</span> = np.sort(estimated_cdf(umi.values.T, mean, disp, size, onehot).T)
  <span class="org-variable-name">cdf_quantiles</span> = np.percentile(cdf, 100 * quantiles, interpolation=<span class="org-string">'higher'</span>, axis=0)

  plt.clf()
  <span class="org-keyword">for</span> q, row <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(quantiles, cdf_quantiles):
    plt.scatter(x=np.linspace(0, 1, row.shape[0]), y=row, c=colorcet.cm[<span class="org-string">'kbc'</span>](q), s=.5, label=q)
  plt.legend()
  plt.xlabel(<span class="org-string">'Expected quantile'</span>)
  plt.ylabel(<span class="org-string">'Estimated quantile'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">diagnostic_quantiles(umi, mean, disp, size, onehot)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/nb-quantiles-dist.png" alt="nb-quantiles-dist.png">
</p>
</div>

<p>
We can also compare the histogram of the observed data to the average density
\(\bar{g}\):
</p>

<p>
\[ \bar{g}(x) = \frac{1}{N} \sum_{i,j,k} \hat{g}_{ijk}(x) \]
</p>

<p>
Following <code>locfdr</code>, we plot the expected number of draws from \(\bar{g}\),
assuming the number of draws in the interval \([a, b]\) is
\(\mathrm{Bin}(N, \bar{G}(b) - \bar{G}(a))\), where \(N\) is the total number
of observations and \(\bar{G}\) is the average CDF:
</p>

<p>
\[ \bar{G}(x) = \frac{1}{N} \sum_{i,j,k} \hat{G}_{ijk}(x) \]
</p>

<p>
The data distribution has an extremely long tail, so truncate it to make the
majority of the fit easier to see:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = (pd.Series([50, 75, 90, 95, 99])
         .<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: np.percentile(umi.values.ravel(), x))
         .to_frame()
         .rename_axis(<span class="org-string">'Percentile'</span>, axis=<span class="org-string">'index'</span>))
<span class="org-variable-name">query.columns</span> = [<span class="org-string">'UMI'</span>]
query
</pre>
</div>

<pre class="example">
UMI
Percentile
50           4.0
75           9.0
90          19.0
95          32.0
99          98.0
</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">expected_counts</span>(umi, mean, disp, size, onehot, grid):
  <span class="org-variable-name">data</span> = umi.values.ravel()
  <span class="org-variable-name">average_cdf</span> = [np.mean(estimated_cdf(g, mean, disp, size, onehot)) <span class="org-keyword">for</span> g <span class="org-keyword">in</span> grid]
  <span class="org-keyword">return</span> data.shape[0] * np.diff(average_cdf)

<span class="org-keyword">def</span> <span class="org-function-name">plot_fit</span>(umi, expected_counts, grid):
  <span class="org-variable-name">data</span> = umi.values.ravel()
  plt.clf()
  plt.hist(data[data &lt;= grid.<span class="org-builtin">max</span>()], bins=grid, color=<span class="org-string">'k'</span>)
  plt.plot(grid[1:] - 0.5, expected_counts, c=<span class="org-string">'r'</span>)
  plt.xticks(grid)
  plt.xlabel(<span class="org-string">'UMI count'</span>)
  plt.ylabel(<span class="org-string">'Frequency'</span>)
</pre>
</div>

<p>
The model appears to be zero-biased as expected:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.arange(20)
<span class="org-variable-name">exp_counts</span> = expected_counts(umi, mean, disp, size, onehot, grid)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plot_fit(umi, exp_counts, grid)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/nb-hist.png" alt="nb-hist.png">
</p>
</div>

<p>
We can also visualize the quantiles of \(\bar{g}\):
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">predicted_counts</span>(umi, mean, disp, size, onehot, grid, quantiles=<span class="org-constant">None</span>):
  <span class="org-keyword">if</span> quantiles <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">quantiles</span> = [.1, .25, .5, .75, .99]
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">quantiles</span> = np.atleast_1d(np.array(quantiles))
    <span class="org-keyword">assert</span> (0 &lt;= quantiles).<span class="org-builtin">all</span>()
    <span class="org-keyword">assert</span> (quantiles &lt;= 1).<span class="org-builtin">all</span>()
  <span class="org-variable-name">data</span> = umi.values.ravel()
  <span class="org-variable-name">cdf</span> = np.array([np.percentile(estimated_cdf(g, mean, disp, size, onehot), 100 * quantiles) <span class="org-keyword">for</span> g <span class="org-keyword">in</span> grid]).T
  <span class="org-keyword">return</span> data.shape[0] * np.diff(cdf)

<span class="org-keyword">def</span> <span class="org-function-name">plot_fit_quantiles</span>(umi, predicted_counts, quantiles, grid):
  <span class="org-variable-name">data</span> = umi.values.ravel()
  plt.clf()
  plt.hist(data[data &lt;= grid.<span class="org-builtin">max</span>()], bins=grid, color=<span class="org-string">'k'</span>)
  <span class="org-keyword">for</span> q, row <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(quantiles, predicted_counts):
    plt.plot(grid[1:] - 0.5, row, c=colorcet.cm[<span class="org-string">'linear_kry_5_95_c72'</span>](q), label=q)
  plt.legend()
  plt.xticks(grid)
  plt.xlabel(<span class="org-string">'UMI count'</span>)
  plt.ylabel(<span class="org-string">'Frequency'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.arange(20)
<span class="org-variable-name">quantiles</span> = [.1, .25, .5, .75, .99]
<span class="org-variable-name">pred_counts</span> = predicted_counts(umi, mean, disp, size, onehot, grid, quantiles)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plot_fit_quantiles(umi, pred_counts, quantiles, grid)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/nb-hist-quantiles.png" alt="nb-hist-quantiles.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org9ee0e04" class="outline-2">
<h2 id="org9ee0e04">Compare NB to ZINB</h2>
<div class="outline-text-2" id="text-org9ee0e04">
<p>
Restrict to the genes which are detected in &gt;70% of samples and estimate the
parameters of the zero-inflated model.
</p>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;
&lt;&lt;read-data&gt;&gt;
&lt;&lt;zinb-impl&gt;&gt;
<span class="org-variable-name">mean</span>, <span class="org-variable-name">dispersion</span>, <span class="org-variable-name">dropout</span> = fit(
  umi=umi.values.T.astype(np.float32),
  onehot=onehot.values.astype(np.float32),
  size_factor=umi.agg(np.<span class="org-builtin">sum</span>).astype(np.float32).values.reshape(-1, 1),
  zero_inflation=<span class="org-constant">True</span>,
  learning_rate=1e-2,
  max_epochs=8000)
pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dropout.T, index=umi.index, columns=umi.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<p>
Plot the data and fitted distribution for the zero-inflated model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zi_mean</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">zi_disp</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.arange(20)
<span class="org-variable-name">exp_counts</span> = expected_counts(umi, zi_mean, zi_disp, size, onehot, grid)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plot_fit(umi, exp_counts, grid)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/zinb-hist.png" alt="zinb-hist.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.arange(20)
<span class="org-variable-name">quantiles</span> = [.1, .25, .5, .75, .99]
<span class="org-variable-name">pred_counts</span> = predicted_counts(umi, zi_mean, zi_disp, size, onehot, grid, quantiles)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plot_fit_quantiles(umi, pred_counts, quantiles, grid)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/zinb-hist-quantiles.png" alt="zinb-hist-quantiles.png">
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2018-01-22 Mon 16:07</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
