<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2018-02-15 Thu 10:41 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Mean/dispersion estimation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Mean/dispersion estimation</h1>

<div id="outline-container-org6af1f01" class="outline-2">
<h2 id="org6af1f01">Introduction</h2>
<div class="outline-text-2" id="text-org6af1f01">
<p>
The simplest approach to call mean/variance QTLs is modular:
</p>

<ol class="org-ol">
<li>Estimate a mean and a dispersion for each individual</li>
<li>Treat the mean/dispersion as continuous phenotypes and perform QTL mapping</li>
</ol>

<p>
Here, we solve (1), and <a href="qtl-mapping.html">use QTLtools</a> to solve (2). We perform the following
analyses:
</p>

<ol class="org-ol">
<li><a href="#org428aa93">We implement GPU-based</a> maximum likelihood estimation of the parameters</li>
<li><a href="#org1a397c5">We develop visual diagnostics</a> of the fitted models to check goodness of fit</li>
<li><a href="#orge17e562">We fit a model without dropout</a> and show it does not fit the data</li>
<li><a href="#org3c6170a">We fit a model assuming dropout per gene</a></li>
</ol>
</div>
</div>

<div id="outline-container-orgad32fb3" class="outline-2">
<h2 id="orgad32fb3">Model specification</h2>
<div class="outline-text-2" id="text-orgad32fb3">
<p>
Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
\(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell. As a
first pass, define \(R_{ij} = \sum_k r_{ijk}\).
</p>

<p>
Following Hilbe 2012, we derive the negative binomial as a Poisson-Gamma
mixture:
</p>

<p>
\[ r_{ijk} \sim \text{Pois}(R_{ij} \mu_{ik} u_{ijk}) \]
</p>

<p>
\[ u_{ijk} \sim \text{Gamma}(\phi_{ik}^{-1}, \phi_{ik}^{-1}) \]
</p>

<p>
Here, \(\mu_{ik}u_{ijk}\) denotes relative expression
(<a href="https://arxiv.org/abs/1104.3889">Pachter 2011</a>). Marginalizing out \(u\)
yields the negative binomial distribution, with log likelihood:
</p>

<p>
\[ \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}) = r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]
</p>

<p>
We have multiple data points (30-200 cells) per mean/dispersion parameter, so
simply minimizing the negative log likelihood should give reasonable
estimates.
</p>

<p>
We can additionally account for zero-inflation, by letting \(\pi_{\cdot}\)
denote the probability of a "technical zero" (i.e., not arising from the
negative-binomial).
</p>

<p>
As a first pass, estimate dropout assuming parameters \(\pi_k\) are shared
across cells (and individuals) for each gene. This assumption allows us to
directly estimate the parameter from the data without requiring
shrinkage/regularization to avoid overfitting.
</p>

<p>
Then, the log-likelihood of the data is:
</p>

<p>
\[ \ln p(r_{ijk} \mid r_{ijk} = 0, \cdot) = -\ln(\pi_\cdot + (1 - \pi_{\cdot})
  \exp(\ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik}))) \]
</p>

<p>
\[ \ln p(r_{ijk} \mid r_{ijk} > 0, \cdot) = \ln(1 - \pi_{\cdot}) + \ln p(r_{ijk} \mid R_{ij}, \mu_{ik}, \phi_{ik})) \]
</p>
</div>
</div>

<div id="outline-container-orgde19fe3" class="outline-2">
<h2 id="orgde19fe3">Read the data</h2>
<div class="outline-text-2" id="text-orgde19fe3">
<p>
Read the QC'ed data.
</p>

<p>
Onehot-encode the samples.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org5400cb3"><span class="org-variable-name">individuals</span> = <span class="org-builtin">sorted</span>(annotations[<span class="org-string">'chip_id'</span>].unique())
<span class="org-variable-name">onehot</span> = np.zeros((umi.shape[1], <span class="org-builtin">len</span>(individuals)), dtype=np.float32)
onehot[np.arange(onehot.shape[0]),annotations[<span class="org-string">'chip_id'</span>].<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: individuals.index(x))] = 1
<span class="org-variable-name">onehot</span> = pd.DataFrame(onehot, columns=individuals, index=umi.columns)
onehot.shape
</pre>
</div>

<pre class="example">
(4985, 54)

</pre>

<p>
Check that one-hot encoding is OK:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(100):
  <span class="org-variable-name">gene</span> = np.random.choice(umi.index)
  <span class="org-variable-name">ind</span> = np.random.choice(individuals)
  <span class="org-variable-name">idx</span> = individuals.index(ind)
  <span class="org-keyword">assert</span> (umi.loc[gene, (annotations[<span class="org-string">'chip_id'</span>] == ind).values] == 
          umi.loc[gene, onehot.dot(np.eye(onehot.shape[1])[idx]).astype(<span class="org-builtin">bool</span>)]).<span class="org-builtin">all</span>()
</pre>
</div>
</div>
</div>

<div id="outline-container-org428aa93" class="outline-2">
<h2 id="org428aa93">Tensorflow implementation</h2>
<div class="outline-text-2" id="text-org428aa93">
<p>
We optimize all of the parameters together, using one-hot encoding to map
parameters to data points. This makes inference more amenable to running on
the GPU. Although this is slower for the NB estimation problem, it will be
faster for the ZINB estimation problem when parameters are shared and we need
to operate on the entire count matrix.
</p>

<p>
Use <code>tensorflow</code> to automatically differentiate the negative log likelihood and
perform gradient descent.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org1fa0c3a"><span class="org-keyword">def</span> <span class="org-function-name">sigmoid</span>(x):
  <span class="org-doc">"""Numerically safe sigmoid"""</span>
  <span class="org-keyword">return</span> tf.clip_by_value(tf.sigmoid(x), -13, 13)

<span class="org-keyword">def</span> <span class="org-function-name">log</span>(x):
  <span class="org-doc">"""Numerically safe log"""</span>
  <span class="org-keyword">return</span> tf.log(x + 1e-8)

<span class="org-keyword">def</span> <span class="org-function-name">nb_llik</span>(x, mean, inv_disp):
  <span class="org-doc">"""Log likelihood of x distributed as NB</span>

<span class="org-doc">  See Hilbe 2012, eq. 8.10</span>

<span class="org-doc">  mean - mean (&gt; 0)</span>
<span class="org-doc">  inv_disp - inverse dispersion (&gt; 0)</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">return</span> (x * log(mean / inv_disp) -
          x * log(1 + mean / inv_disp) -
          inv_disp * log(1 + mean / inv_disp) +
          tf.lgamma(x + inv_disp) -
          tf.lgamma(inv_disp) -
          tf.lgamma(x + 1))

<span class="org-keyword">def</span> <span class="org-function-name">zinb_llik</span>(x, mean, inv_disp, logodds, eps=1e-8):
  <span class="org-doc">"""Log likelihood of x distributed as ZINB</span>

<span class="org-doc">  See Hilbe 2012, eq. 11.12, 11.13</span>

<span class="org-doc">  mean - mean (&gt; 0)</span>
<span class="org-doc">  inv_disp - inverse dispersion (&gt; 0)</span>
<span class="org-doc">  logodds - dropout log odds</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">case_zero</span> = -log(sigmoid(-logodds) + sigmoid(logodds) * tf.exp(nb_llik(x, mean, inv_disp)))
  <span class="org-variable-name">case_non_zero</span> = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
  <span class="org-keyword">return</span> tf.where(tf.less(x, 1e-8), case_zero, case_non_zero)

<span class="org-keyword">def</span> <span class="org-function-name">fit</span>(umi, onehot, size_factor, gene_dropout=<span class="org-constant">False</span>, ind_dropout=<span class="org-constant">False</span>, learning_rate=1e-2, max_epochs=1000):
  <span class="org-doc">"""Return estimated log mean and log dispersion. </span>

<span class="org-doc">  If fitting a zero-inflated model, additionally return dropout log odds.</span>

<span class="org-doc">  umi - count matrix (n x p; float32)</span>
<span class="org-doc">  onehot - mapping of individuals to cells (m x n; float32)</span>
<span class="org-doc">  size_factor - size factor vector (n x 1; float32)</span>

<span class="org-doc">  Returns:</span>

<span class="org-doc">  log_mean - log mean parameter (m x p)</span>
<span class="org-doc">  log_disp - log dispersion parameter (m x p)</span>
<span class="org-doc">  dropout - if zero_inflation, dropout log odds (n x p)</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">n</span>, <span class="org-variable-name">p</span> = umi.shape
  <span class="org-variable-name">_</span>, <span class="org-variable-name">m</span> = onehot.shape

  <span class="org-variable-name">graph</span> = tf.Graph()
  <span class="org-keyword">with</span> graph.as_default(), graph.device(<span class="org-string">'/gpu:*'</span>):
    <span class="org-variable-name">size_factor</span> = tf.Variable(size_factor, trainable=<span class="org-constant">False</span>)
    <span class="org-variable-name">umi</span> = tf.Variable(umi, trainable=<span class="org-constant">False</span>)
    <span class="org-variable-name">onehot</span> = tf.Variable(onehot, trainable=<span class="org-constant">False</span>)

    <span class="org-variable-name">mean</span> = tf.exp(tf.Variable(tf.zeros([m, p])))
    <span class="org-variable-name">inv_disp</span> = tf.exp(tf.Variable(tf.zeros([m, p])))

    <span class="org-keyword">if</span> gene_dropout:
      <span class="org-keyword">if</span> ind_dropout:
        <span class="org-variable-name">dropout_params</span> = tf.Variable(tf.zeros([m, p]))
        <span class="org-variable-name">dropout</span> = tf.matmul(onehot, dropout)
      <span class="org-keyword">else</span>:
        <span class="org-variable-name">dropout_params</span> = tf.Variable(tf.zeros([1, p]))
        <span class="org-variable-name">dropout</span> = dropout_params
      <span class="org-variable-name">llik</span> = tf.reduce_mean(
        zinb_llik(umi, size_factor * tf.matmul(onehot, mean),
                  tf.matmul(onehot, inv_disp), dropout))
    <span class="org-keyword">elif</span> ind_dropout:
      <span class="org-keyword">raise</span> <span class="org-type">ValueError</span>(<span class="org-string">'Cannot specify individual-specific dropout without gene-specific dropout'</span>)
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">llik</span> = tf.reduce_sum(
        nb_llik(umi, size_factor * tf.matmul(onehot, mean),
                tf.matmul(onehot, inv_disp)))

    <span class="org-variable-name">train</span> = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-llik)
    <span class="org-variable-name">opt</span> = [tf.log(mean), -tf.log(inv_disp)]
    <span class="org-keyword">if</span> gene_dropout:
      opt.append(dropout_params)
    <span class="org-variable-name">curr</span> = <span class="org-builtin">float</span>(<span class="org-string">'-inf'</span>)
    <span class="org-keyword">with</span> tf.Session() <span class="org-keyword">as</span> sess:
      sess.run(tf.global_variables_initializer())
      <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_epochs):
        <span class="org-variable-name">_</span>, <span class="org-variable-name">update</span> = sess.run([train, llik])
        <span class="org-keyword">if</span> <span class="org-keyword">not</span> np.isfinite(update):
          <span class="org-keyword">raise</span> tf.train.NanLossDuringTrainingError
        <span class="org-keyword">if</span> <span class="org-keyword">not</span> i % 100:
          <span class="org-keyword">print</span>(i, update)
      <span class="org-keyword">return</span> sess.run(opt)
</pre>
</div>
</div>
</div>

<div id="outline-container-orge3a2968" class="outline-2">
<h2 id="orge3a2968">Quantile-quantile diagnostic plot</h2>
<div class="outline-text-2" id="text-orge3a2968">
<p>
The challenge in visualizing the fitted distributions is that the
observations \(r_{ijk}\) are not drawn iid. from a distribution
\(g_{ik}(\cdot)\). 
</p>

<p>
Instead, we have \(r_{ijk} \sim g_{ijk}(\cdot)\), and we have used maximum
likelihood to estimate distributions \(\hat{g}_{ijk}\).
</p>

<p>
We can use an idea from <code>ashr</code>: Let \(\hat{G}_{ijk}\) denote the CDF of
\(\hat{g}_{ijk}\). Then, the distribution of values
\(\hat{G}_{ijk}(r_{ijk})\) should be uniform.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">estimated_cdf</span>(x, mean, disp, size, onehot, dropout=<span class="org-constant">None</span>):
  <span class="org-variable-name">n</span> = onehot.dot(np.exp(-disp.values.T) + 1e-8)
  <span class="org-variable-name">p</span> = 1 / (1 + size.to_frame().values * onehot.dot(np.exp(mean.values + disp.values).T))
  <span class="org-keyword">assert</span> (n.values &gt; 0).<span class="org-builtin">all</span>()
  <span class="org-keyword">assert</span> (p.values &gt;= 0).<span class="org-builtin">all</span>()
  <span class="org-keyword">assert</span> (p.values &lt;= 1).<span class="org-builtin">all</span>()
  <span class="org-variable-name">G</span> = st.nbinom(n=n, p=p).cdf(x)
  <span class="org-keyword">if</span> dropout <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">G</span> = sp.expit(-dropout.values.T) * G + sp.expit(dropout.values.T)
  <span class="org-keyword">return</span> G

<span class="org-keyword">def</span> <span class="org-function-name">diagnostic</span>(umi, mean, disp, size, onehot, dropout=<span class="org-constant">None</span>):
  <span class="org-variable-name">q</span> = estimated_cdf(umi.values.T, mean, disp, size, onehot, dropout=dropout)
  plt.clf()
  plt.scatter(x=np.linspace(0, 1, q.shape[0]),
              y=<span class="org-builtin">sorted</span>(q.ravel()),
              s=0.5)
  plt.plot([[0, 0], [1, 1]], c=<span class="org-string">'black'</span>)
  plt.title(umi.index[0])
  plt.xlabel(<span class="org-string">'Expected quantile'</span>)
  plt.ylabel(<span class="org-string">'Estimated quantile'</span>)
</pre>
</div>

<p>
Look at the quantiles of the QQ plots over all genes:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">diagnostic_quantiles</span>(umi, mean, disp, size, onehot, dropout=<span class="org-constant">None</span>, quantiles=<span class="org-constant">None</span>):
  <span class="org-keyword">if</span> quantiles <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">quantiles</span> = np.array([.05, .25, .5, .75, .95])
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">quantiles</span> = np.array(quantiles)
    <span class="org-keyword">assert</span> (0 &lt;= quantiles &lt;= 1).<span class="org-builtin">all</span>()
  <span class="org-variable-name">cdf</span> = np.sort(estimated_cdf(umi.values.T, mean, disp, size, onehot, dropout=dropout).T)
  <span class="org-variable-name">cdf_quantiles</span> = np.percentile(cdf, 100 * quantiles, interpolation=<span class="org-string">'higher'</span>, axis=0)

  plt.clf()
  <span class="org-keyword">for</span> q, row <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(quantiles, cdf_quantiles):
    plt.scatter(x=np.linspace(0, 1, row.shape[0]), y=row, c=colorcet.cm[<span class="org-string">'kbc'</span>](q), s=.5, label=q)
  plt.plot([0, 1], [0, 1], c=<span class="org-string">'r'</span>, ls=<span class="org-string">'dashed'</span>)
  plt.legend()
  plt.xlabel(<span class="org-string">'Expected quantile'</span>)
  plt.ylabel(<span class="org-string">'Estimated quantile'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org1a397c5" class="outline-2">
<h2 id="org1a397c5">Histogram diagnostic plot</h2>
<div class="outline-text-2" id="text-org1a397c5">
<p>
The challenge in visualizing the fitted distributions is that the
observations \(r_{ijk}\) are not drawn iid. from a distribution
\(g_{ik}(\cdot)\). 
</p>

<p>
Instead, we have \(r_{ijk} \sim g_{ijk}(\cdot)\), and we have used maximum
likelihood to estimate distributions \(\hat{g}_{ijk}\).
</p>

<p>
Plot the expected number of draws from \(\bar{g}\), assuming the number of
draws in the interval \([a, b]\) is \(\mathrm{Bin}(N, \bar{G}(b) -
  \bar{G}(a))\), where \(N\) is the total number of observations and
\(\bar{G}\) is the average CDF:
</p>

<p>
\[ \bar{G}(x) = \frac{1}{N} \sum_{i,j,k} \hat{G}_{ijk}(x) \]
</p>

<p>
The data distribution has an extremely long tail, so truncate it to make the
majority of the fit easier to see:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = (pd.Series([50, 75, 90, 95, 99])
         .<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: np.percentile(umi.values.ravel(), x))
         .to_frame()
         .rename_axis(<span class="org-string">'Percentile'</span>, axis=<span class="org-string">'index'</span>))
<span class="org-variable-name">query.columns</span> = [<span class="org-string">'UMI'</span>]
query
</pre>
</div>

<pre class="example">
UMI
Percentile
0            1.0
1            3.0
2            8.0
3           14.0
4           49.0
</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">expected_counts</span>(umi, mean, disp, size, onehot, grid):
  <span class="org-variable-name">data</span> = umi.values.ravel()
  <span class="org-variable-name">average_cdf</span> = [np.mean(estimated_cdf(g, mean, disp, size, onehot)) <span class="org-keyword">for</span> g <span class="org-keyword">in</span> grid]
  <span class="org-keyword">return</span> data.shape[0] * np.diff(average_cdf)

<span class="org-keyword">def</span> <span class="org-function-name">plot_fit</span>(umi, expected_counts, grid):
  <span class="org-variable-name">data</span> = umi.values.ravel()
  plt.clf()
  plt.hist(data[data &lt; grid.<span class="org-builtin">max</span>()], bins=grid, color=<span class="org-string">'k'</span>)
  plt.plot(grid[1:] - 0.5, expected_counts, c=<span class="org-string">'r'</span>)
  plt.xticks(grid)
  plt.xlabel(<span class="org-string">'UMI count'</span>)
  plt.ylabel(<span class="org-string">'Frequency'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-orge17e562" class="outline-2">
<h2 id="orge17e562">Fit NB</h2>
<div class="outline-text-2" id="text-orge17e562">
<p>
Estimate means and dispersions assuming no dropout.
</p>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;
&lt;&lt;zinb-impl&gt;&gt;
&lt;&lt;read-data-qc&gt;&gt;
&lt;&lt;onehot-qc&gt;&gt;
<span class="org-variable-name">mean</span>, <span class="org-variable-name">dispersion</span> = fit(
  umi=umi.values.T.astype(np.float32),
  onehot=onehot.values.astype(np.float32),
  size_factor=umi.agg(np.<span class="org-builtin">sum</span>).astype(np.float32).values.reshape(-1, 1),
  learning_rate=1e-2,
  max_epochs=8000)
pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mean2.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<p>
Check the goodness of fit.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mean</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/mean2.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">disp</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/dispersion2.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">size</span> = umi.agg(np.<span class="org-builtin">sum</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">diagnostic(umi.iloc[:1], mean.iloc[:1], disp.iloc[:1], size, onehot)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/nb-quantiles.png" alt="nb-quantiles.png">
</p>
</div>

<p>
The estimated quantiles are higher than the expected quantiles, suggesting
that the estimated distributions have too much density at lower values. This
result is explained by the fact that means are biased downwards and
dispersions are biased upwards due to zero-inflation. Accordingly, we expect
to find some genes which depart even more from uniform quantiles.
</p>

<div class="org-src-container">
<pre class="src src-ipython">diagnostic_quantiles(umi, mean, disp, size, onehot)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/nb-quantiles-dist.png" alt="nb-quantiles-dist.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.arange(14)
plot_fit(umi, expected_counts(umi, mean, disp, size, onehot, grid), grid)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/nb-hist.png" alt="nb-hist.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org3c6170a" class="outline-2">
<h2 id="org3c6170a">Fit ZINB</h2>
<div class="outline-text-2" id="text-org3c6170a">
<p>
Estimate the parameters of the zero-inflated model assuming dropout per gene.
</p>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;
&lt;&lt;zinb-impl&gt;&gt;
&lt;&lt;read-data-qc&gt;&gt;
&lt;&lt;onehot-qc&gt;&gt;
<span class="org-variable-name">mean</span>, <span class="org-variable-name">dispersion</span>, <span class="org-variable-name">dropout</span> = fit(
  umi=umi.values.T.astype(np.float32),
  onehot=onehot.values.astype(np.float32),
  size_factor=umi.agg(np.<span class="org-builtin">sum</span>).astype(np.float32).values.reshape(-1, 1),
  gene_dropout=<span class="org-constant">True</span>,
  learning_rate=1e-2,
  max_epochs=8000)
pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dropout.T, index=umi.index).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<p>
Plot the diagnostic for the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zi_mean</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">zi_disp</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-dispersion.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">zi_dropout</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi-dropout.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">diagnostic_quantiles(umi, zi_mean, zi_disp, size, onehot, zi_dropout)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/zinb-diagnostic.png" alt="zinb-diagnostic.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org72d12f4" class="outline-2">
<h2 id="org72d12f4">Fit ZINB2</h2>
<div class="outline-text-2" id="text-org72d12f4">
<p>
Estimate the parameters of the zero-inflated model assuming dropout per
individual and gene.
</p>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;
&lt;&lt;zinb-impl&gt;&gt;
&lt;&lt;read-data-qc&gt;&gt;
&lt;&lt;onehot-qc&gt;&gt;
<span class="org-variable-name">mean</span>, <span class="org-variable-name">dispersion</span>, <span class="org-variable-name">dropout</span> = fit(
  umi=umi.values.T.astype(np.float32),
  onehot=onehot.values.astype(np.float32),
  size_factor=umi.agg(np.<span class="org-builtin">sum</span>).astype(np.float32).values.reshape(-1, 1),
  gene_dropout=<span class="org-constant">True</span>,
  ind_dropout=<span class="org-constant">True</span>,
  learning_rate=1e-2,
  max_epochs=8000)
pd.DataFrame(mean.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi2-mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dispersion.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi2-dispersion.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dropout.T, index=umi.index, columns=onehot.columns).to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi2-dropout.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<p>
Plot the data and fitted distribution.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zi2_mean</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi2-mean.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">zi2_disp</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi2-dispersion.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
<span class="org-variable-name">zi2_dropout</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/zi2-dropout.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">diagnostic_quantiles(umi, zi2_mean, zi2_disp, size, onehot, zi2_dropout)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/zinb-diagnostic.png" alt="zinb-diagnostic.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.arange(14)
<span class="org-variable-name">exp_counts</span> = expected_counts(umi, zi2_mean, zi2_disp, size, onehot, grid)
plot_fit(umi, exp_counts, grid)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/zinb-hist.png" alt="zinb-hist.png">
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2018-02-15 Thu 10:41</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
