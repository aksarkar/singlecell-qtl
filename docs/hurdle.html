<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2017-12-22 Fri 15:47 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Hurdle model estimation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Hurdle model estimation</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgef8e65e">Model specification and inference</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgef8e65e" class="outline-2">
<h2 id="orgef8e65e">Model specification and inference</h2>
<div class="outline-text-2" id="text-orgef8e65e">
<p>
We specify the model in Edward. We assume a fully factored variational
approximation \(q(\beta_\lambda)q(\beta_\phi)q(u^\lambda)q(u^\phi)\).
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org02c3e16"><span class="org-variable-name">nonzero_cpm</span> = data[<span class="org-string">'log_cpm'</span>].ravel() &gt; 0
<span class="org-variable-name">q</span> = nonzero_cpm.<span class="org-builtin">sum</span>()
<span class="org-variable-name">m</span>, <span class="org-variable-name">n</span> = data[<span class="org-string">'onehot'</span>].shape
<span class="org-variable-name">_</span>, <span class="org-variable-name">p</span> = data[<span class="org-string">'genotypes'</span>].shape

<span class="org-variable-name">onehot</span> = tf.placeholder(tf.float32, [q, n])
<span class="org-variable-name">genotypes</span> = tf.placeholder(tf.float32, [n, p])
<span class="org-variable-name">cell_bias</span> = tf.placeholder(tf.float32, [q, 1])

<span class="org-variable-name">rate_bias_scale</span> = tf.exp(tf.Variable(tf.ones([1])))
<span class="org-variable-name">rate_bias</span> = ed.models.Normal(loc=tf.zeros([n, 1]), scale=rate_bias_scale)

<span class="org-variable-name">rate_effect_scale</span> = tf.exp(tf.Variable(tf.ones([1])))
<span class="org-variable-name">rate_effect</span> = ed.models.Normal(loc=tf.zeros([p, 1]), scale=rate_effect_scale)

<span class="org-variable-name">log_rate</span> = tf.matmul(onehot, tf.matmul(genotypes, rate_effect) + rate_bias)
<span class="org-variable-name">mean</span> = log_rate + cell_bias

<span class="org-variable-name">disp_bias_scale</span> = tf.exp(tf.Variable(tf.ones([1])))
<span class="org-variable-name">disp_bias</span> = ed.models.Normal(loc=tf.zeros([n, 1]), scale=disp_bias_scale)

<span class="org-variable-name">disp_effect_scale</span> = tf.exp(tf.Variable(tf.ones([1])))
<span class="org-variable-name">disp_effect</span> = ed.models.Normal(loc=tf.zeros([p, 1]), scale=disp_effect_scale)

<span class="org-variable-name">disp</span> = tf.matmul(onehot, tf.matmul(genotypes, disp_effect) + disp_bias)
<span class="org-variable-name">resid_var_scale</span> = tf.exp(tf.Variable(tf.zeros([1])))
<span class="org-variable-name">var</span> = tf.exp(-log_rate) + tf.exp(disp) + resid_var_scale

<span class="org-variable-name">log_cpm</span> = ed.models.Normal(loc=mean, scale=tf.sqrt(var))

<span class="org-variable-name">q_rate_bias</span> = ed.models.NormalWithSoftplusScale(
  loc=tf.Variable(tf.random_normal([n, 1])),
  scale=tf.Variable(tf.ones([n, 1])))
<span class="org-variable-name">q_disp_bias</span> = ed.models.NormalWithSoftplusScale(
  loc=tf.Variable(tf.random_normal([n, 1])),
  scale=tf.Variable(tf.ones([n, 1])))

<span class="org-variable-name">q_rate_effect</span> = ed.models.NormalWithSoftplusScale(
  loc=tf.Variable(tf.random_normal([p, 1], stddev=0.1)),
  scale=tf.Variable(tf.fill([p, 1], -8.)))
<span class="org-variable-name">q_disp_effect</span> = ed.models.NormalWithSoftplusScale(
  loc=tf.Variable(tf.random_normal([p, 1], stddev=0.1)),
  scale=tf.Variable(tf.fill([p, 1], -8.)))
</pre>
</div>

<p>
We optimize the evidence lower bound with respect to the variational parameters
and model hyperparameters (scales) simultaneously using the
reparameterization gradient and gradient descent.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org1b6b5db"><span class="org-variable-name">inf</span> = ed.ReparameterizationKLKLqp(
  latent_vars={
    rate_effect: q_rate_effect,
    rate_bias: q_rate_bias,
    disp_effect: q_disp_effect,
    disp_bias: q_disp_bias,
  },
  data={
    onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm],
    genotypes: data[<span class="org-string">'genotypes'</span>],
    cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm],
    log_cpm: data[<span class="org-string">'log_cpm'</span>][nonzero_cpm],
  })
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">inf.run(n_samples=10, n_print=1000, optimizer=tf.train.AdamOptimizer(learning_rate=5e-2))
</pre>
</div>

<p>
Tabulate the estimated hyperparameters:
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org1194b08">pd.DataFrame(
  ed.get_session().run(
    [resid_var_scale,
     rate_effect_scale,
     rate_bias_scale,
     disp_effect_scale,
     disp_bias_scale,
    ]),
  index=[<span class="org-string">'resid_var_scale'</span>,
         <span class="org-string">'rate_effect_scale'</span>,
         <span class="org-string">'rate_bias_scale'</span>,
         <span class="org-string">'disp_effect_scale'</span>,
         <span class="org-string">'disp_bias_scale'</span>])
</pre>
</div>

<pre class="example">
                        0
resid_var_scale    0.008320
rate_effect_scale  1.789584
rate_bias_scale    0.932777
disp_effect_scale  0.018626
disp_bias_scale    0.314646
</pre>

<p>
Compute posterior 95% credible intervals for the effect sizes:
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org35a80a8"><span class="org-variable-name">res</span> = pd.DataFrame(np.hstack(ed.get_session().run(
  [q_rate_effect.mean(),
   1.96 * tf.sqrt(q_rate_effect.variance()),
   q_disp_effect.mean(),
   1.96 * tf.sqrt(q_disp_effect.variance())])),
  columns=[<span class="org-string">'rate_effect_mean'</span>, <span class="org-string">'rate_effect_ci'</span>, <span class="org-string">'disp_effect_mean'</span>, <span class="org-string">'disp_effect_ci'</span>])
res
</pre>
</div>

<pre class="example">
 rate_effect_mean  rate_effect_ci  disp_effect_mean  disp_effect_ci
0          1.792302        0.073195         -0.003314        0.035052
</pre>

<p>
Estimate posterior 95% credible intervals for the bias terms:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">res</span> = pd.DataFrame(np.hstack(ed.get_session().run(
  [q_rate_bias.mean(),
   1.96 * tf.sqrt(q_rate_bias.variance()),
   q_disp_bias.mean(),
   1.96 * tf.sqrt(q_disp_bias.variance())]
)))
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
ax[0].errorbar(x=res.index, y=res[0], yerr=res[1], fmt=<span class="org-string">'o'</span>)
ax[0].set_xticks([])
ax[0].set_xlabel(<span class="org-string">''</span>)
ax[0].set_ylabel(<span class="org-string">'Rate bias'</span>)
ax[1].errorbar(x=res.index, y=res[2], yerr=res[3], fmt=<span class="org-string">'o'</span>)
ax[1].set_xticks([])
ax[1].set_xlabel(<span class="org-string">'Individual'</span>)
ax[1].set_ylabel(<span class="org-string">'Dispersion bias'</span>)
plt.gcf()
</pre>
</div>


<div class="figure">
<p><img src="figure/hurdle.org/bias.png" alt="bias.png">
</p>
</div>

<p>
Plot a posterior predictive draw, and the real data means and twice standard
deviations.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">post_pred</span> = ed.get_session().run(
  ed.copy(log_cpm, inf.latent_vars),
  {
    onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm],
    genotypes: data[<span class="org-string">'genotypes'</span>][:,0:1],
    cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm],
    log_cpm: data[<span class="org-string">'log_cpm'</span>][nonzero_cpm],
  })

plt.clf()
plt.gcf().set_size_inches(8, 6)
plt.scatter(x=np.where(data[<span class="org-string">'onehot'</span>][nonzero_cpm] == 1)[1] + np.random.normal(scale=0.1, size=q), y=post_pred, s=2)
plt.errorbar(x=np.arange(n), y=onehot_cpm.mean(axis=0), yerr=2 * onehot_cpm.std(axis=0), fmt=<span class="org-string">'o'</span>, c=<span class="org-string">'red'</span>)
plt.xlabel(<span class="org-string">'Individual'</span>)
plt.ylabel(<span class="org-string">'$\log_2(CPM + 1)$'</span>)
plt.gca().set_xticks([])
plt.gcf()
</pre>
</div>


<div class="figure">
<p><img src="figure/hurdle.org/post-pred.png" alt="post-pred.png">
</p>
</div>

<p>
Investigate the terms of the variance model to understand why the posterior
predictive distribution has larger variance than the original data. Compare
the sample variance of non-zero CPM between cells within each individual to
the estimated variance (plugging in the estimated posterior means into the
model).
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.DataFrame(np.hstack([
  onehot_cpm.var(axis=0).filled().reshape(-1, 1),
  ed.get_session().run(tf.exp(-tf.matmul(genotypes, q_rate_effect.mean()) - q_rate_bias.mean()), {onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm], genotypes: data[<span class="org-string">'genotypes'</span>], cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm]}),
  ed.get_session().run(tf.exp(tf.matmul(genotypes, q_disp_effect.mean()) + q_disp_bias.mean()), {onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm], genotypes: data[<span class="org-string">'genotypes'</span>], cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm]})
]), columns=[<span class="org-string">'sample_var'</span>, <span class="org-string">'mean_component'</span>, <span class="org-string">'disp_component'</span>])
</pre>
</div>

<pre class="example">
  sample_var  mean_component  disp_component
0     0.871253        0.297413        1.116377
1     0.685687        0.282990        0.647191
2     0.832976        0.159781        0.967025
3     0.700838        0.197719        0.718751
4     0.814616        0.283947        0.682625
5     0.691467        0.334075        0.615903
6     0.783659        0.196551        1.178612
7     0.802240        0.150031        1.096487
8     0.834025        0.242036        1.006448
9     0.772049        0.234511        0.885288
10    0.473087        0.148331        0.819748
11    0.849796        0.277787        0.665290
12    0.866909        0.148379        0.950069
13    1.008157        0.178938        1.106427
14    0.534548        0.146360        0.912476
15    0.577289        0.146111        1.109558
16    0.592843        0.192143        0.861032
17    0.868248        0.244231        0.828985
18    0.643367        0.177224        0.819485
19    0.610603        0.153846        0.782633
20    0.561710        0.275283        0.635205
</pre>

<p>
The dispersion effect is close to zero, which suggests that the problem is
with the estimation of \(u_i^\phi\):
</p>

<div class="org-src-container">
<pre class="src src-ipython">ed.get_session().run(tf.exp(q_disp_bias.mean()), {onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm], genotypes: data[<span class="org-string">'genotypes'</span>], cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm]})
</pre>
</div>

<pre class="example">
array([[ 1.11637723],
         [ 0.64719135],
         [ 0.97023523],
         [ 0.71875077],
         [ 0.6826247 ],
         [ 0.6159026 ],
         [ 1.18252456],
         [ 1.10012722],
         [ 1.00644839],
         [ 0.88528788],
         [ 0.82224023],
         [ 0.66529012],
         [ 0.95322323],
         [ 1.11009932],
         [ 0.91550487],
         [ 1.11324096],
         [ 0.86389071],
         [ 0.82898474],
         [ 0.82220507],
         [ 0.78523099],
         [ 0.63520527]], dtype=float32)
</pre>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2017-12-22 Fri 15:47</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
