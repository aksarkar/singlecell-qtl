<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2017-12-20 Wed 10:22 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Hurdle model estimation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Hurdle model estimation</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgc8dc85f">Introduction</a></li>
<li><a href="#org4d022f2">Test case</a></li>
<li><a href="#orgef8e65e">Model specification and inference</a></li>
<li><a href="#orgbd60d6a">Next steps</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgc8dc85f" class="outline-2">
<h2 id="orgc8dc85f">Introduction</h2>
<div class="outline-text-2" id="text-orgc8dc85f">
<p>
The key idea of <code>voom</code> (<a href="http://genomebiology.com/2014/15/2/R29">Law et al 2014</a>) is that the distribution of
log-transformed counts per million is approximately Gaussian. Consider the
expression of a single gene in individual \(i\), cell \(j\).
</p>

<p>
\[ r_{ij} = \mbox{number of reads} \]
</p>

<p>
Assuming \(r_{ij}\) follows the NB2 negative binomial model (Hilbe 2012):
</p>

<p>
\[ E[r_{ij}] = \lambda_{ij} \]
\[ V[r_{ij}] = \lambda_{ij} + \phi_{ij} \lambda_{ij}^2 \]
</p>

<p>
Let \(L_j\) be the library size of cell \(j\). Then, by the definition of CPM:
</p>

<p>
\[ y_{ij} = \log_2(10^6 \times \frac{r_{ij}}{L_j} + 1) \]
</p>

<p>
\[ = \log_2\left(10^6 \times \left(r_{ij} + \frac{L_j}{10^6}\right)\right) - \log_2(L_j) \]
</p>

<p>
Assuming that \(L_j \ll 10^6\):
</p>

<p>
\[ y_{ij} \approx \log_2(r_{ij}) - \log_2(L_j) + 6 \log_2(10) \]
</p>

<p>
By first order Taylor expansion:
</p>

<p>
\[ E[y_{ij}] = \mu_{ij} \approx \log_2 \lambda_{ij} + \mathrm{const} \]
</p>

<p>
By the delta method:
</p>

<p>
\[ V[y_{ij}] = \sigma_{ij}^2 \approx V[r_{ij}] / \lambda_{ij}^2 = 1/\lambda_{ij} + \phi_{ij} \]
</p>

<p>
We assume:
</p>

<p>
\[ y_{ij} \approx N(\mu_{ij}, \sigma^2_{ij}) \]
</p>

<p>
Now, we seek to write a hierarchical model for \(y\) in terms of the genotype
of individual \(i\) to call mean and variance QTLs.
</p>

<p>
Our idea is to start from generalized linear models for the underlying rate
and dispersion:
</p>

<p>
\[ \log \lambda_{ij} = X_i \beta_\lambda + u_i^\lambda + \epsilon^\lambda_{ij} \]
</p>

<p>
\[ \log \phi_{ij} = X_i \beta_\phi + u_i^\phi + \epsilon^\phi_{ij} \]
</p>

<p>
We assume errors are uncorrelated with \(X, \beta, u\) so we can write
\(\sigma^2 = V[\epsilon_{ij}^\lambda] + V[\epsilon_{ij}^\phi]\)
</p>

<p>
Then, the likelihood of each data point is given by:
</p>

<p>
\[ y_{ij} \sim N(X_i \beta_\lambda + u_i^\lambda, \exp(-(X_i \beta_\lambda +
  u_i^\lambda)) + \exp(X_i \beta_\phi + u_i^\phi) + \sigma^2) \]
</p>

<p>
In this model, between individual variance is explained by <i>cis</i>-genotype
(\(X_i \beta_\lambda)\), unobserved factors (\(u_i^\lambda)\), and sampling
(\(\epsilon_{ij}^\lambda\)).
</p>

<p>
Within individual variance is explained by the rate of expression (as derived
above), <i>cis</i>-genotype (\(X_i \beta_\phi\)), unobserved factors \(u_i^\phi\),
and sampling (\(\epsilon_{ij}^\phi\)).
</p>

<p>
Unobserved factors are needed to account for the fact that two individuals
with the same genotype could still have different underlying rate and
dispersion of expression.
</p>

<p>
We can estimate the posterior \(p(\beta, u \mid Y, \cdot)\) this model using
a combination of black-box variational inference and variational EM.
</p>
</div>
</div>

<div id="outline-container-org4d022f2" class="outline-2">
<h2 id="org4d022f2">Test case</h2>
<div class="outline-text-2" id="text-org4d022f2">
<p>
Look at the top eQTL SNP-gene pair from the bulk RNA-sequencing data,
<code>rs73276049</code> and <i>ZSWIM7</i>.
</p>

<pre class="example">
                symbol        rsid  effect_size     beta_perm
ENSG00000214941   ZSWIM7  rs73276049     1.792670  1.567000e-17
ENSG00000145725  PPIP5K2     rs34822     1.155860  1.280260e-15
ENSG00000164978    NUDT2   rs7848476     0.998739  6.685150e-15
ENSG00000243317  C7orf73   rs6467603     1.371900  1.508780e-14
ENSG00000240344    PPIL3  rs13412214    -1.343370  1.566880e-14
</pre>

<p>
Load the genotype data for <code>rs73276049</code>.
</p>

<div class="org-src-container">
<pre class="src src-sh" id="orgc68f139">zcat /project2/gilad/singlecell-qtl/bulk/genotypes.vcf.gz | head -n1 | awk <span class="org-string">'{for (i=1; i &lt;= NF; i++) {if (i &gt; 9) {$i = "NA" $i}} print $0}'</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh" id="org6d2f60d">zgrep -wm1 <span class="org-string">"rs73276049"</span> /project2/gilad/singlecell-qtl/bulk/genotypes.vcf.gz
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">genotypes</span> = pd.Series(<span class="org-string">"""&lt;&lt;extract-geno()&gt;&gt;"""</span>[2:-2].split())
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">genotypes.index</span> = <span class="org-string">"""&lt;&lt;extract-header()&gt;&gt;"""</span>.split()
</pre>
</div>

<p>
Compute library sizes and CPM.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">normalizers</span> = (6 * np.log(10) - np.log(umi_qc.agg(np.<span class="org-builtin">sum</span>))) / np.log(2)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">data</span> = {<span class="org-string">'onehot'</span>: onehot,
        <span class="org-string">'genotypes'</span>: genotypes.T[<span class="org-builtin">sorted</span>(annotations_qc[<span class="org-string">'chip_id'</span>].unique())].values.reshape(-1, 1),
        <span class="org-string">'counts'</span>: umi_qc.loc[<span class="org-string">'ENSG00000214941'</span>].values.astype(np.float32).reshape(-1, 1),
        <span class="org-string">'log_cpm'</span>: cpm(umi_qc, log2=<span class="org-constant">True</span>).loc[<span class="org-string">'ENSG00000214941'</span>].values.astype(np.float32).reshape(-1, 1),
        <span class="org-string">'normalizers'</span>: normalizers.values.reshape(-1, 1)}
</pre>
</div>

<p>
Write out the processed data to use as a test case.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'test_data.pkl'</span>, <span class="org-string">'wb'</span>) <span class="org-keyword">as</span> f:
  pickle.dump(data, f)
</pre>
</div>

<p>
Plot the data, mean, and two standard deviations per individual (restricted
to non-zero CPM values):
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">onehot_cpm</span> = np.ma.masked_equal(data[<span class="org-string">'log_cpm'</span>] * data[<span class="org-string">'onehot'</span>], 0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(8, 6)
plt.scatter(x=np.where(data[<span class="org-string">'onehot'</span>] == 1)[1] + np.random.normal(scale=0.1, size=data[<span class="org-string">'onehot'</span>].shape[0]), y=data[<span class="org-string">'log_cpm'</span>], s=2, alpha=0.5)
plt.errorbar(x=np.arange(data[<span class="org-string">'onehot'</span>].shape[1]), y=onehot_cpm.mean(axis=0), yerr=2 * onehot_cpm.std(axis=0), fmt=<span class="org-string">'o'</span>, c=<span class="org-string">'red'</span>)
plt.xlabel(<span class="org-string">'Individual'</span>)
plt.ylabel(<span class="org-string">'$\log_2(CPM + 1)$'</span>)
plt.gca().set_xticks([])
plt.gcf()
</pre>
</div>


<div class="figure">
<p><img src="figure/hurdle.org/data.png" alt="data.png">
</p>
</div>

<p>
Use this block to load the data without loading/processing the entire counts
matrix (requires much less memory).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'test_data.pkl'</span>, <span class="org-string">'rb'</span>) <span class="org-keyword">as</span> f:
  <span class="org-variable-name">data</span> = pickle.load(f)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgef8e65e" class="outline-2">
<h2 id="orgef8e65e">Model specification and inference</h2>
<div class="outline-text-2" id="text-orgef8e65e">
<p>
We specify the model in Edward. We assume a fully factored variational
approximation \(q(\beta_\lambda)q(\beta_\phi)q(u^\lambda)q(u^\phi)\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">nonzero_cpm</span> = data[<span class="org-string">'log_cpm'</span>].ravel() &gt; 0
<span class="org-variable-name">q</span> = nonzero_cpm.<span class="org-builtin">sum</span>()
<span class="org-variable-name">m</span>, <span class="org-variable-name">n</span> = data[<span class="org-string">'onehot'</span>].shape
<span class="org-variable-name">_</span>, <span class="org-variable-name">p</span> = data[<span class="org-string">'genotypes'</span>].shape

<span class="org-variable-name">onehot</span> = tf.placeholder(tf.float32, [q, n])
<span class="org-variable-name">genotypes</span> = tf.placeholder(tf.float32, [n, 1])
<span class="org-variable-name">cell_bias</span> = tf.placeholder(tf.float32, [q, 1])

<span class="org-variable-name">resid_var_scale</span> = tf.exp(tf.Variable(tf.zeros([1])))

<span class="org-variable-name">ind_bias_scale</span> = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
<span class="org-variable-name">ind_bias</span> = ed.models.Normal(loc=tf.zeros([n, 1]), scale=ind_bias_scale)

<span class="org-variable-name">rate_effect_scale</span> = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
<span class="org-variable-name">rate_effect</span> = ed.models.Normal(loc=tf.zeros([1, 1]), scale=rate_effect_scale)

<span class="org-variable-name">log_rate</span> = tf.matmul(onehot, tf.matmul(genotypes, rate_effect) + ind_bias)
<span class="org-variable-name">mean</span> = log_rate + cell_bias

<span class="org-variable-name">disp_bias_scale</span> = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
<span class="org-variable-name">disp_bias</span> = ed.models.Normal(loc=tf.zeros([n, 1]), scale=disp_bias_scale)

<span class="org-variable-name">disp_effect_scale</span> = tf.exp(tf.Variable(tf.ones([1]))) * resid_var_scale
<span class="org-variable-name">disp_effect</span> = ed.models.Normal(loc=tf.zeros([1, 1]), scale=disp_effect_scale)

<span class="org-variable-name">disp</span> = tf.matmul(onehot, tf.matmul(genotypes, disp_effect) + disp_bias)
<span class="org-variable-name">var</span> = tf.exp(-log_rate) + tf.exp(disp) + resid_var_scale

<span class="org-variable-name">log_cpm</span> = ed.models.Normal(loc=mean, scale=tf.sqrt(var))

<span class="org-variable-name">q_ind_bias</span> = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([n, 1])), scale=tf.Variable(tf.ones([n, 1])))
<span class="org-variable-name">q_disp_bias</span> = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([n, 1])), scale=tf.Variable(tf.ones([n, 1])))
<span class="org-variable-name">q_rate_effect</span> = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([1, 1])), scale=tf.Variable(tf.random_normal([1, 1])))
<span class="org-variable-name">q_disp_effect</span> = ed.models.NormalWithSoftplusScale(loc=tf.Variable(tf.random_normal([1, 1])), scale=tf.Variable(tf.random_normal([1, 1])))
</pre>
</div>

<p>
We optimize the evidence lower bound with respect to the variational parameters
and model hyperparameters (scales) simultaneously using the
reparameterization gradient and gradient descent.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">inf</span> = ed.ReparameterizationKLKLqp(
  latent_vars={
    rate_effect: q_rate_effect,
    ind_bias: q_ind_bias,
    disp_effect: q_disp_effect,
    disp_bias: q_disp_bias,
  },
  data={
    onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm],
    genotypes: data[<span class="org-string">'genotypes'</span>],
    cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm],
    log_cpm: data[<span class="org-string">'log_cpm'</span>][nonzero_cpm],
  })
inf.run(n_samples=10, optimizer=tf.train.AdamOptimizer(learning_rate=5e-2))
</pre>
</div>

<pre class="example">
1000/1000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 2315.045

</pre>

<p>
Tabulate the estimated hyperparameters:
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.DataFrame(ed.get_session().run(
  [resid_var_scale,
   rate_effect_scale,
   ind_bias_scale,
   disp_effect_scale,
   disp_bias_scale]))
</pre>
</div>

<pre class="example">
        0
0  0.008109
1  1.785125
2  0.930144
3  0.019294
4  0.305007
</pre>

<p>
Compute posterior 95% credible intervals for the effect sizes:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">res</span> = pd.DataFrame(np.hstack(ed.get_session().run(
  [q_rate_effect.mean(),
   1.96 * tf.sqrt(q_rate_effect.variance()),
   q_disp_effect.mean(),
   1.96 * tf.sqrt(q_disp_effect.variance())])),
  columns=[<span class="org-string">'rate_effect_mean'</span>, <span class="org-string">'rate_effect_ci'</span>, <span class="org-string">'disp_effect_mean'</span>, <span class="org-string">'disp_effect_ci'</span>])
res
</pre>
</div>

<pre class="example">
 rate_effect_mean  rate_effect_ci  disp_effect_mean  disp_effect_ci
0          1.811013        0.077898         -0.005509        0.038054
</pre>

<p>
Estimate posterior 95% credible intervals for the bias terms:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">res</span> = pd.DataFrame(np.hstack(ed.get_session().run(
  [q_ind_bias.mean(),
   1.96 * tf.sqrt(q_ind_bias.variance()),
   q_disp_bias.mean(),
   1.96 * tf.sqrt(q_ind_bias.variance())]
)))
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
ax[0].errorbar(x=res.index, y=res[0], yerr=res[1], fmt=<span class="org-string">'o'</span>)
ax[0].set_xticks([])
ax[0].set_xlabel(<span class="org-string">''</span>)
ax[0].set_ylabel(<span class="org-string">'Rate bias'</span>)
ax[1].errorbar(x=res.index, y=res[2], yerr=res[3], fmt=<span class="org-string">'o'</span>)
ax[1].set_xticks([])
ax[1].set_xlabel(<span class="org-string">'Individual'</span>)
ax[1].set_ylabel(<span class="org-string">'Dispersion bias'</span>)
plt.gcf()
</pre>
</div>


<div class="figure">
<p><img src="figure/hurdle.org/bias.png" alt="bias.png">
</p>
</div>

<p>
Plot a posterior predictive draw, and the real data means and twice standard
deviations.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">post_pred</span> = ed.get_session().run(
  ed.copy(log_cpm, inf.latent_vars),
  {
    onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm],
    genotypes: data[<span class="org-string">'genotypes'</span>][:,0:1],
    cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm],
    log_cpm: data[<span class="org-string">'log_cpm'</span>][nonzero_cpm],
  })

plt.clf()
plt.gcf().set_size_inches(8, 6)
plt.scatter(x=np.where(data[<span class="org-string">'onehot'</span>][nonzero_cpm] == 1)[1] + np.random.normal(scale=0.1, size=q), y=post_pred, s=2)
plt.errorbar(x=np.arange(n), y=onehot_cpm.mean(axis=0), yerr=2 * onehot_cpm.std(axis=0), fmt=<span class="org-string">'o'</span>, c=<span class="org-string">'red'</span>)
plt.xlabel(<span class="org-string">'Individual'</span>)
plt.ylabel(<span class="org-string">'$\log_2(CPM + 1)$'</span>)
plt.gca().set_xticks([])
plt.gcf()
</pre>
</div>


<div class="figure">
<p><img src="figure/hurdle.org/post-pred.png" alt="post-pred.png">
</p>
</div>

<p>
Investigate the terms of the variance model to understand why the posterior
predictive distribution has larger variance than the original data. Compare
the sample variance of non-zero CPM between cells within each individual to
the estimated variance (plugging in the estimated posterior means into the
model).
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.DataFrame(np.hstack([
  onehot_cpm.var(axis=0).filled().reshape(-1, 1),
  ed.get_session().run(tf.exp(-tf.matmul(genotypes, q_rate_effect.mean()) - q_ind_bias.mean()), {onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm], genotypes: data[<span class="org-string">'genotypes'</span>], cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm]}),
  ed.get_session().run(tf.exp(tf.matmul(genotypes, q_disp_effect.mean()) + q_disp_bias.mean()), {onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm], genotypes: data[<span class="org-string">'genotypes'</span>], cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm]})
]), columns=[<span class="org-string">'sample_var'</span>, <span class="org-string">'mean_component'</span>, <span class="org-string">'disp_component'</span>])
</pre>
</div>

<pre class="example">
  sample_var  mean_component  disp_component
0     0.871253        0.302830        1.118143
1     0.685687        0.288532        0.619445
2     0.832976        0.156608        0.971450
3     0.700838        0.201736        0.719656
4     0.814616        0.274285        0.666530
5     0.691467        0.333145        0.607604
6     0.783659        0.192529        1.115879
7     0.802240        0.149890        1.041115
8     0.834025        0.244234        1.002957
9     0.772049        0.227092        0.910875
10    0.473087        0.141089        0.815421
11    0.849796        0.278309        0.724199
12    0.866909        0.139993        0.909418
13    1.008157        0.169650        1.079417
14    0.534548        0.137537        0.923383
15    0.577289        0.136400        1.124875
16    0.592843        0.183967        0.855929
17    0.868248        0.262581        0.881569
18    0.643367        0.183501        0.850260
19    0.610603        0.150725        0.781642
20    0.561710        0.275817        0.672184
</pre>

<p>
The dispersion effect is close to zero, which suggests that the problem is
with the estimation of \(u_i^\phi\):
</p>

<div class="org-src-container">
<pre class="src src-ipython">ed.get_session().run(tf.exp(q_disp_bias.mean()), {onehot: data[<span class="org-string">'onehot'</span>][nonzero_cpm], genotypes: data[<span class="org-string">'genotypes'</span>], cell_bias: data[<span class="org-string">'normalizers'</span>][nonzero_cpm]})
</pre>
</div>

<pre class="example">
array([[ 1.08771348],
         [ 0.67666239],
         [ 1.02014589],
         [ 0.7174871 ],
         [ 0.73400629],
         [ 0.60470819],
         [ 1.13071108],
         [ 1.08213329],
         [ 1.0003413 ],
         [ 0.86023176],
         [ 0.82803261],
         [ 0.70752406],
         [ 0.91508389],
         [ 1.10532486],
         [ 0.89808285],
         [ 1.13777578],
         [ 0.83974165],
         [ 0.81459731],
         [ 0.88679075],
         [ 0.82396138],
         [ 0.67643213]], dtype=float32)
</pre>
</div>
</div>

<div id="outline-container-orgbd60d6a" class="outline-2">
<h2 id="orgbd60d6a">Next steps</h2>
<div class="outline-text-2" id="text-orgbd60d6a">
<ol class="org-ol">
<li><b>Do we need to fit a multivariate model?</b> Doing inference one SNP at a
time will be slow (6s per SNP). We should be able to fit a multivariate
regression with the spike and slab prior in roughly the same amount of
time. However, inference of the corresponding variational approximation is
only guaranteed to find a SNP in LD with the causal variant (Carbonetto
and Stephens 2012), and is known to overstate the confidence in its
posterior inclusion probability (Park et al. 2016).</li>

<li><p>
<b>Do we need to actually fit a dropout model?</b> The key idea of <code>mast</code>
(<a href="https://dx.doi.org/10.1186/s13059-015-0844-5">Finak et al 2015</a>) is to model non-zero \(R\) using a Gaussian
distribution, and model zero \(R\) using logistic regression.
</p>

<p>
This can be easily be incorporated in the BBVI algorithm (although likely
not in the Edward probabilistic programming language).
</p>

<p>
Incorporating the mean expression model as well as other known covariates
into the dropout model might allow us to reliably estimate the mean
parameters (i.e., reduce their posterior variance) even for genes with
moderate levels of zero-inflation.
</p>

<p>
We might choose not to model zeros because <code>mast</code> conditions on the
observed \(Y = 0\), not on a latent \(Z = 0\). This means we could simply
do the same and ignore zeros.
</p>

<p>
If we did so, then mean/variance QTL effect size estimation will be less
robust for genes with high dropout. This might not be a problem depending
on the stringency of gene filtering.
</p></li>

<li><p>
<b>For a single gene, do we need to worry about mean QTLs in LD with
variance QTLs?</b> We previously <a href="https://github.com/YPARK/fqtl">built multivariate mean/variance QTL models</a>
which could account for LD, and could share information between the mean
and variance models.
</p>

<p>
The fundamental problem is that if we assume that the mean and dispersion
both have genetic components, then the mean is no longer independent of
the dispersion.
</p>

<p>
This actually could be derived without using the fact that both depend on
the same genotypes if we use second-order Taylor expansion:
</p>

<p>
\[ \mu \approx \log_2 \lambda + \frac{V[R]}{2 \lambda^2} \]
</p></li>

<li><p>
<b>Do we need to share parameters between genes?</b> <code>mast</code> assumes genes are
conditionally independent. But this is no longer true when nearby genes
can be driven by overlapping (or correlated) <i>cis</i>-genotypes.
</p>

<p>
We previously developed multiresponse QTL models which learned the target
genes of causal variants, allowing the true target gene to explain away
nearby correlated genes (<a href="https://www.biorxiv.org/content/early/2017/11/14/219428">Park et al 2017</a>).
</p></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2017-12-20 Wed 10:22</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
